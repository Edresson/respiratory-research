{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "DtXp3DMyU4u2",
    "outputId": "9a02581f-ebc9-4307-c524-f915fc001341"
   },
   "source": [
    "# Rede CNN - Modelo 1\n",
    "\n",
    "## Dataset Respiratory_Sound_Database_Pneumo_Healthy_Only - Dataset 1 - no_augment_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "colab": {},
    "colab_type": "code",
    "id": "sxUgP6_bSR0C"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# Necessário na minha máquina. Estava ocorrendo um erro devido à GPU e esse código resolveu.\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUP_TRAIN = 'no_augment_5'\n",
    "GROUP_TEST = 'no_augment_5'\n",
    "DATASET = 'dataset_1'\n",
    "DURATION = 5\n",
    "SIZE = 216\n",
    "CSV_TRAIN = 'train1.csv'\n",
    "CSV_TEST = 'test1.csv'\n",
    "MODEL_NAME = f'CNN1_{DATASET}_{GROUP_TRAIN}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mMb5_PxwSR0N"
   },
   "outputs": [],
   "source": [
    "train_file_paths = glob.glob(f'../datasets/{DATASET}/{GROUP_TRAIN}/train/**/*.wav', recursive=True)\n",
    "train_file_names = [os.path.splitext(os.path.basename(p))[0] for p in train_file_paths]\n",
    "\n",
    "test_file_paths = glob.glob(f'../datasets/{DATASET}/{GROUP_TEST}/test/**/*.wav', recursive=True)\n",
    "test_file_names = [os.path.splitext(os.path.basename(p))[0] for p in test_file_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wTf5HxHzSR0U"
   },
   "outputs": [],
   "source": [
    "train_p_id_in_file = [] # patient IDs corresponding to each file\n",
    "test_p_id_in_file = [] # patient IDs corresponding to each file\n",
    "for name in train_file_names:\n",
    "    train_p_id_in_file.append(int(name[:3]))\n",
    "\n",
    "for name in test_file_names:\n",
    "    test_p_id_in_file.append(int(name[:3]))\n",
    "\n",
    "train_p_id_in_file = np.array(train_p_id_in_file)\n",
    "test_p_id_in_file = np.array(test_p_id_in_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UbK7vc1kSR0c"
   },
   "outputs": [],
   "source": [
    "max_pad_len = SIZE\n",
    "\n",
    "os.makedirs(\"features/\", exist_ok=True)\n",
    "\n",
    "def extract_features(file_name):\n",
    "    \"\"\"\n",
    "    This function takes in the path for an audio file as a string, loads it, and returns the MFCC\n",
    "    of the audio\"\"\"\n",
    "    feature = os.path.splitext(os.path.basename(file_name))[0] + \".npy\"\n",
    "#     if (os.path.isfile(os.path.join(\"./Respiratory_Sound_Database/features/\", feature))):\n",
    "#         return np.load(os.path.join(\"./Respiratory_Sound_Database/features/\", feature))\n",
    "    \n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast', duration=DURATION) \n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        pad_width = max_pad_len - mfccs.shape[1]\n",
    "        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None \n",
    "    np.save(os.path.join(\"./features/\", feature), mfccs)\n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xkBHJzJDSR0h"
   },
   "outputs": [],
   "source": [
    "#filepaths = [join(mypath, f) for f in filenames] # full paths of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UQjbbn7MSR0n"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>102</td>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>121</td>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>123</td>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>125</td>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>126</td>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0        1\n",
       "0  102  Healthy\n",
       "1  121  Healthy\n",
       "2  123  Healthy\n",
       "3  125  Healthy\n",
       "4  126  Healthy"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_p_diag = pd.read_csv(f\"../Respiratory_Sound_Database/Respiratory_Sound_Database_Pneumo_Healthy_Only/{CSV_TRAIN}\", header=None) # patient diagnosis file\n",
    "test_p_diag = pd.read_csv(f\"../Respiratory_Sound_Database/Respiratory_Sound_Database_Pneumo_Healthy_Only/{CSV_TEST}\", header=None) # patient diagnosis file\n",
    "train_p_diag.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yskEMhphSR0s"
   },
   "outputs": [],
   "source": [
    "train_labels = np.array([train_p_diag[train_p_diag[0] == x][1].values[0] for x in train_p_id_in_file]) \n",
    "test_labels = np.array([test_p_diag[test_p_diag[0] == x][1].values[0] for x in test_p_id_in_file]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "yIlzZ5gRSR0w",
    "outputId": "e42143d5-d247-457f-c891-0c714e51cb05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished feature extraction from  122  files\n"
     ]
    }
   ],
   "source": [
    "train_features = [] \n",
    "test_features = []\n",
    "\n",
    "for file_name in train_file_paths:\n",
    "    data = extract_features(file_name)\n",
    "    train_features.append(data)\n",
    "\n",
    "for file_name in test_file_paths:\n",
    "    data = extract_features(file_name)\n",
    "    test_features.append(data)\n",
    "\n",
    "print('Finished feature extraction from ', (len(train_features)+len(train_features)), ' files')\n",
    "train_features = np.array(train_features)\n",
    "test_features = np.array(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "aPWfXalkSR00",
    "outputId": "0741865f-420e-4c29-8cb1-42b0fe8302cb"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot an MFCC\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(train_features[7], x_axis='time')\n",
    "plt.colorbar()\n",
    "plt.title('MFCC')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Healthy', 'Pneumonia'], dtype='<U9')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "YAHg4HTzSR1C",
    "outputId": "be27b27d-3389-4d26-b8ee-d795b2259a5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Healthy' 'Pneumonia']\n",
      " ['32' '29']]\n",
      "[['Healthy' 'Pneumonia']\n",
      " ['3' '8']]\n"
     ]
    }
   ],
   "source": [
    "unique_elements, counts_elements = np.unique(train_labels, return_counts=True)\n",
    "print(np.asarray((unique_elements, counts_elements)))\n",
    "\n",
    "unique_elements_test, counts_elements_test = np.unique(test_labels, return_counts=True)\n",
    "print(np.asarray((unique_elements_test, counts_elements_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "mtNpDgBOSR1G",
    "outputId": "0f7584a0-d821-4196-c22f-3847fa3031b6"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot class counts\n",
    "y_pos = np.arange(len(unique_elements))\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.bar(unique_elements, counts_elements, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, unique_elements)\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Disease')\n",
    "plt.title('Disease Count in Sound Files (No Asthma or LRTI)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WtqGtxmPSR1K"
   },
   "outputs": [],
   "source": [
    "# One-hot encode labels\n",
    "le = LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "train_labels_cat = to_categorical(le.transform(train_labels)) \n",
    "test_labels_cat = to_categorical(le.transform(test_labels)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61, 40, 216)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MgH8aGqeSR1P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61, 40, 216, 1) (61, 2)\n",
      "(11, 40, 216, 1) (11, 2)\n"
     ]
    }
   ],
   "source": [
    "train_features = np.reshape(train_features, (*train_features.shape,1)) \n",
    "print(train_features.shape, train_labels_cat.shape)\n",
    "test_features = np.reshape(test_features, (*test_features.shape,1)) \n",
    "print(test_features.shape, test_labels_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xFPaVmUESR1T"
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_features, train_labels_cat, test_size=0.2, random_state = 42)\n",
    "x_test, y_test = test_features, test_labels_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SN1ipKhfSR1X"
   },
   "source": [
    "**CNN model architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oZ5PcMOrcV1B"
   },
   "outputs": [],
   "source": [
    "num_labels = train_labels_cat.shape[1]\n",
    "\n",
    "num_rows = 40\n",
    "num_columns = SIZE\n",
    "num_channels = 1\n",
    "\n",
    "filter_size = 2\n",
    "# Construct model \n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=filter_size,\n",
    "                 input_shape=(num_rows, num_columns, num_channels), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=filter_size, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=filter_size, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=filter_size, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(num_labels, activation='softmax')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VcSipiVsSR1c"
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 719
    },
    "colab_type": "code",
    "id": "UvvyonaaSR1h",
    "outputId": "9c2154c5-f927-4c3d-e89e-951ba479c079"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 39, 215, 16)       80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 19, 107, 16)       0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 19, 107, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 18, 106, 32)       2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 9, 53, 32)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 9, 53, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 52, 64)         8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 26, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 26, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 25, 128)        32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 1, 12, 128)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1, 12, 128)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 43,570\n",
      "Trainable params: 43,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "11/11 [==============================] - 1s 80ms/sample - loss: 10.5720 - accuracy: 0.2727\n",
      "Pre-training accuracy: 27.2727%\n"
     ]
    }
   ],
   "source": [
    "# Display model architecture summary \n",
    "model.summary()\n",
    "\n",
    "# Calculate pre-training accuracy \n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "print(\"Pre-training accuracy: %.4f%%\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TVtD0mcDSR1j"
   },
   "source": [
    "**Training**\n",
    "\n",
    "Here we will train the model. If we have a trained model, we can load it instead from the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ruRXrsrhSR1k",
    "outputId": "ec26a26d-ad59-4e89-c88f-6277e4d0c283"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48 samples, validate on 13 samples\n",
      "Epoch 1/500\n",
      "10/48 [=====>........................] - ETA: 2s - loss: 13.8462 - accuracy: 0.4000\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.53846, saving model to models/CNN2_dataset_1_no_augment_5_01.h5\n",
      "48/48 [==============================] - 1s 16ms/sample - loss: 6.7051 - accuracy: 0.3958 - val_loss: 2.5901 - val_accuracy: 0.5385\n",
      "Epoch 2/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.7830 - accuracy: 0.5000\n",
      "Epoch 00002: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 857us/sample - loss: 3.7125 - accuracy: 0.5000 - val_loss: 1.6321 - val_accuracy: 0.4615\n",
      "Epoch 3/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.2852 - accuracy: 0.4000\n",
      "Epoch 00003: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 856us/sample - loss: 2.5449 - accuracy: 0.4792 - val_loss: 1.9417 - val_accuracy: 0.5385\n",
      "Epoch 4/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.1343 - accuracy: 0.5000\n",
      "Epoch 00004: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 815us/sample - loss: 1.2418 - accuracy: 0.5208 - val_loss: 1.2269 - val_accuracy: 0.3846\n",
      "Epoch 5/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 2.1772 - accuracy: 0.5000\n",
      "Epoch 00005: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 790us/sample - loss: 1.7078 - accuracy: 0.5208 - val_loss: 1.9231 - val_accuracy: 0.5385\n",
      "Epoch 6/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 2.5010 - accuracy: 0.4000\n",
      "Epoch 00006: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 832us/sample - loss: 1.4060 - accuracy: 0.5000 - val_loss: 1.0494 - val_accuracy: 0.3846\n",
      "Epoch 7/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.1081 - accuracy: 0.5000\n",
      "Epoch 00007: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 774us/sample - loss: 1.4142 - accuracy: 0.5833 - val_loss: 1.4696 - val_accuracy: 0.5385\n",
      "Epoch 8/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.0492 - accuracy: 0.6000\n",
      "Epoch 00008: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 874us/sample - loss: 1.0633 - accuracy: 0.5417 - val_loss: 1.0616 - val_accuracy: 0.5385\n",
      "Epoch 9/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.3894 - accuracy: 0.6000\n",
      "Epoch 00009: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 881us/sample - loss: 1.0534 - accuracy: 0.5833 - val_loss: 1.1656 - val_accuracy: 0.5385\n",
      "Epoch 10/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.5743 - accuracy: 0.6000\n",
      "Epoch 00010: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 752us/sample - loss: 0.6244 - accuracy: 0.5625 - val_loss: 0.9860 - val_accuracy: 0.5385\n",
      "Epoch 11/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.7342 - accuracy: 0.7000\n",
      "Epoch 00011: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 875us/sample - loss: 1.0816 - accuracy: 0.5625 - val_loss: 1.1879 - val_accuracy: 0.5385\n",
      "Epoch 12/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.9357 - accuracy: 0.5000\n",
      "Epoch 00012: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 734us/sample - loss: 0.9521 - accuracy: 0.4792 - val_loss: 0.8667 - val_accuracy: 0.5385\n",
      "Epoch 13/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.5189 - accuracy: 0.6000\n",
      "Epoch 00013: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 845us/sample - loss: 0.6860 - accuracy: 0.6458 - val_loss: 0.9917 - val_accuracy: 0.5385\n",
      "Epoch 14/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.8836 - accuracy: 0.7000\n",
      "Epoch 00014: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 825us/sample - loss: 0.7795 - accuracy: 0.6667 - val_loss: 0.9070 - val_accuracy: 0.5385\n",
      "Epoch 15/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.6664 - accuracy: 0.6000\n",
      "Epoch 00015: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 822us/sample - loss: 0.5253 - accuracy: 0.7292 - val_loss: 0.8694 - val_accuracy: 0.5385\n",
      "Epoch 16/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.7199 - accuracy: 0.6000\n",
      "Epoch 00016: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 830us/sample - loss: 0.6968 - accuracy: 0.7083 - val_loss: 0.9847 - val_accuracy: 0.5385\n",
      "Epoch 17/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.4427 - accuracy: 0.8000\n",
      "Epoch 00017: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 822us/sample - loss: 0.6137 - accuracy: 0.6875 - val_loss: 0.7726 - val_accuracy: 0.5385\n",
      "Epoch 18/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.8645 - accuracy: 0.6000\n",
      "Epoch 00018: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 839us/sample - loss: 0.8319 - accuracy: 0.5833 - val_loss: 1.1050 - val_accuracy: 0.5385\n",
      "Epoch 19/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.4270 - accuracy: 0.7000\n",
      "Epoch 00019: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 835us/sample - loss: 0.4751 - accuracy: 0.7708 - val_loss: 0.9118 - val_accuracy: 0.5385\n",
      "Epoch 20/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.0059 - accuracy: 0.7000\n",
      "Epoch 00020: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 819us/sample - loss: 0.7466 - accuracy: 0.7083 - val_loss: 1.0472 - val_accuracy: 0.5385\n",
      "Epoch 21/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.8788 - accuracy: 0.7000\n",
      "Epoch 00021: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 850us/sample - loss: 0.6748 - accuracy: 0.7083 - val_loss: 0.8838 - val_accuracy: 0.5385\n",
      "Epoch 22/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.4852 - accuracy: 0.9000\n",
      "Epoch 00022: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 816us/sample - loss: 0.6199 - accuracy: 0.7500 - val_loss: 1.1479 - val_accuracy: 0.5385\n",
      "Epoch 23/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.7623 - accuracy: 0.7000\n",
      "Epoch 00023: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 902us/sample - loss: 0.5103 - accuracy: 0.8125 - val_loss: 0.8864 - val_accuracy: 0.5385\n",
      "Epoch 24/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.3435 - accuracy: 0.9000\n",
      "Epoch 00024: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 801us/sample - loss: 0.4653 - accuracy: 0.7917 - val_loss: 1.0008 - val_accuracy: 0.5385\n",
      "Epoch 25/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.3219 - accuracy: 0.9000\n",
      "Epoch 00025: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 861us/sample - loss: 0.5365 - accuracy: 0.7917 - val_loss: 0.7884 - val_accuracy: 0.5385\n",
      "Epoch 26/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.3347 - accuracy: 0.8000\n",
      "Epoch 00026: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 748us/sample - loss: 0.4247 - accuracy: 0.8542 - val_loss: 0.7099 - val_accuracy: 0.5385\n",
      "Epoch 27/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.6178 - accuracy: 0.6000\n",
      "Epoch 00027: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 830us/sample - loss: 0.5259 - accuracy: 0.7083 - val_loss: 0.7637 - val_accuracy: 0.5385\n",
      "Epoch 28/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.5204 - accuracy: 0.9000\n",
      "Epoch 00028: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 832us/sample - loss: 0.4064 - accuracy: 0.7917 - val_loss: 0.6735 - val_accuracy: 0.5385\n",
      "Epoch 29/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.5689 - accuracy: 0.7000\n",
      "Epoch 00029: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 883us/sample - loss: 0.5842 - accuracy: 0.7292 - val_loss: 0.6660 - val_accuracy: 0.5385\n",
      "Epoch 30/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.6619 - accuracy: 0.8000\n",
      "Epoch 00030: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 846us/sample - loss: 0.4694 - accuracy: 0.8333 - val_loss: 0.7514 - val_accuracy: 0.5385\n",
      "Epoch 31/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.3008 - accuracy: 0.8000\n",
      "Epoch 00031: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 795us/sample - loss: 0.4623 - accuracy: 0.8125 - val_loss: 0.8037 - val_accuracy: 0.5385\n",
      "Epoch 32/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2269 - accuracy: 0.9000\n",
      "Epoch 00032: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 793us/sample - loss: 0.3633 - accuracy: 0.8542 - val_loss: 0.6294 - val_accuracy: 0.5385\n",
      "Epoch 33/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.3236 - accuracy: 0.9000\n",
      "Epoch 00033: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 811us/sample - loss: 0.3176 - accuracy: 0.8125 - val_loss: 0.7651 - val_accuracy: 0.5385\n",
      "Epoch 34/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.5405 - accuracy: 0.8000\n",
      "Epoch 00034: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 750us/sample - loss: 0.4983 - accuracy: 0.7708 - val_loss: 0.6223 - val_accuracy: 0.5385\n",
      "Epoch 35/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2410 - accuracy: 1.0000\n",
      "Epoch 00035: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 728us/sample - loss: 0.5586 - accuracy: 0.8333 - val_loss: 0.6601 - val_accuracy: 0.5385\n",
      "Epoch 36/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1547 - accuracy: 0.9000\n",
      "Epoch 00036: val_accuracy improved from 0.53846 to 0.61538, saving model to models/CNN2_dataset_1_no_augment_5_36.h5\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.2601 - accuracy: 0.9167 - val_loss: 0.5087 - val_accuracy: 0.6154\n",
      "Epoch 37/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.3551 - accuracy: 0.9000\n",
      "Epoch 00037: val_accuracy did not improve from 0.61538\n",
      "48/48 [==============================] - 0s 773us/sample - loss: 0.2953 - accuracy: 0.8750 - val_loss: 0.6177 - val_accuracy: 0.6154\n",
      "Epoch 38/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.7702 - accuracy: 0.8000\n",
      "Epoch 00038: val_accuracy did not improve from 0.61538\n",
      "48/48 [==============================] - 0s 843us/sample - loss: 0.3222 - accuracy: 0.9167 - val_loss: 0.5258 - val_accuracy: 0.6154\n",
      "Epoch 39/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2080 - accuracy: 0.9000\n",
      "Epoch 00039: val_accuracy did not improve from 0.61538\n",
      "48/48 [==============================] - 0s 839us/sample - loss: 0.3093 - accuracy: 0.8958 - val_loss: 0.6064 - val_accuracy: 0.6154\n",
      "Epoch 40/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.3516 - accuracy: 0.8000\n",
      "Epoch 00040: val_accuracy did not improve from 0.61538\n",
      "48/48 [==============================] - 0s 895us/sample - loss: 0.3269 - accuracy: 0.7917 - val_loss: 0.6058 - val_accuracy: 0.6154\n",
      "Epoch 41/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1904 - accuracy: 0.9000\n",
      "Epoch 00041: val_accuracy improved from 0.61538 to 0.69231, saving model to models/CNN2_dataset_1_no_augment_5_41.h5\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.3686 - accuracy: 0.7917 - val_loss: 0.4815 - val_accuracy: 0.6923\n",
      "Epoch 42/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1519 - accuracy: 1.0000\n",
      "Epoch 00042: val_accuracy did not improve from 0.69231\n",
      "48/48 [==============================] - 0s 723us/sample - loss: 0.3897 - accuracy: 0.8333 - val_loss: 0.6753 - val_accuracy: 0.6154\n",
      "Epoch 43/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.3325 - accuracy: 0.9000\n",
      "Epoch 00043: val_accuracy improved from 0.69231 to 0.76923, saving model to models/CNN2_dataset_1_no_augment_5_43.h5\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.4375 - accuracy: 0.8542 - val_loss: 0.3660 - val_accuracy: 0.7692\n",
      "Epoch 44/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2162 - accuracy: 0.8000\n",
      "Epoch 00044: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 811us/sample - loss: 0.4003 - accuracy: 0.8125 - val_loss: 0.4982 - val_accuracy: 0.6923\n",
      "Epoch 45/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0838 - accuracy: 1.0000\n",
      "Epoch 00045: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 840us/sample - loss: 0.1633 - accuracy: 0.9375 - val_loss: 0.4830 - val_accuracy: 0.6923\n",
      "Epoch 46/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.4305 - accuracy: 0.7000\n",
      "Epoch 00046: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 824us/sample - loss: 0.2723 - accuracy: 0.8542 - val_loss: 0.4009 - val_accuracy: 0.6923\n",
      "Epoch 47/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2756 - accuracy: 1.0000\n",
      "Epoch 00047: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 817us/sample - loss: 0.1965 - accuracy: 0.9375 - val_loss: 0.5944 - val_accuracy: 0.6923\n",
      "Epoch 48/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1553 - accuracy: 0.9000\n",
      "Epoch 00048: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.3122 - accuracy: 0.8333 - val_loss: 0.3672 - val_accuracy: 0.6923\n",
      "Epoch 49/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1473 - accuracy: 1.0000\n",
      "Epoch 00049: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 767us/sample - loss: 0.2411 - accuracy: 0.9167 - val_loss: 0.5200 - val_accuracy: 0.6923\n",
      "Epoch 50/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.4152 - accuracy: 0.9000\n",
      "Epoch 00050: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 885us/sample - loss: 0.3164 - accuracy: 0.8542 - val_loss: 0.3651 - val_accuracy: 0.6923\n",
      "Epoch 51/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1379 - accuracy: 0.9000\n",
      "Epoch 00051: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 741us/sample - loss: 0.3272 - accuracy: 0.8750 - val_loss: 0.3277 - val_accuracy: 0.7692\n",
      "Epoch 52/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.3080 - accuracy: 0.8000\n",
      "Epoch 00052: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 689us/sample - loss: 0.3781 - accuracy: 0.8125 - val_loss: 0.4506 - val_accuracy: 0.6923\n",
      "Epoch 53/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1021 - accuracy: 1.0000\n",
      "Epoch 00053: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 713us/sample - loss: 0.2794 - accuracy: 0.9167 - val_loss: 0.3315 - val_accuracy: 0.6923\n",
      "Epoch 54/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1365 - accuracy: 1.0000\n",
      "Epoch 00054: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 700us/sample - loss: 0.2428 - accuracy: 0.8542 - val_loss: 0.4381 - val_accuracy: 0.6923\n",
      "Epoch 55/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.3663 - accuracy: 0.7000\n",
      "Epoch 00055: val_accuracy improved from 0.76923 to 0.92308, saving model to models/CNN2_dataset_1_no_augment_5_55.h5\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.3281 - accuracy: 0.7917 - val_loss: 0.2550 - val_accuracy: 0.9231\n",
      "Epoch 56/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2270 - accuracy: 0.8000\n",
      "Epoch 00056: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 758us/sample - loss: 0.1944 - accuracy: 0.8750 - val_loss: 0.4171 - val_accuracy: 0.6923\n",
      "Epoch 57/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2361 - accuracy: 0.9000\n",
      "Epoch 00057: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 737us/sample - loss: 0.2079 - accuracy: 0.9167 - val_loss: 0.2667 - val_accuracy: 0.8462\n",
      "Epoch 58/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0464 - accuracy: 1.0000\n",
      "Epoch 00058: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 774us/sample - loss: 0.2074 - accuracy: 0.9167 - val_loss: 0.3398 - val_accuracy: 0.6923\n",
      "Epoch 59/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2542 - accuracy: 0.8000\n",
      "Epoch 00059: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 785us/sample - loss: 0.2939 - accuracy: 0.8750 - val_loss: 0.2901 - val_accuracy: 0.8462\n",
      "Epoch 60/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2275 - accuracy: 0.8000\n",
      "Epoch 00060: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 787us/sample - loss: 0.2051 - accuracy: 0.8750 - val_loss: 0.2656 - val_accuracy: 0.9231\n",
      "Epoch 61/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2943 - accuracy: 0.9000\n",
      "Epoch 00061: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 739us/sample - loss: 0.2635 - accuracy: 0.8542 - val_loss: 0.3379 - val_accuracy: 0.6923\n",
      "Epoch 62/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1878 - accuracy: 0.9000\n",
      "Epoch 00062: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 727us/sample - loss: 0.1268 - accuracy: 0.9792 - val_loss: 0.2498 - val_accuracy: 0.9231\n",
      "Epoch 63/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1353 - accuracy: 0.9000\n",
      "Epoch 00063: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 728us/sample - loss: 0.2294 - accuracy: 0.8750 - val_loss: 0.2926 - val_accuracy: 0.7692\n",
      "Epoch 64/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0547 - accuracy: 1.0000\n",
      "Epoch 00064: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 699us/sample - loss: 0.1796 - accuracy: 0.8958 - val_loss: 0.2540 - val_accuracy: 0.9231\n",
      "Epoch 65/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1760 - accuracy: 0.8000\n",
      "Epoch 00065: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 703us/sample - loss: 0.2423 - accuracy: 0.8958 - val_loss: 0.2723 - val_accuracy: 0.8462\n",
      "Epoch 66/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.4411 - accuracy: 0.8000\n",
      "Epoch 00066: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 717us/sample - loss: 0.2565 - accuracy: 0.8542 - val_loss: 0.1962 - val_accuracy: 0.9231\n",
      "Epoch 67/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.3257 - accuracy: 0.8000\n",
      "Epoch 00067: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 720us/sample - loss: 0.2035 - accuracy: 0.8958 - val_loss: 0.2948 - val_accuracy: 0.7692\n",
      "Epoch 68/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0261 - accuracy: 1.0000\n",
      "Epoch 00068: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 724us/sample - loss: 0.1448 - accuracy: 0.9167 - val_loss: 0.2020 - val_accuracy: 0.9231\n",
      "Epoch 69/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0253 - accuracy: 1.0000\n",
      "Epoch 00069: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 738us/sample - loss: 0.1971 - accuracy: 0.9583 - val_loss: 0.2449 - val_accuracy: 0.9231\n",
      "Epoch 70/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.3582 - accuracy: 0.7000\n",
      "Epoch 00070: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 719us/sample - loss: 0.2437 - accuracy: 0.8958 - val_loss: 0.2132 - val_accuracy: 0.9231\n",
      "Epoch 71/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1184 - accuracy: 0.9000\n",
      "Epoch 00071: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 744us/sample - loss: 0.1732 - accuracy: 0.9167 - val_loss: 0.2665 - val_accuracy: 0.7692\n",
      "Epoch 72/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2638 - accuracy: 0.8000\n",
      "Epoch 00072: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 740us/sample - loss: 0.1836 - accuracy: 0.8958 - val_loss: 0.2034 - val_accuracy: 0.9231\n",
      "Epoch 73/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1556 - accuracy: 0.9000\n",
      "Epoch 00073: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 751us/sample - loss: 0.1907 - accuracy: 0.9167 - val_loss: 0.2720 - val_accuracy: 0.7692\n",
      "Epoch 74/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0662 - accuracy: 1.0000\n",
      "Epoch 00074: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 732us/sample - loss: 0.2046 - accuracy: 0.9167 - val_loss: 0.1898 - val_accuracy: 0.9231\n",
      "Epoch 75/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.3085 - accuracy: 0.9000\n",
      "Epoch 00075: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 738us/sample - loss: 0.2849 - accuracy: 0.8542 - val_loss: 0.2174 - val_accuracy: 0.9231\n",
      "Epoch 76/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0855 - accuracy: 1.0000\n",
      "Epoch 00076: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 742us/sample - loss: 0.1988 - accuracy: 0.9583 - val_loss: 0.2385 - val_accuracy: 0.9231\n",
      "Epoch 77/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0909 - accuracy: 1.0000\n",
      "Epoch 00077: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 745us/sample - loss: 0.2497 - accuracy: 0.8958 - val_loss: 0.1992 - val_accuracy: 0.9231\n",
      "Epoch 78/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0383 - accuracy: 1.0000\n",
      "Epoch 00078: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 739us/sample - loss: 0.2234 - accuracy: 0.9167 - val_loss: 0.2629 - val_accuracy: 0.8462\n",
      "Epoch 79/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1981 - accuracy: 0.9000\n",
      "Epoch 00079: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 737us/sample - loss: 0.1911 - accuracy: 0.9167 - val_loss: 0.2750 - val_accuracy: 0.8462\n",
      "Epoch 80/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1390 - accuracy: 0.9000\n",
      "Epoch 00080: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 918us/sample - loss: 0.1576 - accuracy: 0.9583 - val_loss: 0.1912 - val_accuracy: 0.9231\n",
      "Epoch 81/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0844 - accuracy: 1.0000\n",
      "Epoch 00081: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 698us/sample - loss: 0.1444 - accuracy: 0.9583 - val_loss: 0.3451 - val_accuracy: 0.6923\n",
      "Epoch 82/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1505 - accuracy: 0.9000\n",
      "Epoch 00082: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 720us/sample - loss: 0.1703 - accuracy: 0.9375 - val_loss: 0.2083 - val_accuracy: 0.9231\n",
      "Epoch 83/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1209 - accuracy: 1.0000\n",
      "Epoch 00083: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 707us/sample - loss: 0.1347 - accuracy: 0.9375 - val_loss: 0.2441 - val_accuracy: 0.9231\n",
      "Epoch 84/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0400 - accuracy: 1.0000\n",
      "Epoch 00084: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 714us/sample - loss: 0.1717 - accuracy: 0.8750 - val_loss: 0.2927 - val_accuracy: 0.8462\n",
      "Epoch 85/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2034 - accuracy: 0.9000\n",
      "Epoch 00085: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 698us/sample - loss: 0.1614 - accuracy: 0.9167 - val_loss: 0.1976 - val_accuracy: 0.9231\n",
      "Epoch 86/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0219 - accuracy: 1.0000\n",
      "Epoch 00086: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 783us/sample - loss: 0.1192 - accuracy: 0.9583 - val_loss: 0.2131 - val_accuracy: 0.9231\n",
      "Epoch 87/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1177 - accuracy: 1.0000\n",
      "Epoch 00087: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 839us/sample - loss: 0.1556 - accuracy: 0.9375 - val_loss: 0.2141 - val_accuracy: 0.9231\n",
      "Epoch 88/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0912 - accuracy: 1.0000\n",
      "Epoch 00088: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 754us/sample - loss: 0.0997 - accuracy: 0.9583 - val_loss: 0.1966 - val_accuracy: 0.9231\n",
      "Epoch 89/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0432 - accuracy: 1.0000\n",
      "Epoch 00089: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 721us/sample - loss: 0.1117 - accuracy: 0.9792 - val_loss: 0.1854 - val_accuracy: 0.9231\n",
      "Epoch 90/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0638 - accuracy: 1.0000\n",
      "Epoch 00090: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 706us/sample - loss: 0.1701 - accuracy: 0.9375 - val_loss: 0.2878 - val_accuracy: 0.7692\n",
      "Epoch 91/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1266 - accuracy: 1.0000\n",
      "Epoch 00091: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 702us/sample - loss: 0.2669 - accuracy: 0.8958 - val_loss: 0.1694 - val_accuracy: 0.9231\n",
      "Epoch 92/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0360 - accuracy: 1.0000\n",
      "Epoch 00092: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 698us/sample - loss: 0.3372 - accuracy: 0.8958 - val_loss: 0.3090 - val_accuracy: 0.7692\n",
      "Epoch 93/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1282 - accuracy: 0.9000\n",
      "Epoch 00093: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 709us/sample - loss: 0.2032 - accuracy: 0.8958 - val_loss: 0.2488 - val_accuracy: 0.9231\n",
      "Epoch 94/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.3551 - accuracy: 0.9000\n",
      "Epoch 00094: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 712us/sample - loss: 0.2230 - accuracy: 0.9167 - val_loss: 0.1663 - val_accuracy: 0.9231\n",
      "Epoch 95/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0863 - accuracy: 0.9000\n",
      "Epoch 00095: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 735us/sample - loss: 0.1243 - accuracy: 0.9375 - val_loss: 0.3712 - val_accuracy: 0.6923\n",
      "Epoch 96/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2939 - accuracy: 0.9000\n",
      "Epoch 00096: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 746us/sample - loss: 0.1738 - accuracy: 0.9167 - val_loss: 0.1662 - val_accuracy: 0.9231\n",
      "Epoch 97/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1561 - accuracy: 0.9000\n",
      "Epoch 00097: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 702us/sample - loss: 0.2296 - accuracy: 0.8750 - val_loss: 0.2916 - val_accuracy: 0.7692\n",
      "Epoch 98/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1724 - accuracy: 0.9000\n",
      "Epoch 00098: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 717us/sample - loss: 0.1773 - accuracy: 0.8958 - val_loss: 0.2274 - val_accuracy: 0.9231\n",
      "Epoch 99/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0797 - accuracy: 1.0000\n",
      "Epoch 00099: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 705us/sample - loss: 0.1635 - accuracy: 0.9167 - val_loss: 0.2905 - val_accuracy: 0.7692\n",
      "Epoch 100/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2944 - accuracy: 0.9000\n",
      "Epoch 00100: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 722us/sample - loss: 0.1725 - accuracy: 0.9375 - val_loss: 0.2699 - val_accuracy: 0.8462\n",
      "Epoch 101/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0684 - accuracy: 1.0000\n",
      "Epoch 00101: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 722us/sample - loss: 0.0632 - accuracy: 1.0000 - val_loss: 0.2593 - val_accuracy: 0.8462\n",
      "Epoch 102/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1676 - accuracy: 0.9000\n",
      "Epoch 00102: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 779us/sample - loss: 0.1413 - accuracy: 0.9375 - val_loss: 0.3080 - val_accuracy: 0.7692\n",
      "Epoch 103/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0682 - accuracy: 1.0000\n",
      "Epoch 00103: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 714us/sample - loss: 0.1328 - accuracy: 0.9583 - val_loss: 0.2063 - val_accuracy: 0.9231\n",
      "Epoch 104/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0825 - accuracy: 1.0000\n",
      "Epoch 00104: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 704us/sample - loss: 0.1380 - accuracy: 0.9583 - val_loss: 0.2828 - val_accuracy: 0.7692\n",
      "Epoch 105/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1347 - accuracy: 1.0000\n",
      "Epoch 00105: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 697us/sample - loss: 0.1391 - accuracy: 0.9583 - val_loss: 0.2587 - val_accuracy: 0.8462\n",
      "Epoch 106/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1260 - accuracy: 1.0000\n",
      "Epoch 00106: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 704us/sample - loss: 0.1379 - accuracy: 0.9583 - val_loss: 0.2113 - val_accuracy: 0.9231\n",
      "Epoch 107/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0238 - accuracy: 1.0000\n",
      "Epoch 00107: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 760us/sample - loss: 0.2464 - accuracy: 0.8750 - val_loss: 0.3682 - val_accuracy: 0.7692\n",
      "Epoch 108/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2372 - accuracy: 0.8000\n",
      "Epoch 00108: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 769us/sample - loss: 0.2530 - accuracy: 0.8542 - val_loss: 0.1570 - val_accuracy: 0.9231\n",
      "Epoch 109/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0784 - accuracy: 1.0000\n",
      "Epoch 00109: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 767us/sample - loss: 0.1891 - accuracy: 0.9375 - val_loss: 0.2537 - val_accuracy: 0.8462\n",
      "Epoch 110/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2158 - accuracy: 0.8000\n",
      "Epoch 00110: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 777us/sample - loss: 0.1569 - accuracy: 0.9167 - val_loss: 0.1921 - val_accuracy: 0.9231\n",
      "Epoch 111/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.3617 - accuracy: 0.8000\n",
      "Epoch 00111: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 748us/sample - loss: 0.1988 - accuracy: 0.8958 - val_loss: 0.2007 - val_accuracy: 0.9231\n",
      "Epoch 112/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2053 - accuracy: 0.9000\n",
      "Epoch 00112: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 733us/sample - loss: 0.1434 - accuracy: 0.9375 - val_loss: 0.2130 - val_accuracy: 0.9231\n",
      "Epoch 113/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.3516 - accuracy: 0.9000\n",
      "Epoch 00113: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 715us/sample - loss: 0.2295 - accuracy: 0.8958 - val_loss: 0.1894 - val_accuracy: 0.9231\n",
      "Epoch 114/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1100 - accuracy: 0.9000\n",
      "Epoch 00114: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 703us/sample - loss: 0.2392 - accuracy: 0.8750 - val_loss: 0.1792 - val_accuracy: 0.9231\n",
      "Epoch 115/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2058 - accuracy: 0.9000\n",
      "Epoch 00115: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 717us/sample - loss: 0.2804 - accuracy: 0.8542 - val_loss: 0.2285 - val_accuracy: 0.9231\n",
      "Epoch 116/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0259 - accuracy: 1.0000\n",
      "Epoch 00116: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 732us/sample - loss: 0.3238 - accuracy: 0.8958 - val_loss: 0.1635 - val_accuracy: 0.9231\n",
      "Epoch 117/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0450 - accuracy: 1.0000\n",
      "Epoch 00117: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 716us/sample - loss: 0.1815 - accuracy: 0.8958 - val_loss: 0.3533 - val_accuracy: 0.7692\n",
      "Epoch 118/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2204 - accuracy: 0.9000\n",
      "Epoch 00118: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 714us/sample - loss: 0.2619 - accuracy: 0.9167 - val_loss: 0.1573 - val_accuracy: 0.9231\n",
      "Epoch 119/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0271 - accuracy: 1.0000\n",
      "Epoch 00119: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 704us/sample - loss: 0.1191 - accuracy: 0.9792 - val_loss: 0.3541 - val_accuracy: 0.7692\n",
      "Epoch 120/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2523 - accuracy: 0.9000\n",
      "Epoch 00120: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 725us/sample - loss: 0.1281 - accuracy: 0.9583 - val_loss: 0.2000 - val_accuracy: 0.9231\n",
      "Epoch 121/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.4141 - accuracy: 0.9000\n",
      "Epoch 00121: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 714us/sample - loss: 0.1990 - accuracy: 0.9583 - val_loss: 0.1694 - val_accuracy: 0.9231\n",
      "Epoch 122/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0222 - accuracy: 1.0000\n",
      "Epoch 00122: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 709us/sample - loss: 0.1474 - accuracy: 0.9583 - val_loss: 0.2878 - val_accuracy: 0.8462\n",
      "Epoch 123/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1437 - accuracy: 0.9000\n",
      "Epoch 00123: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 804us/sample - loss: 0.1147 - accuracy: 0.9583 - val_loss: 0.1926 - val_accuracy: 0.9231\n",
      "Epoch 124/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0338 - accuracy: 1.0000\n",
      "Epoch 00124: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 783us/sample - loss: 0.0830 - accuracy: 0.9792 - val_loss: 0.1962 - val_accuracy: 0.9231\n",
      "Epoch 125/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0868 - accuracy: 1.0000\n",
      "Epoch 00125: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 867us/sample - loss: 0.1070 - accuracy: 0.9583 - val_loss: 0.2332 - val_accuracy: 0.8462\n",
      "Epoch 126/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0329 - accuracy: 1.0000\n",
      "Epoch 00126: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 870us/sample - loss: 0.1691 - accuracy: 0.9167 - val_loss: 0.2256 - val_accuracy: 0.8462\n",
      "Epoch 127/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2315 - accuracy: 0.9000\n",
      "Epoch 00127: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 841us/sample - loss: 0.1326 - accuracy: 0.9583 - val_loss: 0.1843 - val_accuracy: 0.9231\n",
      "Epoch 128/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 00128: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 771us/sample - loss: 0.1518 - accuracy: 0.9167 - val_loss: 0.2446 - val_accuracy: 0.8462\n",
      "Epoch 129/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 00129: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 848us/sample - loss: 0.1628 - accuracy: 0.9167 - val_loss: 0.1846 - val_accuracy: 0.9231\n",
      "Epoch 130/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0577 - accuracy: 1.0000\n",
      "Epoch 00130: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 757us/sample - loss: 0.1446 - accuracy: 0.9583 - val_loss: 0.1858 - val_accuracy: 0.9231\n",
      "Epoch 131/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0704 - accuracy: 1.0000\n",
      "Epoch 00131: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 750us/sample - loss: 0.1415 - accuracy: 0.9167 - val_loss: 0.2375 - val_accuracy: 0.8462\n",
      "Epoch 132/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0804 - accuracy: 1.0000\n",
      "Epoch 00132: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 797us/sample - loss: 0.1402 - accuracy: 0.9375 - val_loss: 0.2284 - val_accuracy: 0.8462\n",
      "Epoch 133/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0253 - accuracy: 1.0000\n",
      "Epoch 00133: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 748us/sample - loss: 0.1525 - accuracy: 0.9375 - val_loss: 0.1650 - val_accuracy: 0.9231\n",
      "Epoch 134/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0630 - accuracy: 1.0000\n",
      "Epoch 00134: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 765us/sample - loss: 0.1482 - accuracy: 0.9375 - val_loss: 0.2053 - val_accuracy: 0.8462\n",
      "Epoch 135/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1669 - accuracy: 0.9000\n",
      "Epoch 00135: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 773us/sample - loss: 0.1073 - accuracy: 0.9583 - val_loss: 0.1643 - val_accuracy: 0.9231\n",
      "Epoch 136/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1370 - accuracy: 0.9000\n",
      "Epoch 00136: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 759us/sample - loss: 0.0732 - accuracy: 0.9583 - val_loss: 0.1634 - val_accuracy: 0.9231\n",
      "Epoch 137/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.3234 - accuracy: 0.9000\n",
      "Epoch 00137: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 775us/sample - loss: 0.1238 - accuracy: 0.9792 - val_loss: 0.1715 - val_accuracy: 0.9231\n",
      "Epoch 138/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0605 - accuracy: 1.0000\n",
      "Epoch 00138: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 787us/sample - loss: 0.1050 - accuracy: 0.9792 - val_loss: 0.1800 - val_accuracy: 0.9231\n",
      "Epoch 139/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0164 - accuracy: 1.0000\n",
      "Epoch 00139: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 799us/sample - loss: 0.0814 - accuracy: 0.9792 - val_loss: 0.2454 - val_accuracy: 0.8462\n",
      "Epoch 140/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 00140: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 785us/sample - loss: 0.1529 - accuracy: 0.8958 - val_loss: 0.1445 - val_accuracy: 0.9231\n",
      "Epoch 141/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0502 - accuracy: 1.0000\n",
      "Epoch 00141: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 745us/sample - loss: 0.0901 - accuracy: 0.9583 - val_loss: 0.1523 - val_accuracy: 0.9231\n",
      "Epoch 142/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0753 - accuracy: 1.0000\n",
      "Epoch 00142: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 774us/sample - loss: 0.0691 - accuracy: 1.0000 - val_loss: 0.2111 - val_accuracy: 0.9231\n",
      "Epoch 143/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0437 - accuracy: 1.0000\n",
      "Epoch 00143: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 769us/sample - loss: 0.0915 - accuracy: 0.9583 - val_loss: 0.1733 - val_accuracy: 0.9231\n",
      "Epoch 144/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2258 - accuracy: 0.9000\n",
      "Epoch 00144: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 779us/sample - loss: 0.0784 - accuracy: 0.9792 - val_loss: 0.1516 - val_accuracy: 0.9231\n",
      "Epoch 145/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1539 - accuracy: 0.9000\n",
      "Epoch 00145: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 779us/sample - loss: 0.1393 - accuracy: 0.9167 - val_loss: 0.1823 - val_accuracy: 0.9231\n",
      "Epoch 146/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0938 - accuracy: 1.0000\n",
      "Epoch 00146: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 810us/sample - loss: 0.0763 - accuracy: 0.9792 - val_loss: 0.2286 - val_accuracy: 0.8462\n",
      "Epoch 147/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 00147: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 750us/sample - loss: 0.0632 - accuracy: 1.0000 - val_loss: 0.1517 - val_accuracy: 0.9231\n",
      "Epoch 148/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0202 - accuracy: 1.0000\n",
      "Epoch 00148: val_accuracy improved from 0.92308 to 1.00000, saving model to models/CNN2_dataset_1_no_augment_5_148.h5\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.1491 - accuracy: 0.9792 - val_loss: 0.1339 - val_accuracy: 1.0000\n",
      "Epoch 149/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1946 - accuracy: 0.8000\n",
      "Epoch 00149: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 731us/sample - loss: 0.1028 - accuracy: 0.9375 - val_loss: 0.3373 - val_accuracy: 0.7692\n",
      "Epoch 150/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1002 - accuracy: 0.9000\n",
      "Epoch 00150: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 730us/sample - loss: 0.1528 - accuracy: 0.9375 - val_loss: 0.1313 - val_accuracy: 1.0000\n",
      "Epoch 151/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2738 - accuracy: 0.9000\n",
      "Epoch 00151: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 704us/sample - loss: 0.1867 - accuracy: 0.8958 - val_loss: 0.2524 - val_accuracy: 0.8462\n",
      "Epoch 152/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0749 - accuracy: 1.0000\n",
      "Epoch 00152: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 790us/sample - loss: 0.2954 - accuracy: 0.8750 - val_loss: 0.1310 - val_accuracy: 0.9231\n",
      "Epoch 153/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0170 - accuracy: 1.0000\n",
      "Epoch 00153: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 779us/sample - loss: 0.3195 - accuracy: 0.8958 - val_loss: 0.1461 - val_accuracy: 1.0000\n",
      "Epoch 154/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1077 - accuracy: 0.9000\n",
      "Epoch 00154: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 720us/sample - loss: 0.1270 - accuracy: 0.9375 - val_loss: 0.2553 - val_accuracy: 0.8462\n",
      "Epoch 155/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1648 - accuracy: 0.9000\n",
      "Epoch 00155: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 799us/sample - loss: 0.1269 - accuracy: 0.9167 - val_loss: 0.1540 - val_accuracy: 0.9231\n",
      "Epoch 156/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0209 - accuracy: 1.0000\n",
      "Epoch 00156: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 751us/sample - loss: 0.1200 - accuracy: 0.9792 - val_loss: 0.1735 - val_accuracy: 0.9231\n",
      "Epoch 157/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2894 - accuracy: 0.9000\n",
      "Epoch 00157: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 802us/sample - loss: 0.1545 - accuracy: 0.9375 - val_loss: 0.1607 - val_accuracy: 0.9231\n",
      "Epoch 158/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0288 - accuracy: 1.0000\n",
      "Epoch 00158: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 797us/sample - loss: 0.0860 - accuracy: 0.9583 - val_loss: 0.1517 - val_accuracy: 0.9231\n",
      "Epoch 159/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0317 - accuracy: 1.0000\n",
      "Epoch 00159: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 813us/sample - loss: 0.0535 - accuracy: 1.0000 - val_loss: 0.1695 - val_accuracy: 0.9231\n",
      "Epoch 160/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0268 - accuracy: 1.0000\n",
      "Epoch 00160: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 758us/sample - loss: 0.1412 - accuracy: 0.9375 - val_loss: 0.1335 - val_accuracy: 1.0000\n",
      "Epoch 161/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.5293 - accuracy: 0.9000\n",
      "Epoch 00161: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 767us/sample - loss: 0.1767 - accuracy: 0.9375 - val_loss: 0.2192 - val_accuracy: 0.8462\n",
      "Epoch 162/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0210 - accuracy: 1.0000\n",
      "Epoch 00162: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 755us/sample - loss: 0.1460 - accuracy: 0.8958 - val_loss: 0.1251 - val_accuracy: 1.0000\n",
      "Epoch 163/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2350 - accuracy: 0.9000\n",
      "Epoch 00163: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 788us/sample - loss: 0.1061 - accuracy: 0.9583 - val_loss: 0.1665 - val_accuracy: 0.9231\n",
      "Epoch 164/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0426 - accuracy: 1.0000\n",
      "Epoch 00164: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 766us/sample - loss: 0.1023 - accuracy: 0.9583 - val_loss: 0.1245 - val_accuracy: 1.0000\n",
      "Epoch 165/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0255 - accuracy: 1.0000\n",
      "Epoch 00165: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 793us/sample - loss: 0.1370 - accuracy: 0.9375 - val_loss: 0.1260 - val_accuracy: 1.0000\n",
      "Epoch 166/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0314 - accuracy: 1.0000\n",
      "Epoch 00166: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 822us/sample - loss: 0.1171 - accuracy: 0.9167 - val_loss: 0.1548 - val_accuracy: 0.9231\n",
      "Epoch 167/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1788 - accuracy: 0.9000\n",
      "Epoch 00167: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 752us/sample - loss: 0.1003 - accuracy: 0.9375 - val_loss: 0.1239 - val_accuracy: 0.9231\n",
      "Epoch 168/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 00168: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 740us/sample - loss: 0.0748 - accuracy: 0.9583 - val_loss: 0.1486 - val_accuracy: 1.0000\n",
      "Epoch 169/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0621 - accuracy: 1.0000\n",
      "Epoch 00169: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 728us/sample - loss: 0.0903 - accuracy: 0.9792 - val_loss: 0.1565 - val_accuracy: 0.9231\n",
      "Epoch 170/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 00170: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 776us/sample - loss: 0.0915 - accuracy: 0.9583 - val_loss: 0.1264 - val_accuracy: 1.0000\n",
      "Epoch 171/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0125 - accuracy: 1.0000\n",
      "Epoch 00171: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 739us/sample - loss: 0.1497 - accuracy: 0.9167 - val_loss: 0.2132 - val_accuracy: 0.8462\n",
      "Epoch 172/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 00172: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 758us/sample - loss: 0.1325 - accuracy: 0.9167 - val_loss: 0.1477 - val_accuracy: 1.0000\n",
      "Epoch 173/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1447 - accuracy: 0.9000\n",
      "Epoch 00173: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 821us/sample - loss: 0.0842 - accuracy: 0.9583 - val_loss: 0.1263 - val_accuracy: 1.0000\n",
      "Epoch 174/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2658 - accuracy: 0.8000\n",
      "Epoch 00174: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 700us/sample - loss: 0.1064 - accuracy: 0.9375 - val_loss: 0.1599 - val_accuracy: 0.9231\n",
      "Epoch 175/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0654 - accuracy: 1.0000\n",
      "Epoch 00175: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 723us/sample - loss: 0.1135 - accuracy: 0.9583 - val_loss: 0.2125 - val_accuracy: 0.8462\n",
      "Epoch 176/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.3389 - accuracy: 0.8000\n",
      "Epoch 00176: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 700us/sample - loss: 0.1827 - accuracy: 0.9375 - val_loss: 0.1239 - val_accuracy: 1.0000\n",
      "Epoch 177/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1922 - accuracy: 0.9000\n",
      "Epoch 00177: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 735us/sample - loss: 0.1251 - accuracy: 0.9375 - val_loss: 0.1655 - val_accuracy: 0.8462\n",
      "Epoch 178/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0908 - accuracy: 1.0000\n",
      "Epoch 00178: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 713us/sample - loss: 0.0634 - accuracy: 1.0000 - val_loss: 0.1326 - val_accuracy: 0.9231\n",
      "Epoch 179/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1077 - accuracy: 1.0000\n",
      "Epoch 00179: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 712us/sample - loss: 0.1050 - accuracy: 0.9583 - val_loss: 0.1830 - val_accuracy: 0.8462\n",
      "Epoch 180/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1848 - accuracy: 0.9000\n",
      "Epoch 00180: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 813us/sample - loss: 0.2395 - accuracy: 0.9167 - val_loss: 0.1451 - val_accuracy: 1.0000\n",
      "Epoch 181/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1724 - accuracy: 0.9000\n",
      "Epoch 00181: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 801us/sample - loss: 0.3464 - accuracy: 0.8542 - val_loss: 0.1679 - val_accuracy: 0.8462\n",
      "Epoch 182/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.3972 - accuracy: 0.9000\n",
      "Epoch 00182: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 717us/sample - loss: 0.2106 - accuracy: 0.9167 - val_loss: 0.2743 - val_accuracy: 0.8462\n",
      "Epoch 183/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0898 - accuracy: 1.0000\n",
      "Epoch 00183: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 707us/sample - loss: 0.0548 - accuracy: 1.0000 - val_loss: 0.1330 - val_accuracy: 1.0000\n",
      "Epoch 184/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0977 - accuracy: 1.0000\n",
      "Epoch 00184: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 712us/sample - loss: 0.2328 - accuracy: 0.9583 - val_loss: 0.1531 - val_accuracy: 0.9231\n",
      "Epoch 185/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0807 - accuracy: 0.9000\n",
      "Epoch 00185: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 718us/sample - loss: 0.1232 - accuracy: 0.9167 - val_loss: 0.1899 - val_accuracy: 0.8462\n",
      "Epoch 186/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0802 - accuracy: 1.0000\n",
      "Epoch 00186: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 727us/sample - loss: 0.0756 - accuracy: 0.9792 - val_loss: 0.1271 - val_accuracy: 1.0000\n",
      "Epoch 187/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.3379 - accuracy: 0.9000\n",
      "Epoch 00187: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 726us/sample - loss: 0.1139 - accuracy: 0.9792 - val_loss: 0.1969 - val_accuracy: 0.8462\n",
      "Epoch 188/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2104 - accuracy: 0.9000\n",
      "Epoch 00188: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 720us/sample - loss: 0.0957 - accuracy: 0.9583 - val_loss: 0.1501 - val_accuracy: 1.0000\n",
      "Epoch 189/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0720 - accuracy: 1.0000\n",
      "Epoch 00189: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 723us/sample - loss: 0.0899 - accuracy: 0.9792 - val_loss: 0.1538 - val_accuracy: 1.0000\n",
      "Epoch 190/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0167 - accuracy: 1.0000\n",
      "Epoch 00190: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 776us/sample - loss: 0.0705 - accuracy: 0.9375 - val_loss: 0.1410 - val_accuracy: 1.0000\n",
      "Epoch 191/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0982 - accuracy: 1.0000\n",
      "Epoch 00191: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 724us/sample - loss: 0.0786 - accuracy: 0.9792 - val_loss: 0.1193 - val_accuracy: 1.0000\n",
      "Epoch 192/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0253 - accuracy: 1.0000\n",
      "Epoch 00192: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 751us/sample - loss: 0.0500 - accuracy: 0.9792 - val_loss: 0.1999 - val_accuracy: 0.8462\n",
      "Epoch 193/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0567 - accuracy: 1.0000\n",
      "Epoch 00193: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 751us/sample - loss: 0.0673 - accuracy: 0.9792 - val_loss: 0.2332 - val_accuracy: 0.8462\n",
      "Epoch 194/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 00194: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 747us/sample - loss: 0.0586 - accuracy: 0.9792 - val_loss: 0.1320 - val_accuracy: 1.0000\n",
      "Epoch 195/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0178 - accuracy: 1.0000\n",
      "Epoch 00195: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 835us/sample - loss: 0.0573 - accuracy: 0.9792 - val_loss: 0.1151 - val_accuracy: 1.0000\n",
      "Epoch 196/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0155 - accuracy: 1.0000\n",
      "Epoch 00196: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 860us/sample - loss: 0.1106 - accuracy: 0.9375 - val_loss: 0.1523 - val_accuracy: 0.9231\n",
      "Epoch 197/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2013 - accuracy: 0.8000\n",
      "Epoch 00197: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 736us/sample - loss: 0.0689 - accuracy: 0.9375 - val_loss: 0.1427 - val_accuracy: 0.9231\n",
      "Epoch 198/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1598 - accuracy: 0.9000\n",
      "Epoch 00198: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 851us/sample - loss: 0.0759 - accuracy: 0.9583 - val_loss: 0.1282 - val_accuracy: 1.0000\n",
      "Epoch 199/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0235 - accuracy: 1.0000\n",
      "Epoch 00199: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 725us/sample - loss: 0.0630 - accuracy: 0.9792 - val_loss: 0.1728 - val_accuracy: 0.9231\n",
      "Epoch 200/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0398 - accuracy: 1.0000\n",
      "Epoch 00200: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 810us/sample - loss: 0.0638 - accuracy: 0.9792 - val_loss: 0.1199 - val_accuracy: 1.0000\n",
      "Epoch 201/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0142 - accuracy: 1.0000\n",
      "Epoch 00201: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 818us/sample - loss: 0.1130 - accuracy: 0.9583 - val_loss: 0.1204 - val_accuracy: 1.0000\n",
      "Epoch 202/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0354 - accuracy: 1.0000\n",
      "Epoch 00202: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 795us/sample - loss: 0.0797 - accuracy: 0.9583 - val_loss: 0.1412 - val_accuracy: 0.9231\n",
      "Epoch 203/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0201 - accuracy: 1.0000\n",
      "Epoch 00203: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 833us/sample - loss: 0.1150 - accuracy: 0.9583 - val_loss: 0.1146 - val_accuracy: 1.0000\n",
      "Epoch 204/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0474 - accuracy: 1.0000\n",
      "Epoch 00204: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 916us/sample - loss: 0.1553 - accuracy: 0.9375 - val_loss: 0.1056 - val_accuracy: 1.0000\n",
      "Epoch 205/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2784 - accuracy: 0.9000\n",
      "Epoch 00205: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 852us/sample - loss: 0.1402 - accuracy: 0.9583 - val_loss: 0.2137 - val_accuracy: 0.8462\n",
      "Epoch 206/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1888 - accuracy: 0.9000\n",
      "Epoch 00206: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 833us/sample - loss: 0.1189 - accuracy: 0.9167 - val_loss: 0.0971 - val_accuracy: 1.0000\n",
      "Epoch 207/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1488 - accuracy: 0.9000\n",
      "Epoch 00207: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 787us/sample - loss: 0.0601 - accuracy: 0.9792 - val_loss: 0.1667 - val_accuracy: 0.8462\n",
      "Epoch 208/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1325 - accuracy: 0.9000\n",
      "Epoch 00208: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 797us/sample - loss: 0.0779 - accuracy: 0.9583 - val_loss: 0.0944 - val_accuracy: 1.0000\n",
      "Epoch 209/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 00209: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 782us/sample - loss: 0.1150 - accuracy: 0.9583 - val_loss: 0.1057 - val_accuracy: 1.0000\n",
      "Epoch 210/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 00210: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 792us/sample - loss: 0.0936 - accuracy: 0.9583 - val_loss: 0.1138 - val_accuracy: 1.0000\n",
      "Epoch 211/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0217 - accuracy: 1.0000\n",
      "Epoch 00211: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 825us/sample - loss: 0.1500 - accuracy: 0.9583 - val_loss: 0.0993 - val_accuracy: 1.0000\n",
      "Epoch 212/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0511 - accuracy: 1.0000\n",
      "Epoch 00212: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 807us/sample - loss: 0.0554 - accuracy: 0.9792 - val_loss: 0.1490 - val_accuracy: 0.9231\n",
      "Epoch 213/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 00213: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 780us/sample - loss: 0.0351 - accuracy: 1.0000 - val_loss: 0.1008 - val_accuracy: 1.0000\n",
      "Epoch 214/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0282 - accuracy: 1.0000\n",
      "Epoch 00214: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 843us/sample - loss: 0.0899 - accuracy: 0.9583 - val_loss: 0.1010 - val_accuracy: 1.0000\n",
      "Epoch 215/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0393 - accuracy: 1.0000\n",
      "Epoch 00215: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 854us/sample - loss: 0.0488 - accuracy: 0.9792 - val_loss: 0.1600 - val_accuracy: 0.9231\n",
      "Epoch 216/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0316 - accuracy: 1.0000\n",
      "Epoch 00216: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 802us/sample - loss: 0.0959 - accuracy: 0.9583 - val_loss: 0.1093 - val_accuracy: 1.0000\n",
      "Epoch 217/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0103 - accuracy: 1.0000\n",
      "Epoch 00217: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 849us/sample - loss: 0.0473 - accuracy: 0.9792 - val_loss: 0.1662 - val_accuracy: 0.9231\n",
      "Epoch 218/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0428 - accuracy: 1.0000\n",
      "Epoch 00218: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 796us/sample - loss: 0.0962 - accuracy: 0.9583 - val_loss: 0.1260 - val_accuracy: 1.0000\n",
      "Epoch 219/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0816 - accuracy: 1.0000\n",
      "Epoch 00219: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 818us/sample - loss: 0.1388 - accuracy: 0.9792 - val_loss: 0.1031 - val_accuracy: 1.0000\n",
      "Epoch 220/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2244 - accuracy: 0.9000\n",
      "Epoch 00220: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 806us/sample - loss: 0.1388 - accuracy: 0.9167 - val_loss: 0.1625 - val_accuracy: 0.8462\n",
      "Epoch 221/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0306 - accuracy: 1.0000\n",
      "Epoch 00221: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 949us/sample - loss: 0.0640 - accuracy: 0.9792 - val_loss: 0.0870 - val_accuracy: 1.0000\n",
      "Epoch 222/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 00222: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 882us/sample - loss: 0.0916 - accuracy: 0.9583 - val_loss: 0.1162 - val_accuracy: 1.0000\n",
      "Epoch 223/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0157 - accuracy: 1.0000\n",
      "Epoch 00223: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 932us/sample - loss: 0.0720 - accuracy: 0.9583 - val_loss: 0.1298 - val_accuracy: 0.9231\n",
      "Epoch 224/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 00224: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 949us/sample - loss: 0.0544 - accuracy: 0.9792 - val_loss: 0.0812 - val_accuracy: 1.0000\n",
      "Epoch 225/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1211 - accuracy: 1.0000\n",
      "Epoch 00225: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 981us/sample - loss: 0.0643 - accuracy: 1.0000 - val_loss: 0.1957 - val_accuracy: 0.8462\n",
      "Epoch 226/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1968 - accuracy: 0.8000\n",
      "Epoch 00226: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 818us/sample - loss: 0.1004 - accuracy: 0.9167 - val_loss: 0.0790 - val_accuracy: 1.0000\n",
      "Epoch 227/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.3377 - accuracy: 0.9000\n",
      "Epoch 00227: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 933us/sample - loss: 0.1179 - accuracy: 0.9583 - val_loss: 0.1353 - val_accuracy: 1.0000\n",
      "Epoch 228/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 00228: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 993us/sample - loss: 0.0624 - accuracy: 0.9583 - val_loss: 0.1077 - val_accuracy: 1.0000\n",
      "Epoch 229/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 00229: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 824us/sample - loss: 0.0687 - accuracy: 0.9583 - val_loss: 0.0870 - val_accuracy: 1.0000\n",
      "Epoch 230/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0140 - accuracy: 1.0000\n",
      "Epoch 00230: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 834us/sample - loss: 0.0865 - accuracy: 0.9583 - val_loss: 0.1626 - val_accuracy: 0.8462\n",
      "Epoch 231/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 00231: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 765us/sample - loss: 0.1584 - accuracy: 0.9583 - val_loss: 0.0841 - val_accuracy: 1.0000\n",
      "Epoch 232/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2250 - accuracy: 0.9000\n",
      "Epoch 00232: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 742us/sample - loss: 0.1943 - accuracy: 0.9167 - val_loss: 0.1269 - val_accuracy: 1.0000\n",
      "Epoch 233/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1128 - accuracy: 0.9000\n",
      "Epoch 00233: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 817us/sample - loss: 0.2774 - accuracy: 0.8750 - val_loss: 0.1067 - val_accuracy: 1.0000\n",
      "Epoch 234/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 00234: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 839us/sample - loss: 0.0986 - accuracy: 0.9583 - val_loss: 0.1256 - val_accuracy: 1.0000\n",
      "Epoch 235/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 00235: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 814us/sample - loss: 0.0623 - accuracy: 0.9792 - val_loss: 0.1004 - val_accuracy: 1.0000\n",
      "Epoch 236/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 00236: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 753us/sample - loss: 0.0506 - accuracy: 0.9792 - val_loss: 0.1964 - val_accuracy: 0.9231\n",
      "Epoch 237/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0542 - accuracy: 1.0000\n",
      "Epoch 00237: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 849us/sample - loss: 0.0925 - accuracy: 0.9583 - val_loss: 0.0915 - val_accuracy: 1.0000\n",
      "Epoch 238/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0971 - accuracy: 1.0000\n",
      "Epoch 00238: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0623 - accuracy: 0.9792 - val_loss: 0.0836 - val_accuracy: 1.0000\n",
      "Epoch 239/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.3617 - accuracy: 0.8000\n",
      "Epoch 00239: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 950us/sample - loss: 0.1217 - accuracy: 0.9375 - val_loss: 0.2475 - val_accuracy: 0.7692\n",
      "Epoch 240/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.3960 - accuracy: 0.9000\n",
      "Epoch 00240: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 834us/sample - loss: 0.1661 - accuracy: 0.9167 - val_loss: 0.0873 - val_accuracy: 1.0000\n",
      "Epoch 241/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0572 - accuracy: 1.0000\n",
      "Epoch 00241: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 830us/sample - loss: 0.0711 - accuracy: 0.9583 - val_loss: 0.1105 - val_accuracy: 0.9231\n",
      "Epoch 242/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0421 - accuracy: 1.0000\n",
      "Epoch 00242: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 757us/sample - loss: 0.0563 - accuracy: 0.9792 - val_loss: 0.1011 - val_accuracy: 1.0000\n",
      "Epoch 243/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0840 - accuracy: 1.0000\n",
      "Epoch 00243: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 783us/sample - loss: 0.0573 - accuracy: 0.9792 - val_loss: 0.0871 - val_accuracy: 1.0000\n",
      "Epoch 244/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0267 - accuracy: 1.0000\n",
      "Epoch 00244: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0956 - accuracy: 0.9583 - val_loss: 0.0967 - val_accuracy: 1.0000\n",
      "Epoch 245/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1669 - accuracy: 0.9000\n",
      "Epoch 00245: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 780us/sample - loss: 0.0527 - accuracy: 0.9792 - val_loss: 0.1020 - val_accuracy: 0.9231\n",
      "Epoch 246/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0200 - accuracy: 1.0000\n",
      "Epoch 00246: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 783us/sample - loss: 0.0735 - accuracy: 0.9583 - val_loss: 0.0776 - val_accuracy: 1.0000\n",
      "Epoch 247/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0758 - accuracy: 1.0000\n",
      "Epoch 00247: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 992us/sample - loss: 0.1044 - accuracy: 0.9792 - val_loss: 0.1023 - val_accuracy: 1.0000\n",
      "Epoch 248/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0120 - accuracy: 1.0000\n",
      "Epoch 00248: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 837us/sample - loss: 0.1038 - accuracy: 0.9375 - val_loss: 0.0900 - val_accuracy: 1.0000\n",
      "Epoch 249/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0266 - accuracy: 1.0000\n",
      "Epoch 00249: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 768us/sample - loss: 0.1264 - accuracy: 0.9583 - val_loss: 0.0842 - val_accuracy: 1.0000\n",
      "Epoch 250/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0180 - accuracy: 1.0000\n",
      "Epoch 00250: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 875us/sample - loss: 0.0423 - accuracy: 0.9792 - val_loss: 0.1557 - val_accuracy: 0.9231\n",
      "Epoch 251/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0140 - accuracy: 1.0000\n",
      "Epoch 00251: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 968us/sample - loss: 0.0980 - accuracy: 0.9583 - val_loss: 0.0794 - val_accuracy: 1.0000\n",
      "Epoch 252/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 2.5462e-04 - accuracy: 1.0000\n",
      "Epoch 00252: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 927us/sample - loss: 0.0480 - accuracy: 0.9792 - val_loss: 0.0750 - val_accuracy: 1.0000\n",
      "Epoch 253/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 00253: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 871us/sample - loss: 0.0307 - accuracy: 1.0000 - val_loss: 0.1076 - val_accuracy: 0.9231\n",
      "Epoch 254/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0699 - accuracy: 1.0000\n",
      "Epoch 00254: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 813us/sample - loss: 0.0802 - accuracy: 0.9792 - val_loss: 0.0874 - val_accuracy: 1.0000\n",
      "Epoch 255/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0176 - accuracy: 1.0000\n",
      "Epoch 00255: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 883us/sample - loss: 0.1133 - accuracy: 0.9375 - val_loss: 0.0746 - val_accuracy: 1.0000\n",
      "Epoch 256/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0429 - accuracy: 1.0000\n",
      "Epoch 00256: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 869us/sample - loss: 0.0416 - accuracy: 0.9792 - val_loss: 0.3074 - val_accuracy: 0.7692\n",
      "Epoch 257/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2176 - accuracy: 0.9000\n",
      "Epoch 00257: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 848us/sample - loss: 0.1779 - accuracy: 0.8958 - val_loss: 0.0802 - val_accuracy: 1.0000\n",
      "Epoch 258/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0231 - accuracy: 1.0000\n",
      "Epoch 00258: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 828us/sample - loss: 0.1625 - accuracy: 0.8958 - val_loss: 0.0904 - val_accuracy: 1.0000\n",
      "Epoch 259/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0216 - accuracy: 1.0000\n",
      "Epoch 00259: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 857us/sample - loss: 0.1511 - accuracy: 0.9375 - val_loss: 0.1391 - val_accuracy: 0.9231\n",
      "Epoch 260/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1173 - accuracy: 0.9000\n",
      "Epoch 00260: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 800us/sample - loss: 0.1600 - accuracy: 0.9375 - val_loss: 0.0764 - val_accuracy: 1.0000\n",
      "Epoch 261/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0379 - accuracy: 1.0000\n",
      "Epoch 00261: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 810us/sample - loss: 0.0501 - accuracy: 0.9792 - val_loss: 0.0856 - val_accuracy: 1.0000\n",
      "Epoch 262/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1767 - accuracy: 0.9000\n",
      "Epoch 00262: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 758us/sample - loss: 0.1174 - accuracy: 0.9583 - val_loss: 0.1270 - val_accuracy: 0.9231\n",
      "Epoch 263/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0712 - accuracy: 1.0000\n",
      "Epoch 00263: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 829us/sample - loss: 0.0708 - accuracy: 0.9792 - val_loss: 0.0788 - val_accuracy: 1.0000\n",
      "Epoch 264/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0983 - accuracy: 0.9000\n",
      "Epoch 00264: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 867us/sample - loss: 0.0679 - accuracy: 0.9583 - val_loss: 0.0904 - val_accuracy: 1.0000\n",
      "Epoch 265/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 00265: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 824us/sample - loss: 0.0983 - accuracy: 0.9792 - val_loss: 0.1670 - val_accuracy: 0.9231\n",
      "Epoch 266/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0572 - accuracy: 1.0000\n",
      "Epoch 00266: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 843us/sample - loss: 0.0321 - accuracy: 1.0000 - val_loss: 0.0698 - val_accuracy: 1.0000\n",
      "Epoch 267/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0152 - accuracy: 1.0000\n",
      "Epoch 00267: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 893us/sample - loss: 0.0814 - accuracy: 0.9792 - val_loss: 0.1488 - val_accuracy: 0.9231\n",
      "Epoch 268/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0898 - accuracy: 1.0000\n",
      "Epoch 00268: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 806us/sample - loss: 0.0918 - accuracy: 0.9792 - val_loss: 0.1375 - val_accuracy: 1.0000\n",
      "Epoch 269/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0170 - accuracy: 1.0000\n",
      "Epoch 00269: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 760us/sample - loss: 0.0817 - accuracy: 0.9583 - val_loss: 0.0851 - val_accuracy: 1.0000\n",
      "Epoch 270/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0295 - accuracy: 1.0000\n",
      "Epoch 00270: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 814us/sample - loss: 0.0786 - accuracy: 0.9792 - val_loss: 0.1589 - val_accuracy: 0.9231\n",
      "Epoch 271/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1129 - accuracy: 1.0000\n",
      "Epoch 00271: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 769us/sample - loss: 0.0525 - accuracy: 0.9792 - val_loss: 0.1007 - val_accuracy: 1.0000\n",
      "Epoch 272/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0319 - accuracy: 1.0000\n",
      "Epoch 00272: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 787us/sample - loss: 0.0392 - accuracy: 1.0000 - val_loss: 0.0812 - val_accuracy: 1.0000\n",
      "Epoch 273/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0525 - accuracy: 1.0000\n",
      "Epoch 00273: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 787us/sample - loss: 0.0811 - accuracy: 0.9792 - val_loss: 0.0737 - val_accuracy: 1.0000\n",
      "Epoch 274/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 00274: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 794us/sample - loss: 0.0331 - accuracy: 0.9792 - val_loss: 0.1330 - val_accuracy: 0.9231\n",
      "Epoch 275/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0418 - accuracy: 1.0000\n",
      "Epoch 00275: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 827us/sample - loss: 0.0570 - accuracy: 0.9792 - val_loss: 0.0577 - val_accuracy: 1.0000\n",
      "Epoch 276/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 00276: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 806us/sample - loss: 0.0932 - accuracy: 0.9583 - val_loss: 0.0697 - val_accuracy: 1.0000\n",
      "Epoch 277/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0139 - accuracy: 1.0000\n",
      "Epoch 00277: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 860us/sample - loss: 0.0309 - accuracy: 1.0000 - val_loss: 0.0953 - val_accuracy: 0.9231\n",
      "Epoch 278/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0362 - accuracy: 1.0000\n",
      "Epoch 00278: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 850us/sample - loss: 0.0443 - accuracy: 0.9792 - val_loss: 0.0566 - val_accuracy: 1.0000\n",
      "Epoch 279/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0263 - accuracy: 1.0000\n",
      "Epoch 00279: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 919us/sample - loss: 0.0619 - accuracy: 0.9583 - val_loss: 0.1032 - val_accuracy: 0.9231\n",
      "Epoch 280/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1691 - accuracy: 0.9000\n",
      "Epoch 00280: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 889us/sample - loss: 0.0817 - accuracy: 0.9583 - val_loss: 0.0545 - val_accuracy: 1.0000\n",
      "Epoch 281/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 00281: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 877us/sample - loss: 0.0351 - accuracy: 1.0000 - val_loss: 0.0687 - val_accuracy: 1.0000\n",
      "Epoch 282/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0393 - accuracy: 1.0000\n",
      "Epoch 00282: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 877us/sample - loss: 0.0482 - accuracy: 0.9792 - val_loss: 0.0687 - val_accuracy: 1.0000\n",
      "Epoch 283/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 00283: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 937us/sample - loss: 0.0316 - accuracy: 1.0000 - val_loss: 0.0513 - val_accuracy: 1.0000\n",
      "Epoch 284/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 6.2080e-04 - accuracy: 1.0000\n",
      "Epoch 00284: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 880us/sample - loss: 0.0423 - accuracy: 0.9792 - val_loss: 0.1030 - val_accuracy: 0.9231\n",
      "Epoch 285/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0170 - accuracy: 1.0000\n",
      "Epoch 00285: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 885us/sample - loss: 0.0635 - accuracy: 0.9583 - val_loss: 0.0531 - val_accuracy: 1.0000\n",
      "Epoch 286/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0285 - accuracy: 1.0000\n",
      "Epoch 00286: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 871us/sample - loss: 0.0367 - accuracy: 1.0000 - val_loss: 0.0552 - val_accuracy: 1.0000\n",
      "Epoch 287/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1578 - accuracy: 0.9000\n",
      "Epoch 00287: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 915us/sample - loss: 0.1010 - accuracy: 0.9375 - val_loss: 0.1225 - val_accuracy: 0.9231\n",
      "Epoch 288/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0256 - accuracy: 1.0000\n",
      "Epoch 00288: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 744us/sample - loss: 0.0409 - accuracy: 1.0000 - val_loss: 0.0570 - val_accuracy: 1.0000\n",
      "Epoch 289/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 00289: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 780us/sample - loss: 0.0523 - accuracy: 0.9583 - val_loss: 0.1061 - val_accuracy: 0.9231\n",
      "Epoch 290/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 00290: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 790us/sample - loss: 0.0678 - accuracy: 0.9792 - val_loss: 0.0584 - val_accuracy: 1.0000\n",
      "Epoch 291/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0217 - accuracy: 1.0000\n",
      "Epoch 00291: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 836us/sample - loss: 0.1051 - accuracy: 0.9792 - val_loss: 0.0633 - val_accuracy: 1.0000\n",
      "Epoch 292/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0142 - accuracy: 1.0000\n",
      "Epoch 00292: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 850us/sample - loss: 0.1130 - accuracy: 0.9167 - val_loss: 0.1447 - val_accuracy: 0.9231\n",
      "Epoch 293/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 00293: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 808us/sample - loss: 0.0197 - accuracy: 1.0000 - val_loss: 0.0517 - val_accuracy: 1.0000\n",
      "Epoch 294/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 00294: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 834us/sample - loss: 0.0406 - accuracy: 1.0000 - val_loss: 0.0483 - val_accuracy: 1.0000\n",
      "Epoch 295/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0399 - accuracy: 1.0000\n",
      "Epoch 00295: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 965us/sample - loss: 0.0410 - accuracy: 0.9792 - val_loss: 0.1202 - val_accuracy: 0.9231\n",
      "Epoch 296/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0670 - accuracy: 1.0000\n",
      "Epoch 00296: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 898us/sample - loss: 0.0696 - accuracy: 0.9792 - val_loss: 0.0691 - val_accuracy: 1.0000\n",
      "Epoch 297/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 00297: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 848us/sample - loss: 0.0243 - accuracy: 1.0000 - val_loss: 0.0477 - val_accuracy: 1.0000\n",
      "Epoch 298/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0383 - accuracy: 1.0000\n",
      "Epoch 00298: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 899us/sample - loss: 0.0685 - accuracy: 0.9792 - val_loss: 0.0629 - val_accuracy: 1.0000\n",
      "Epoch 299/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0155 - accuracy: 1.0000\n",
      "Epoch 00299: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 863us/sample - loss: 0.0669 - accuracy: 0.9792 - val_loss: 0.0856 - val_accuracy: 0.9231\n",
      "Epoch 300/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0470 - accuracy: 1.0000\n",
      "Epoch 00300: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 863us/sample - loss: 0.0822 - accuracy: 0.9375 - val_loss: 0.0610 - val_accuracy: 1.0000\n",
      "Epoch 301/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0520 - accuracy: 1.0000\n",
      "Epoch 00301: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 824us/sample - loss: 0.2057 - accuracy: 0.9583 - val_loss: 0.1153 - val_accuracy: 0.9231\n",
      "Epoch 302/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1273 - accuracy: 1.0000\n",
      "Epoch 00302: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 894us/sample - loss: 0.0716 - accuracy: 0.9792 - val_loss: 0.1119 - val_accuracy: 0.9231\n",
      "Epoch 303/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1998 - accuracy: 0.9000\n",
      "Epoch 00303: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 829us/sample - loss: 0.0760 - accuracy: 0.9583 - val_loss: 0.0692 - val_accuracy: 1.0000\n",
      "Epoch 304/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0295 - accuracy: 1.0000\n",
      "Epoch 00304: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 817us/sample - loss: 0.0715 - accuracy: 0.9583 - val_loss: 0.0703 - val_accuracy: 1.0000\n",
      "Epoch 305/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0290 - accuracy: 1.0000\n",
      "Epoch 00305: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 732us/sample - loss: 0.0449 - accuracy: 1.0000 - val_loss: 0.1592 - val_accuracy: 0.9231\n",
      "Epoch 306/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0864 - accuracy: 1.0000\n",
      "Epoch 00306: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 866us/sample - loss: 0.0678 - accuracy: 0.9792 - val_loss: 0.0620 - val_accuracy: 1.0000\n",
      "Epoch 307/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0596 - accuracy: 1.0000\n",
      "Epoch 00307: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 764us/sample - loss: 0.1352 - accuracy: 0.9583 - val_loss: 0.0488 - val_accuracy: 1.0000\n",
      "Epoch 308/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 00308: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 850us/sample - loss: 0.1342 - accuracy: 0.9375 - val_loss: 0.1097 - val_accuracy: 0.9231\n",
      "Epoch 309/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0345 - accuracy: 1.0000\n",
      "Epoch 00309: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 803us/sample - loss: 0.0361 - accuracy: 1.0000 - val_loss: 0.0686 - val_accuracy: 1.0000\n",
      "Epoch 310/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 00310: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 850us/sample - loss: 0.1824 - accuracy: 0.9375 - val_loss: 0.0742 - val_accuracy: 1.0000\n",
      "Epoch 311/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 8.2126e-04 - accuracy: 1.0000\n",
      "Epoch 00311: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 839us/sample - loss: 0.1316 - accuracy: 0.9375 - val_loss: 0.0727 - val_accuracy: 1.0000\n",
      "Epoch 312/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0704 - accuracy: 1.0000\n",
      "Epoch 00312: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 841us/sample - loss: 0.0984 - accuracy: 0.9583 - val_loss: 0.0699 - val_accuracy: 1.0000\n",
      "Epoch 313/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.4170 - accuracy: 0.9000\n",
      "Epoch 00313: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 837us/sample - loss: 0.1353 - accuracy: 0.9583 - val_loss: 0.1522 - val_accuracy: 0.9231\n",
      "Epoch 314/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0600 - accuracy: 1.0000\n",
      "Epoch 00314: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 862us/sample - loss: 0.0649 - accuracy: 0.9792 - val_loss: 0.0566 - val_accuracy: 1.0000\n",
      "Epoch 315/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0196 - accuracy: 1.0000\n",
      "Epoch 00315: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 826us/sample - loss: 0.1192 - accuracy: 0.9583 - val_loss: 0.0595 - val_accuracy: 1.0000\n",
      "Epoch 316/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 5.2094e-04 - accuracy: 1.0000\n",
      "Epoch 00316: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 829us/sample - loss: 0.0950 - accuracy: 0.9583 - val_loss: 0.1373 - val_accuracy: 0.9231\n",
      "Epoch 317/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0648 - accuracy: 1.0000\n",
      "Epoch 00317: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 817us/sample - loss: 0.0596 - accuracy: 1.0000 - val_loss: 0.0732 - val_accuracy: 1.0000\n",
      "Epoch 318/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0296 - accuracy: 1.0000\n",
      "Epoch 00318: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 826us/sample - loss: 0.0482 - accuracy: 0.9792 - val_loss: 0.0701 - val_accuracy: 1.0000\n",
      "Epoch 319/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0242 - accuracy: 1.0000\n",
      "Epoch 00319: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 775us/sample - loss: 0.0328 - accuracy: 1.0000 - val_loss: 0.0573 - val_accuracy: 1.0000\n",
      "Epoch 320/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1561 - accuracy: 0.9000\n",
      "Epoch 00320: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 856us/sample - loss: 0.0981 - accuracy: 0.9375 - val_loss: 0.0878 - val_accuracy: 0.9231\n",
      "Epoch 321/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0363 - accuracy: 1.0000\n",
      "Epoch 00321: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 749us/sample - loss: 0.0227 - accuracy: 1.0000 - val_loss: 0.0582 - val_accuracy: 1.0000\n",
      "Epoch 322/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1068 - accuracy: 0.9000\n",
      "Epoch 00322: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 742us/sample - loss: 0.0900 - accuracy: 0.9583 - val_loss: 0.0897 - val_accuracy: 0.9231\n",
      "Epoch 323/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0289 - accuracy: 1.0000\n",
      "Epoch 00323: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 811us/sample - loss: 0.0593 - accuracy: 0.9792 - val_loss: 0.0673 - val_accuracy: 1.0000\n",
      "Epoch 324/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 00324: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 824us/sample - loss: 0.0380 - accuracy: 0.9792 - val_loss: 0.0651 - val_accuracy: 1.0000\n",
      "Epoch 325/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0528 - accuracy: 1.0000\n",
      "Epoch 00325: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 772us/sample - loss: 0.0516 - accuracy: 0.9792 - val_loss: 0.0923 - val_accuracy: 1.0000\n",
      "Epoch 326/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1041 - accuracy: 0.9000\n",
      "Epoch 00326: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 834us/sample - loss: 0.0788 - accuracy: 0.9583 - val_loss: 0.0802 - val_accuracy: 1.0000\n",
      "Epoch 327/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0223 - accuracy: 1.0000\n",
      "Epoch 00327: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 818us/sample - loss: 0.0509 - accuracy: 0.9792 - val_loss: 0.0703 - val_accuracy: 1.0000\n",
      "Epoch 328/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0183 - accuracy: 1.0000\n",
      "Epoch 00328: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 758us/sample - loss: 0.0407 - accuracy: 0.9792 - val_loss: 0.0769 - val_accuracy: 1.0000\n",
      "Epoch 329/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0543 - accuracy: 1.0000\n",
      "Epoch 00329: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 912us/sample - loss: 0.0503 - accuracy: 0.9792 - val_loss: 0.0777 - val_accuracy: 1.0000\n",
      "Epoch 330/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0835 - accuracy: 0.9000\n",
      "Epoch 00330: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 818us/sample - loss: 0.0315 - accuracy: 0.9792 - val_loss: 0.0702 - val_accuracy: 1.0000\n",
      "Epoch 331/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 6.4672e-04 - accuracy: 1.0000\n",
      "Epoch 00331: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 794us/sample - loss: 0.0229 - accuracy: 1.0000 - val_loss: 0.0673 - val_accuracy: 1.0000\n",
      "Epoch 332/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 00332: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 831us/sample - loss: 0.0396 - accuracy: 0.9792 - val_loss: 0.1214 - val_accuracy: 0.9231\n",
      "Epoch 333/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0783 - accuracy: 1.0000\n",
      "Epoch 00333: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 880us/sample - loss: 0.0453 - accuracy: 1.0000 - val_loss: 0.0682 - val_accuracy: 1.0000\n",
      "Epoch 334/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0469 - accuracy: 1.0000\n",
      "Epoch 00334: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 769us/sample - loss: 0.0685 - accuracy: 0.9792 - val_loss: 0.0616 - val_accuracy: 1.0000\n",
      "Epoch 335/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0179 - accuracy: 1.0000\n",
      "Epoch 00335: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 738us/sample - loss: 0.0130 - accuracy: 1.0000 - val_loss: 0.1263 - val_accuracy: 0.9231\n",
      "Epoch 336/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.9659e-04 - accuracy: 1.0000\n",
      "Epoch 00336: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 778us/sample - loss: 0.0612 - accuracy: 1.0000 - val_loss: 0.0613 - val_accuracy: 1.0000\n",
      "Epoch 337/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 00337: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 753us/sample - loss: 0.0826 - accuracy: 0.9583 - val_loss: 0.0646 - val_accuracy: 1.0000\n",
      "Epoch 338/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0161 - accuracy: 1.0000\n",
      "Epoch 00338: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 847us/sample - loss: 0.0156 - accuracy: 1.0000 - val_loss: 0.0702 - val_accuracy: 1.0000\n",
      "Epoch 339/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 00339: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 746us/sample - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.0633 - val_accuracy: 1.0000\n",
      "Epoch 340/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 00340: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 849us/sample - loss: 0.0373 - accuracy: 0.9792 - val_loss: 0.0760 - val_accuracy: 0.9231\n",
      "Epoch 341/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 00341: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 833us/sample - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.1196 - val_accuracy: 0.9231\n",
      "Epoch 342/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0281 - accuracy: 1.0000\n",
      "Epoch 00342: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 780us/sample - loss: 0.0275 - accuracy: 1.0000 - val_loss: 0.0614 - val_accuracy: 1.0000\n",
      "Epoch 343/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 00343: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 825us/sample - loss: 0.0383 - accuracy: 1.0000 - val_loss: 0.0583 - val_accuracy: 1.0000\n",
      "Epoch 344/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 7.0868e-04 - accuracy: 1.0000\n",
      "Epoch 00344: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 879us/sample - loss: 0.0388 - accuracy: 0.9792 - val_loss: 0.1359 - val_accuracy: 0.9231\n",
      "Epoch 345/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0835 - accuracy: 0.9000\n",
      "Epoch 00345: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 795us/sample - loss: 0.0579 - accuracy: 0.9583 - val_loss: 0.0489 - val_accuracy: 1.0000\n",
      "Epoch 346/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 00346: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 784us/sample - loss: 0.0482 - accuracy: 0.9792 - val_loss: 0.0483 - val_accuracy: 1.0000\n",
      "Epoch 347/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 00347: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 844us/sample - loss: 0.0162 - accuracy: 1.0000 - val_loss: 0.1205 - val_accuracy: 0.9231\n",
      "Epoch 348/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1594 - accuracy: 0.9000\n",
      "Epoch 00348: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 838us/sample - loss: 0.0676 - accuracy: 0.9583 - val_loss: 0.0383 - val_accuracy: 1.0000\n",
      "Epoch 349/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1516 - accuracy: 0.9000\n",
      "Epoch 00349: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 807us/sample - loss: 0.0399 - accuracy: 0.9792 - val_loss: 0.0425 - val_accuracy: 1.0000\n",
      "Epoch 350/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0757 - accuracy: 1.0000\n",
      "Epoch 00350: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 759us/sample - loss: 0.0248 - accuracy: 1.0000 - val_loss: 0.0392 - val_accuracy: 1.0000\n",
      "Epoch 351/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 00351: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 765us/sample - loss: 0.0237 - accuracy: 1.0000 - val_loss: 0.0707 - val_accuracy: 0.9231\n",
      "Epoch 352/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.5417e-04 - accuracy: 1.0000\n",
      "Epoch 00352: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 807us/sample - loss: 0.0246 - accuracy: 1.0000 - val_loss: 0.0611 - val_accuracy: 1.0000\n",
      "Epoch 353/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0276 - accuracy: 1.0000\n",
      "Epoch 00353: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 736us/sample - loss: 0.0185 - accuracy: 1.0000 - val_loss: 0.0427 - val_accuracy: 1.0000\n",
      "Epoch 354/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 00354: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 840us/sample - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.0387 - val_accuracy: 1.0000\n",
      "Epoch 355/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0290 - accuracy: 1.0000\n",
      "Epoch 00355: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 795us/sample - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.0548 - val_accuracy: 1.0000\n",
      "Epoch 356/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0139 - accuracy: 1.0000\n",
      "Epoch 00356: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 864us/sample - loss: 0.0151 - accuracy: 1.0000 - val_loss: 0.0648 - val_accuracy: 1.0000\n",
      "Epoch 357/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0307 - accuracy: 1.0000\n",
      "Epoch 00357: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 863us/sample - loss: 0.0153 - accuracy: 1.0000 - val_loss: 0.0440 - val_accuracy: 1.0000\n",
      "Epoch 358/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1624 - accuracy: 0.9000\n",
      "Epoch 00358: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 881us/sample - loss: 0.0571 - accuracy: 0.9583 - val_loss: 0.0662 - val_accuracy: 1.0000\n",
      "Epoch 359/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0742 - accuracy: 1.0000\n",
      "Epoch 00359: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 778us/sample - loss: 0.0502 - accuracy: 0.9792 - val_loss: 0.0872 - val_accuracy: 0.9231\n",
      "Epoch 360/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0666 - accuracy: 1.0000\n",
      "Epoch 00360: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 847us/sample - loss: 0.0371 - accuracy: 1.0000 - val_loss: 0.0548 - val_accuracy: 1.0000\n",
      "Epoch 361/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0496 - accuracy: 1.0000\n",
      "Epoch 00361: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 820us/sample - loss: 0.0251 - accuracy: 1.0000 - val_loss: 0.0350 - val_accuracy: 1.0000\n",
      "Epoch 362/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 00362: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 777us/sample - loss: 0.0344 - accuracy: 0.9792 - val_loss: 0.0422 - val_accuracy: 1.0000\n",
      "Epoch 363/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 00363: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 893us/sample - loss: 0.0307 - accuracy: 0.9792 - val_loss: 0.0684 - val_accuracy: 0.9231\n",
      "Epoch 364/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 00364: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 839us/sample - loss: 0.0139 - accuracy: 1.0000 - val_loss: 0.0350 - val_accuracy: 1.0000\n",
      "Epoch 365/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0140 - accuracy: 1.0000\n",
      "Epoch 00365: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 736us/sample - loss: 0.0355 - accuracy: 0.9792 - val_loss: 0.0679 - val_accuracy: 0.9231\n",
      "Epoch 366/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 00366: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 762us/sample - loss: 0.0215 - accuracy: 0.9792 - val_loss: 0.0541 - val_accuracy: 1.0000\n",
      "Epoch 367/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0392 - accuracy: 1.0000\n",
      "Epoch 00367: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 759us/sample - loss: 0.0280 - accuracy: 1.0000 - val_loss: 0.0383 - val_accuracy: 1.0000\n",
      "Epoch 368/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 4.5691e-04 - accuracy: 1.0000\n",
      "Epoch 00368: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 818us/sample - loss: 0.0217 - accuracy: 1.0000 - val_loss: 0.0321 - val_accuracy: 1.0000\n",
      "Epoch 369/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0175 - accuracy: 1.0000\n",
      "Epoch 00369: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 773us/sample - loss: 0.0151 - accuracy: 1.0000 - val_loss: 0.0620 - val_accuracy: 1.0000\n",
      "Epoch 370/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0078 - accuracy: 1.0000\n",
      "Epoch 00370: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 830us/sample - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.0815 - val_accuracy: 0.9231\n",
      "Epoch 371/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 00371: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 810us/sample - loss: 0.0291 - accuracy: 1.0000 - val_loss: 0.0364 - val_accuracy: 1.0000\n",
      "Epoch 372/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0197 - accuracy: 1.0000\n",
      "Epoch 00372: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 957us/sample - loss: 0.0937 - accuracy: 0.9583 - val_loss: 0.0368 - val_accuracy: 1.0000\n",
      "Epoch 373/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 00373: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 848us/sample - loss: 0.1532 - accuracy: 0.9167 - val_loss: 0.0427 - val_accuracy: 1.0000\n",
      "Epoch 374/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 00374: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 800us/sample - loss: 0.1567 - accuracy: 0.9375 - val_loss: 0.0509 - val_accuracy: 1.0000\n",
      "Epoch 375/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 6.9078e-04 - accuracy: 1.0000\n",
      "Epoch 00375: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 838us/sample - loss: 0.0278 - accuracy: 0.9792 - val_loss: 0.1585 - val_accuracy: 0.9231\n",
      "Epoch 376/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0820 - accuracy: 1.0000\n",
      "Epoch 00376: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 800us/sample - loss: 0.0375 - accuracy: 1.0000 - val_loss: 0.1444 - val_accuracy: 0.9231\n",
      "Epoch 377/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 00377: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 826us/sample - loss: 0.0248 - accuracy: 0.9792 - val_loss: 0.0535 - val_accuracy: 1.0000\n",
      "Epoch 378/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0172 - accuracy: 1.0000\n",
      "Epoch 00378: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 840us/sample - loss: 0.0165 - accuracy: 1.0000 - val_loss: 0.0663 - val_accuracy: 1.0000\n",
      "Epoch 379/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0306 - accuracy: 1.0000\n",
      "Epoch 00379: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 839us/sample - loss: 0.0141 - accuracy: 1.0000 - val_loss: 0.0495 - val_accuracy: 1.0000\n",
      "Epoch 380/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 00380: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 724us/sample - loss: 0.0441 - accuracy: 0.9792 - val_loss: 0.0585 - val_accuracy: 1.0000\n",
      "Epoch 381/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 3.1018e-04 - accuracy: 1.0000\n",
      "Epoch 00381: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 721us/sample - loss: 0.0217 - accuracy: 0.9792 - val_loss: 0.0763 - val_accuracy: 0.9231\n",
      "Epoch 382/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 00382: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 735us/sample - loss: 0.0217 - accuracy: 1.0000 - val_loss: 0.0447 - val_accuracy: 1.0000\n",
      "Epoch 383/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0306 - accuracy: 1.0000\n",
      "Epoch 00383: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 802us/sample - loss: 0.0163 - accuracy: 1.0000 - val_loss: 0.0575 - val_accuracy: 1.0000\n",
      "Epoch 384/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0112 - accuracy: 1.0000\n",
      "Epoch 00384: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 729us/sample - loss: 0.0209 - accuracy: 1.0000 - val_loss: 0.0659 - val_accuracy: 1.0000\n",
      "Epoch 385/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0188 - accuracy: 1.0000\n",
      "Epoch 00385: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 725us/sample - loss: 0.0437 - accuracy: 0.9583 - val_loss: 0.0560 - val_accuracy: 1.0000\n",
      "Epoch 386/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 7.0310e-04 - accuracy: 1.0000\n",
      "Epoch 00386: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 838us/sample - loss: 0.0566 - accuracy: 0.9583 - val_loss: 0.1867 - val_accuracy: 0.9231\n",
      "Epoch 387/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0118 - accuracy: 1.0000\n",
      "Epoch 00387: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 903us/sample - loss: 0.0247 - accuracy: 1.0000 - val_loss: 0.0812 - val_accuracy: 0.9231\n",
      "Epoch 388/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 00388: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 798us/sample - loss: 0.0262 - accuracy: 1.0000 - val_loss: 0.0458 - val_accuracy: 1.0000\n",
      "Epoch 389/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0913 - accuracy: 0.9000\n",
      "Epoch 00389: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 820us/sample - loss: 0.0225 - accuracy: 0.9792 - val_loss: 0.1000 - val_accuracy: 0.9231\n",
      "Epoch 390/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0115 - accuracy: 1.0000\n",
      "Epoch 00390: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 862us/sample - loss: 0.0241 - accuracy: 0.9792 - val_loss: 0.0749 - val_accuracy: 0.9231\n",
      "Epoch 391/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 00391: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 829us/sample - loss: 0.0390 - accuracy: 0.9792 - val_loss: 0.0660 - val_accuracy: 1.0000\n",
      "Epoch 392/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 00392: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 819us/sample - loss: 0.0211 - accuracy: 1.0000 - val_loss: 0.0612 - val_accuracy: 1.0000\n",
      "Epoch 393/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 00393: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 767us/sample - loss: 0.0176 - accuracy: 1.0000 - val_loss: 0.0647 - val_accuracy: 1.0000\n",
      "Epoch 394/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0132 - accuracy: 1.0000\n",
      "Epoch 00394: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 839us/sample - loss: 0.0208 - accuracy: 1.0000 - val_loss: 0.0529 - val_accuracy: 1.0000\n",
      "Epoch 395/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0311 - accuracy: 1.0000\n",
      "Epoch 00395: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 769us/sample - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.0389 - val_accuracy: 1.0000\n",
      "Epoch 396/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0078 - accuracy: 1.0000\n",
      "Epoch 00396: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 787us/sample - loss: 0.0256 - accuracy: 0.9792 - val_loss: 0.0469 - val_accuracy: 1.0000\n",
      "Epoch 397/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 00397: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 745us/sample - loss: 0.0456 - accuracy: 0.9583 - val_loss: 0.0499 - val_accuracy: 1.0000\n",
      "Epoch 398/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 00398: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 750us/sample - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.0438 - val_accuracy: 1.0000\n",
      "Epoch 399/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 00399: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 726us/sample - loss: 0.0172 - accuracy: 1.0000 - val_loss: 0.0687 - val_accuracy: 0.9231\n",
      "Epoch 400/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 00400: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 807us/sample - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.0790 - val_accuracy: 0.9231\n",
      "Epoch 401/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0139 - accuracy: 1.0000\n",
      "Epoch 00401: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 873us/sample - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.0534 - val_accuracy: 1.0000\n",
      "Epoch 402/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0452 - accuracy: 1.0000\n",
      "Epoch 00402: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 810us/sample - loss: 0.0317 - accuracy: 0.9792 - val_loss: 0.0444 - val_accuracy: 1.0000\n",
      "Epoch 403/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 00403: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 928us/sample - loss: 0.0152 - accuracy: 1.0000 - val_loss: 0.0694 - val_accuracy: 0.9231\n",
      "Epoch 404/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0091 - accuracy: 1.0000\n",
      "Epoch 00404: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 839us/sample - loss: 0.0259 - accuracy: 0.9792 - val_loss: 0.0812 - val_accuracy: 0.9231\n",
      "Epoch 405/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 00405: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 811us/sample - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.0641 - val_accuracy: 1.0000\n",
      "Epoch 406/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 00406: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 896us/sample - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.0628 - val_accuracy: 1.0000\n",
      "Epoch 407/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 00407: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 808us/sample - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.0740 - val_accuracy: 0.9231\n",
      "Epoch 408/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 00408: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 941us/sample - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.0851 - val_accuracy: 0.9231\n",
      "Epoch 409/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0321 - accuracy: 1.0000\n",
      "Epoch 00409: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 851us/sample - loss: 0.0192 - accuracy: 1.0000 - val_loss: 0.0735 - val_accuracy: 0.9231\n",
      "Epoch 410/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 5.4509e-04 - accuracy: 1.0000\n",
      "Epoch 00410: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 854us/sample - loss: 0.0239 - accuracy: 0.9792 - val_loss: 0.0611 - val_accuracy: 1.0000\n",
      "Epoch 411/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0326 - accuracy: 1.0000\n",
      "Epoch 00411: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 815us/sample - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.0413 - val_accuracy: 1.0000\n",
      "Epoch 412/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 00412: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 831us/sample - loss: 0.0232 - accuracy: 0.9792 - val_loss: 0.0760 - val_accuracy: 0.9231\n",
      "Epoch 413/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 00413: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 782us/sample - loss: 0.0122 - accuracy: 1.0000 - val_loss: 0.1066 - val_accuracy: 0.9231\n",
      "Epoch 414/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 00414: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 754us/sample - loss: 0.0370 - accuracy: 0.9792 - val_loss: 0.0326 - val_accuracy: 1.0000\n",
      "Epoch 415/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0631 - accuracy: 1.0000\n",
      "Epoch 00415: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 727us/sample - loss: 0.0482 - accuracy: 0.9792 - val_loss: 0.0406 - val_accuracy: 1.0000\n",
      "Epoch 416/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 00416: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 744us/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.1132 - val_accuracy: 0.9231\n",
      "Epoch 417/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0742 - accuracy: 0.9000\n",
      "Epoch 00417: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 727us/sample - loss: 0.0233 - accuracy: 0.9792 - val_loss: 0.0765 - val_accuracy: 0.9231\n",
      "Epoch 418/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0141 - accuracy: 1.0000\n",
      "Epoch 00418: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 798us/sample - loss: 0.0145 - accuracy: 1.0000 - val_loss: 0.0392 - val_accuracy: 1.0000\n",
      "Epoch 419/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 4.7727e-04 - accuracy: 1.0000\n",
      "Epoch 00419: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 770us/sample - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.0316 - val_accuracy: 1.0000\n",
      "Epoch 420/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 00420: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 784us/sample - loss: 0.0497 - accuracy: 0.9792 - val_loss: 0.1209 - val_accuracy: 0.9231\n",
      "Epoch 421/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1631 - accuracy: 0.9000\n",
      "Epoch 00421: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 831us/sample - loss: 0.0475 - accuracy: 0.9792 - val_loss: 0.0310 - val_accuracy: 1.0000\n",
      "Epoch 422/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 00422: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 835us/sample - loss: 0.0159 - accuracy: 1.0000 - val_loss: 0.0296 - val_accuracy: 1.0000\n",
      "Epoch 423/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 2.2552e-04 - accuracy: 1.0000\n",
      "Epoch 00423: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 781us/sample - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.0452 - val_accuracy: 1.0000\n",
      "Epoch 424/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0212 - accuracy: 1.0000\n",
      "Epoch 00424: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 840us/sample - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.0660 - val_accuracy: 0.9231\n",
      "Epoch 425/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 00425: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 802us/sample - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.0525 - val_accuracy: 1.0000\n",
      "Epoch 426/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0262 - accuracy: 1.0000\n",
      "Epoch 00426: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 886us/sample - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.0379 - val_accuracy: 1.0000\n",
      "Epoch 427/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 00427: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 779us/sample - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.0302 - val_accuracy: 1.0000\n",
      "Epoch 428/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 00428: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 840us/sample - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.0354 - val_accuracy: 1.0000\n",
      "Epoch 429/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 00429: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 817us/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0431 - val_accuracy: 1.0000\n",
      "Epoch 430/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 00430: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 853us/sample - loss: 0.0153 - accuracy: 1.0000 - val_loss: 0.0246 - val_accuracy: 1.0000\n",
      "Epoch 431/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 00431: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 939us/sample - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.0256 - val_accuracy: 1.0000\n",
      "Epoch 432/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 00432: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 763us/sample - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.0308 - val_accuracy: 1.0000\n",
      "Epoch 433/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 00433: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 866us/sample - loss: 0.0130 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 1.0000\n",
      "Epoch 434/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 00434: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 783us/sample - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.0361 - val_accuracy: 1.0000\n",
      "Epoch 435/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0145 - accuracy: 1.0000\n",
      "Epoch 00435: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 778us/sample - loss: 0.0173 - accuracy: 1.0000 - val_loss: 0.0475 - val_accuracy: 1.0000\n",
      "Epoch 436/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0103 - accuracy: 1.0000\n",
      "Epoch 00436: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 770us/sample - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.1486 - val_accuracy: 0.9231\n",
      "Epoch 437/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 3.8676e-04 - accuracy: 1.0000\n",
      "Epoch 00437: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 755us/sample - loss: 0.0493 - accuracy: 0.9792 - val_loss: 0.0289 - val_accuracy: 1.0000\n",
      "Epoch 438/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 00438: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 785us/sample - loss: 0.0625 - accuracy: 0.9792 - val_loss: 0.0278 - val_accuracy: 1.0000\n",
      "Epoch 439/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 00439: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 822us/sample - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.0803 - val_accuracy: 0.9231\n",
      "Epoch 440/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1521 - accuracy: 0.9000\n",
      "Epoch 00440: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 812us/sample - loss: 0.0398 - accuracy: 0.9792 - val_loss: 0.0320 - val_accuracy: 1.0000\n",
      "Epoch 441/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.5955e-04 - accuracy: 1.0000\n",
      "Epoch 00441: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 877us/sample - loss: 0.0262 - accuracy: 0.9792 - val_loss: 0.0623 - val_accuracy: 0.9231\n",
      "Epoch 442/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 7.0942e-04 - accuracy: 1.0000\n",
      "Epoch 00442: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 841us/sample - loss: 0.0165 - accuracy: 1.0000 - val_loss: 0.0811 - val_accuracy: 0.9231\n",
      "Epoch 443/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 00443: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 835us/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0447 - val_accuracy: 1.0000\n",
      "Epoch 444/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 00444: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 845us/sample - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.0335 - val_accuracy: 1.0000\n",
      "Epoch 445/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0867 - accuracy: 0.9000\n",
      "Epoch 00445: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 782us/sample - loss: 0.0193 - accuracy: 0.9792 - val_loss: 0.0830 - val_accuracy: 0.9231\n",
      "Epoch 446/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0671 - accuracy: 1.0000\n",
      "Epoch 00446: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 755us/sample - loss: 0.0195 - accuracy: 1.0000 - val_loss: 0.0432 - val_accuracy: 1.0000\n",
      "Epoch 447/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 00447: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 829us/sample - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.0325 - val_accuracy: 1.0000\n",
      "Epoch 448/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 3.3233e-05 - accuracy: 1.0000\n",
      "Epoch 00448: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 844us/sample - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.0301 - val_accuracy: 1.0000\n",
      "Epoch 449/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 00449: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 842us/sample - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0299 - val_accuracy: 1.0000\n",
      "Epoch 450/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 6.6985e-04 - accuracy: 1.0000\n",
      "Epoch 00450: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 775us/sample - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0281 - val_accuracy: 1.0000\n",
      "Epoch 451/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 7.3029e-04 - accuracy: 1.0000\n",
      "Epoch 00451: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 772us/sample - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.0356 - val_accuracy: 1.0000\n",
      "Epoch 452/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.8991e-04 - accuracy: 1.0000\n",
      "Epoch 00452: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 751us/sample - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.0661 - val_accuracy: 0.9231\n",
      "Epoch 453/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0316 - accuracy: 1.0000\n",
      "Epoch 00453: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 806us/sample - loss: 0.0237 - accuracy: 1.0000 - val_loss: 0.0245 - val_accuracy: 1.0000\n",
      "Epoch 454/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 00454: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 759us/sample - loss: 0.0164 - accuracy: 0.9792 - val_loss: 0.0229 - val_accuracy: 1.0000\n",
      "Epoch 455/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0194 - accuracy: 1.0000\n",
      "Epoch 00455: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 795us/sample - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.0343 - val_accuracy: 1.0000\n",
      "Epoch 456/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0241 - accuracy: 1.0000\n",
      "Epoch 00456: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 828us/sample - loss: 0.0139 - accuracy: 1.0000 - val_loss: 0.0527 - val_accuracy: 1.0000\n",
      "Epoch 457/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0138 - accuracy: 1.0000\n",
      "Epoch 00457: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 838us/sample - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.0270 - val_accuracy: 1.0000\n",
      "Epoch 458/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 00458: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 742us/sample - loss: 0.0269 - accuracy: 0.9792 - val_loss: 0.0272 - val_accuracy: 1.0000\n",
      "Epoch 459/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0518 - accuracy: 1.0000\n",
      "Epoch 00459: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 809us/sample - loss: 0.0168 - accuracy: 1.0000 - val_loss: 0.1295 - val_accuracy: 0.9231\n",
      "Epoch 460/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0216 - accuracy: 1.0000\n",
      "Epoch 00460: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 841us/sample - loss: 0.0444 - accuracy: 0.9792 - val_loss: 0.0273 - val_accuracy: 1.0000\n",
      "Epoch 461/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0283 - accuracy: 1.0000\n",
      "Epoch 00461: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 822us/sample - loss: 0.0228 - accuracy: 1.0000 - val_loss: 0.0260 - val_accuracy: 1.0000\n",
      "Epoch 462/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 2.6601e-04 - accuracy: 1.0000\n",
      "Epoch 00462: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 846us/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0445 - val_accuracy: 1.0000\n",
      "Epoch 463/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 00463: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 787us/sample - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0641 - val_accuracy: 0.9231\n",
      "Epoch 464/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 00464: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 767us/sample - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.0446 - val_accuracy: 1.0000\n",
      "Epoch 465/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0670 - accuracy: 1.0000\n",
      "Epoch 00465: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 830us/sample - loss: 0.0319 - accuracy: 1.0000 - val_loss: 0.0283 - val_accuracy: 1.0000\n",
      "Epoch 466/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 00466: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 831us/sample - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.0322 - val_accuracy: 1.0000\n",
      "Epoch 467/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 3.8487e-05 - accuracy: 1.0000\n",
      "Epoch 00467: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 742us/sample - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.0790 - val_accuracy: 0.9231\n",
      "Epoch 468/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 00468: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 766us/sample - loss: 0.0968 - accuracy: 0.9792 - val_loss: 0.0346 - val_accuracy: 1.0000\n",
      "Epoch 469/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0127 - accuracy: 1.0000\n",
      "Epoch 00469: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 767us/sample - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.0299 - val_accuracy: 1.0000\n",
      "Epoch 470/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 5.2936e-04 - accuracy: 1.0000\n",
      "Epoch 00470: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 770us/sample - loss: 0.0357 - accuracy: 0.9792 - val_loss: 0.0306 - val_accuracy: 1.0000\n",
      "Epoch 471/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 00471: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 759us/sample - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.2018 - val_accuracy: 0.9231\n",
      "Epoch 472/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0153 - accuracy: 1.0000\n",
      "Epoch 00472: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 742us/sample - loss: 0.0638 - accuracy: 0.9583 - val_loss: 0.0355 - val_accuracy: 1.0000\n",
      "Epoch 473/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 7.4317e-04 - accuracy: 1.0000\n",
      "Epoch 00473: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 834us/sample - loss: 0.0463 - accuracy: 0.9792 - val_loss: 0.0496 - val_accuracy: 1.0000\n",
      "Epoch 474/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 00474: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 783us/sample - loss: 0.0174 - accuracy: 1.0000 - val_loss: 0.0725 - val_accuracy: 0.9231\n",
      "Epoch 475/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 00475: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 840us/sample - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.0448 - val_accuracy: 1.0000\n",
      "Epoch 476/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.1786e-04 - accuracy: 1.0000\n",
      "Epoch 00476: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 839us/sample - loss: 0.0127 - accuracy: 1.0000 - val_loss: 0.0466 - val_accuracy: 1.0000\n",
      "Epoch 477/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0225 - accuracy: 1.0000\n",
      "Epoch 00477: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 740us/sample - loss: 0.0183 - accuracy: 1.0000 - val_loss: 0.0512 - val_accuracy: 1.0000\n",
      "Epoch 478/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 00478: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 826us/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0488 - val_accuracy: 1.0000\n",
      "Epoch 479/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 6.6596e-04 - accuracy: 1.0000\n",
      "Epoch 00479: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 801us/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0487 - val_accuracy: 1.0000\n",
      "Epoch 480/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0201 - accuracy: 1.0000\n",
      "Epoch 00480: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 774us/sample - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.0542 - val_accuracy: 1.0000\n",
      "Epoch 481/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0182 - accuracy: 1.0000\n",
      "Epoch 00481: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 860us/sample - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.0680 - val_accuracy: 0.9231\n",
      "Epoch 482/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 00482: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 816us/sample - loss: 0.0152 - accuracy: 1.0000 - val_loss: 0.0285 - val_accuracy: 1.0000\n",
      "Epoch 483/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 6.7289e-04 - accuracy: 1.0000\n",
      "Epoch 00483: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 750us/sample - loss: 0.0941 - accuracy: 0.9792 - val_loss: 0.0427 - val_accuracy: 1.0000\n",
      "Epoch 484/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0453 - accuracy: 1.0000\n",
      "Epoch 00484: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 744us/sample - loss: 0.0250 - accuracy: 1.0000 - val_loss: 0.1055 - val_accuracy: 0.9231\n",
      "Epoch 485/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 00485: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 767us/sample - loss: 0.0404 - accuracy: 0.9792 - val_loss: 0.0624 - val_accuracy: 1.0000\n",
      "Epoch 486/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0411 - accuracy: 1.0000\n",
      "Epoch 00486: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 756us/sample - loss: 0.1126 - accuracy: 0.9583 - val_loss: 0.0353 - val_accuracy: 1.0000\n",
      "Epoch 487/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 00487: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 751us/sample - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.0570 - val_accuracy: 1.0000\n",
      "Epoch 488/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 00488: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 744us/sample - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.0777 - val_accuracy: 0.9231\n",
      "Epoch 489/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.3975 - accuracy: 0.9000\n",
      "Epoch 00489: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 735us/sample - loss: 0.0978 - accuracy: 0.9792 - val_loss: 0.0400 - val_accuracy: 1.0000\n",
      "Epoch 490/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0403 - accuracy: 1.0000\n",
      "Epoch 00490: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 792us/sample - loss: 0.0215 - accuracy: 1.0000 - val_loss: 0.0405 - val_accuracy: 1.0000\n",
      "Epoch 491/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 00491: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 867us/sample - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.1072 - val_accuracy: 0.9231\n",
      "Epoch 492/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0775 - accuracy: 1.0000\n",
      "Epoch 00492: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 838us/sample - loss: 0.0455 - accuracy: 0.9792 - val_loss: 0.0415 - val_accuracy: 1.0000\n",
      "Epoch 493/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0145 - accuracy: 1.0000\n",
      "Epoch 00493: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 782us/sample - loss: 0.1390 - accuracy: 0.9375 - val_loss: 0.0371 - val_accuracy: 1.0000\n",
      "Epoch 494/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 2.7935e-04 - accuracy: 1.0000\n",
      "Epoch 00494: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 828us/sample - loss: 0.1508 - accuracy: 0.9375 - val_loss: 0.1015 - val_accuracy: 0.9231\n",
      "Epoch 495/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0575 - accuracy: 1.0000\n",
      "Epoch 00495: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 835us/sample - loss: 0.0292 - accuracy: 1.0000 - val_loss: 0.2979 - val_accuracy: 0.8462\n",
      "Epoch 496/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1583 - accuracy: 0.9000\n",
      "Epoch 00496: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 799us/sample - loss: 0.1048 - accuracy: 0.9583 - val_loss: 0.0326 - val_accuracy: 1.0000\n",
      "Epoch 497/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 00497: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 804us/sample - loss: 0.1508 - accuracy: 0.9167 - val_loss: 0.0862 - val_accuracy: 0.9231\n",
      "Epoch 498/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 00498: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 815us/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0343 - val_accuracy: 1.0000\n",
      "Epoch 499/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0124 - accuracy: 1.0000\n",
      "Epoch 00499: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 765us/sample - loss: 0.0127 - accuracy: 1.0000 - val_loss: 0.0322 - val_accuracy: 1.0000\n",
      "Epoch 500/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 7.2099e-04 - accuracy: 1.0000\n",
      "Epoch 00500: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 842us/sample - loss: 0.0543 - accuracy: 0.9792 - val_loss: 0.0740 - val_accuracy: 0.9231\n",
      "Training completed in time:  0:00:20.444212\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "num_epochs = 500\n",
    "num_batch_size = 10\n",
    "\n",
    "os.makedirs('models', exist_ok=True)\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        filepath='models/' + MODEL_NAME + '_{epoch:02d}.h5',\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_accuracy` score has improved.\n",
    "        save_best_only=True,\n",
    "        monitor='val_accuracy',\n",
    "        verbose=1)\n",
    "]\n",
    "start = datetime.now()\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs,\n",
    "                    validation_data=(x_val, y_val), callbacks=callbacks, verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot accuracies and losses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "diDQglrhSR1n"
   },
   "source": [
    "## Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Qyg7uylzSR1p",
    "outputId": "4b953e0d-5437-422d-cbc5-6b19a29879f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  1.0\n",
      "Testing Accuracy:  0.54545456\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Training Accuracy: \", score[1])\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Testing Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KnzqoZRbSR1v"
   },
   "outputs": [],
   "source": [
    "preds = model.predict(x_test) # label scores \n",
    "\n",
    "classpreds = np.argmax(preds, axis=1) # predicted classes \n",
    "\n",
    "y_testclass = np.argmax(y_test, axis=1) # true classes\n",
    "\n",
    "n_classes=2 # number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EMrG8FeJSR14"
   },
   "outputs": [],
   "source": [
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], preds[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oOZSWEULSR17"
   },
   "outputs": [],
   "source": [
    "c_names = ['Bronchiectasis', 'Bronchiolitis', 'COPD', 'Healthy', 'Pneumonia', 'URTI']\n",
    "c_names = ['Healthy', 'Pneumonia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 851
    },
    "colab_type": "code",
    "id": "HY5qEVRrSR2A",
    "outputId": "9c5341de-bbdf-4c87-c999-82ee08ddd1fb"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot ROC curves\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "ax.plot([0, 1], [0, 1], 'k--')\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curve for Each Class')\n",
    "for i in range(n_classes):\n",
    "    ax.plot(fpr[i], tpr[i], linewidth=3, label='ROC curve (area = %0.2f) for %s' % (roc_auc[i], c_names[i]))\n",
    "ax.legend(loc=\"best\", fontsize='x-large')\n",
    "ax.grid(alpha=.4)\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240
    },
    "colab_type": "code",
    "id": "qlQ12gHQSR2D",
    "outputId": "42af0b0e-037c-4177-a382-aff8a8550509"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Healthy       0.38      1.00      0.55         3\n",
      "   Pneumonia       1.00      0.38      0.55         8\n",
      "\n",
      "    accuracy                           0.55        11\n",
      "   macro avg       0.69      0.69      0.55        11\n",
      "weighted avg       0.83      0.55      0.55        11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report\n",
    "print(classification_report(y_testclass, classpreds, target_names=c_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "colab_type": "code",
    "id": "jGzk3U9kSR2G",
    "outputId": "cd147c90-fc8e-47c5-9cd0-6e3e224b3def"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 0]\n",
      " [5 3]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "print(confusion_matrix(y_testclass, classpreds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def plot_confusion_matrix(cm, unique_labels, show=True, output=None,\n",
    "                          title='Confusion matrix', cmap=plt.cm.Oranges):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(cm.shape[1])\n",
    "    plt.xticks(tick_marks, rotation=45)\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticklabels((ax.get_xticks() + 1).astype(str))\n",
    "    plt.yticks(tick_marks)\n",
    "\n",
    "    ax.set_xticklabels(unique_labels)\n",
    "    ax.set_yticklabels(unique_labels)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], '.1f'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    if output is not None:\n",
    "        plt.savefig(output)\n",
    "    plt.close()\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(confusion_matrix(y_testclass, classpreds), unique_labels=[\"Healthy\", \"Pneumonia\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Corona-Disease Classification by CNN using MFCC.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
