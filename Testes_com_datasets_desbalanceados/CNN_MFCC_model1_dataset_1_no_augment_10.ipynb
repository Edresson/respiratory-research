{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "DtXp3DMyU4u2",
    "outputId": "9a02581f-ebc9-4307-c524-f915fc001341"
   },
   "source": [
    "# Rede CNN - Modelo 1\n",
    "\n",
    "## Dataset Respiratory_Sound_Database_Pneumo_Healthy_Only - Dataset 1 - no_augment_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "colab": {},
    "colab_type": "code",
    "id": "sxUgP6_bSR0C"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# Necessário na minha máquina. Estava ocorrendo um erro devido à GPU e esse código resolveu.\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUP_TRAIN = 'no_augment_10'\n",
    "GROUP_TEST = 'no_augment_10'\n",
    "DATASET = 'dataset_1'\n",
    "DURATION = 10\n",
    "SIZE = 431\n",
    "CSV_TRAIN = 'train1.csv'\n",
    "CSV_TEST = 'test1.csv'\n",
    "MODEL_NAME = f'CNN1_{DATASET}_{GROUP_TRAIN}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mMb5_PxwSR0N"
   },
   "outputs": [],
   "source": [
    "train_file_paths = glob.glob(f'../datasets/{DATASET}/{GROUP_TRAIN}/train/**/*.wav', recursive=True)\n",
    "train_file_names = [os.path.splitext(os.path.basename(p))[0] for p in train_file_paths]\n",
    "\n",
    "test_file_paths = glob.glob(f'../datasets/{DATASET}/{GROUP_TEST}/test/**/*.wav', recursive=True)\n",
    "test_file_names = [os.path.splitext(os.path.basename(p))[0] for p in test_file_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wTf5HxHzSR0U"
   },
   "outputs": [],
   "source": [
    "train_p_id_in_file = [] # patient IDs corresponding to each file\n",
    "test_p_id_in_file = [] # patient IDs corresponding to each file\n",
    "for name in train_file_names:\n",
    "    train_p_id_in_file.append(int(name[:3]))\n",
    "\n",
    "for name in test_file_names:\n",
    "    test_p_id_in_file.append(int(name[:3]))\n",
    "\n",
    "train_p_id_in_file = np.array(train_p_id_in_file)\n",
    "test_p_id_in_file = np.array(test_p_id_in_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UbK7vc1kSR0c"
   },
   "outputs": [],
   "source": [
    "max_pad_len = SIZE\n",
    "\n",
    "os.makedirs(\"features/\", exist_ok=True)\n",
    "\n",
    "def extract_features(file_name):\n",
    "    \"\"\"\n",
    "    This function takes in the path for an audio file as a string, loads it, and returns the MFCC\n",
    "    of the audio\"\"\"\n",
    "    feature = os.path.splitext(os.path.basename(file_name))[0] + \".npy\"\n",
    "#     if (os.path.isfile(os.path.join(\"./Respiratory_Sound_Database/features/\", feature))):\n",
    "#         return np.load(os.path.join(\"./Respiratory_Sound_Database/features/\", feature))\n",
    "    \n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast', duration=DURATION) \n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        pad_width = max_pad_len - mfccs.shape[1]\n",
    "        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None \n",
    "    np.save(os.path.join(\"./features/\", feature), mfccs)\n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xkBHJzJDSR0h"
   },
   "outputs": [],
   "source": [
    "#filepaths = [join(mypath, f) for f in filenames] # full paths of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UQjbbn7MSR0n"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>102</td>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>121</td>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>123</td>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>125</td>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>126</td>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0        1\n",
       "0  102  Healthy\n",
       "1  121  Healthy\n",
       "2  123  Healthy\n",
       "3  125  Healthy\n",
       "4  126  Healthy"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_p_diag = pd.read_csv(f\"../Respiratory_Sound_Database/Respiratory_Sound_Database_Pneumo_Healthy_Only/{CSV_TRAIN}\", header=None) # patient diagnosis file\n",
    "test_p_diag = pd.read_csv(f\"../Respiratory_Sound_Database/Respiratory_Sound_Database_Pneumo_Healthy_Only/{CSV_TEST}\", header=None) # patient diagnosis file\n",
    "train_p_diag.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yskEMhphSR0s"
   },
   "outputs": [],
   "source": [
    "train_labels = np.array([train_p_diag[train_p_diag[0] == x][1].values[0] for x in train_p_id_in_file]) \n",
    "test_labels = np.array([test_p_diag[test_p_diag[0] == x][1].values[0] for x in test_p_id_in_file]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "yIlzZ5gRSR0w",
    "outputId": "e42143d5-d247-457f-c891-0c714e51cb05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished feature extraction from  122  files\n"
     ]
    }
   ],
   "source": [
    "train_features = [] \n",
    "test_features = []\n",
    "\n",
    "for file_name in train_file_paths:\n",
    "    data = extract_features(file_name)\n",
    "    train_features.append(data)\n",
    "\n",
    "for file_name in test_file_paths:\n",
    "    data = extract_features(file_name)\n",
    "    test_features.append(data)\n",
    "\n",
    "print('Finished feature extraction from ', (len(train_features)+len(train_features)), ' files')\n",
    "train_features = np.array(train_features)\n",
    "test_features = np.array(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "aPWfXalkSR00",
    "outputId": "0741865f-420e-4c29-8cb1-42b0fe8302cb"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot an MFCC\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(train_features[7], x_axis='time')\n",
    "plt.colorbar()\n",
    "plt.title('MFCC')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Healthy', 'Pneumonia'], dtype='<U9')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "YAHg4HTzSR1C",
    "outputId": "be27b27d-3389-4d26-b8ee-d795b2259a5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Healthy' 'Pneumonia']\n",
      " ['32' '29']]\n",
      "[['Healthy' 'Pneumonia']\n",
      " ['3' '8']]\n"
     ]
    }
   ],
   "source": [
    "unique_elements, counts_elements = np.unique(train_labels, return_counts=True)\n",
    "print(np.asarray((unique_elements, counts_elements)))\n",
    "\n",
    "unique_elements_test, counts_elements_test = np.unique(test_labels, return_counts=True)\n",
    "print(np.asarray((unique_elements_test, counts_elements_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "mtNpDgBOSR1G",
    "outputId": "0f7584a0-d821-4196-c22f-3847fa3031b6"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot class counts\n",
    "y_pos = np.arange(len(unique_elements))\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.bar(unique_elements, counts_elements, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, unique_elements)\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Disease')\n",
    "plt.title('Disease Count in Sound Files (No Asthma or LRTI)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WtqGtxmPSR1K"
   },
   "outputs": [],
   "source": [
    "# One-hot encode labels\n",
    "le = LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "train_labels_cat = to_categorical(le.transform(train_labels)) \n",
    "test_labels_cat = to_categorical(le.transform(test_labels)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61, 40, 431)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = train_features.shape[1]*train_features.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MgH8aGqeSR1P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61, 40, 431, 1) (61, 2)\n",
      "(11, 40, 431, 1) (11, 2)\n"
     ]
    }
   ],
   "source": [
    "train_features = np.reshape(train_features, (*train_features.shape,1)) \n",
    "print(train_features.shape, train_labels_cat.shape)\n",
    "test_features = np.reshape(test_features, (*test_features.shape,1)) \n",
    "print(test_features.shape, test_labels_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xFPaVmUESR1T"
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_features, train_labels_cat, test_size=0.2, random_state = 42)\n",
    "x_test, y_test = test_features, test_labels_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SN1ipKhfSR1X"
   },
   "source": [
    "**CNN model architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oZ5PcMOrcV1B"
   },
   "outputs": [],
   "source": [
    "num_labels = train_labels_cat.shape[1]\n",
    "\n",
    "num_rows = 40\n",
    "num_columns = SIZE\n",
    "num_channels = 1\n",
    "\n",
    "filter_size = 2\n",
    "\n",
    "# Construct model \n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=filter_size,\n",
    "                 input_shape=(num_rows, num_columns, num_channels), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=filter_size, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=filter_size, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=filter_size, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(num_labels, activation='softmax')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VcSipiVsSR1c"
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 719
    },
    "colab_type": "code",
    "id": "UvvyonaaSR1h",
    "outputId": "9c2154c5-f927-4c3d-e89e-951ba479c079"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 39, 430, 16)       80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 19, 215, 16)       0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 19, 215, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 18, 214, 32)       2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 9, 107, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 9, 107, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 106, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 53, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 53, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 52, 128)        32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 1, 26, 128)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1, 26, 128)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 43,570\n",
      "Trainable params: 43,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "11/11 [==============================] - 1s 86ms/sample - loss: 0.7936 - accuracy: 0.7273\n",
      "Pre-training accuracy: 72.7273%\n"
     ]
    }
   ],
   "source": [
    "# Display model architecture summary \n",
    "model.summary()\n",
    "\n",
    "# Calculate pre-training accuracy \n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "print(\"Pre-training accuracy: %.4f%%\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TVtD0mcDSR1j"
   },
   "source": [
    "**Training**\n",
    "\n",
    "Here we will train the model. If we have a trained model, we can load it instead from the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ruRXrsrhSR1k",
    "outputId": "ec26a26d-ad59-4e89-c88f-6277e4d0c283"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48 samples, validate on 13 samples\n",
      "Epoch 1/500\n",
      "10/48 [=====>........................] - ETA: 2s - loss: 8.8043 - accuracy: 0.5000\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.46154, saving model to models/CNN2_dataset_1_no_augment_10_01.h5\n",
      "48/48 [==============================] - 1s 20ms/sample - loss: 10.9909 - accuracy: 0.5625 - val_loss: 1.1087 - val_accuracy: 0.4615\n",
      "Epoch 2/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 8.8537 - accuracy: 0.5000\n",
      "Epoch 00002: val_accuracy improved from 0.46154 to 0.53846, saving model to models/CNN2_dataset_1_no_augment_10_02.h5\n",
      "48/48 [==============================] - 0s 2ms/sample - loss: 4.9917 - accuracy: 0.5417 - val_loss: 4.4898 - val_accuracy: 0.5385\n",
      "Epoch 3/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 3.5965 - accuracy: 0.5000\n",
      "Epoch 00003: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 2.9951 - accuracy: 0.5208 - val_loss: 2.3005 - val_accuracy: 0.5385\n",
      "Epoch 4/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.3461 - accuracy: 0.7000\n",
      "Epoch 00004: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 2.6246 - accuracy: 0.5625 - val_loss: 1.7119 - val_accuracy: 0.5385\n",
      "Epoch 5/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.4978 - accuracy: 0.7000\n",
      "Epoch 00005: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 3.0158 - accuracy: 0.5000 - val_loss: 2.8255 - val_accuracy: 0.5385\n",
      "Epoch 6/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.7357 - accuracy: 0.7000\n",
      "Epoch 00006: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 2.6506 - accuracy: 0.5417 - val_loss: 1.1497 - val_accuracy: 0.5385\n",
      "Epoch 7/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 2.3415 - accuracy: 0.7000\n",
      "Epoch 00007: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 3.0957 - accuracy: 0.5208 - val_loss: 1.6078 - val_accuracy: 0.5385\n",
      "Epoch 8/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.5100 - accuracy: 0.8000\n",
      "Epoch 00008: val_accuracy did not improve from 0.53846\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 2.3252 - accuracy: 0.6250 - val_loss: 1.2156 - val_accuracy: 0.5385\n",
      "Epoch 9/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.7310 - accuracy: 0.7000\n",
      "Epoch 00009: val_accuracy improved from 0.53846 to 0.76923, saving model to models/CNN2_dataset_1_no_augment_10_09.h5\n",
      "48/48 [==============================] - 0s 2ms/sample - loss: 1.5711 - accuracy: 0.5208 - val_loss: 0.5695 - val_accuracy: 0.7692\n",
      "Epoch 10/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.1884 - accuracy: 0.3000\n",
      "Epoch 00010: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 1.3203 - accuracy: 0.5000 - val_loss: 0.6402 - val_accuracy: 0.5385\n",
      "Epoch 11/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.6871 - accuracy: 0.6000\n",
      "Epoch 00011: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.9713 - accuracy: 0.5000 - val_loss: 0.5962 - val_accuracy: 0.6154\n",
      "Epoch 12/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.0301 - accuracy: 0.5000\n",
      "Epoch 00012: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.8877 - accuracy: 0.6875 - val_loss: 0.7430 - val_accuracy: 0.5385\n",
      "Epoch 13/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.0067 - accuracy: 0.6000\n",
      "Epoch 00013: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.7191 - accuracy: 0.7292 - val_loss: 0.7130 - val_accuracy: 0.5385\n",
      "Epoch 14/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.5923 - accuracy: 0.7000\n",
      "Epoch 00014: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6769 - accuracy: 0.6875 - val_loss: 0.9254 - val_accuracy: 0.5385\n",
      "Epoch 15/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.7988 - accuracy: 0.6000\n",
      "Epoch 00015: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6774 - accuracy: 0.6667 - val_loss: 1.0685 - val_accuracy: 0.5385\n",
      "Epoch 16/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.0623 - accuracy: 0.5000\n",
      "Epoch 00016: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.8276 - accuracy: 0.6042 - val_loss: 1.0779 - val_accuracy: 0.5385\n",
      "Epoch 17/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2188 - accuracy: 0.9000\n",
      "Epoch 00017: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.7605 - accuracy: 0.7083 - val_loss: 1.2864 - val_accuracy: 0.5385\n",
      "Epoch 18/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.3051 - accuracy: 0.9000\n",
      "Epoch 00018: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.7579 - accuracy: 0.6875 - val_loss: 1.0741 - val_accuracy: 0.5385\n",
      "Epoch 19/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.3043 - accuracy: 0.9000\n",
      "Epoch 00019: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.5711 - accuracy: 0.7500 - val_loss: 1.1341 - val_accuracy: 0.5385\n",
      "Epoch 20/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.9291 - accuracy: 0.6000\n",
      "Epoch 00020: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6154 - accuracy: 0.7292 - val_loss: 1.1673 - val_accuracy: 0.5385\n",
      "Epoch 21/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.4000 - accuracy: 0.9000\n",
      "Epoch 00021: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.5298 - accuracy: 0.8125 - val_loss: 0.9389 - val_accuracy: 0.5385\n",
      "Epoch 22/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.5793 - accuracy: 0.7000\n",
      "Epoch 00022: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 2ms/sample - loss: 0.5366 - accuracy: 0.7083 - val_loss: 1.1843 - val_accuracy: 0.5385\n",
      "Epoch 23/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.4284 - accuracy: 0.8000\n",
      "Epoch 00023: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6180 - accuracy: 0.7500 - val_loss: 0.8701 - val_accuracy: 0.5385\n",
      "Epoch 24/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.9392 - accuracy: 0.6000\n",
      "Epoch 00024: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.8225 - accuracy: 0.7083 - val_loss: 1.1779 - val_accuracy: 0.5385\n",
      "Epoch 25/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.4620 - accuracy: 0.7000\n",
      "Epoch 00025: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6551 - accuracy: 0.7292 - val_loss: 0.7322 - val_accuracy: 0.5385\n",
      "Epoch 26/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.6479 - accuracy: 0.4000\n",
      "Epoch 00026: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.4355 - accuracy: 0.7292 - val_loss: 1.1088 - val_accuracy: 0.5385\n",
      "Epoch 27/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.4725 - accuracy: 0.8000\n",
      "Epoch 00027: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.4692 - accuracy: 0.7708 - val_loss: 0.6926 - val_accuracy: 0.5385\n",
      "Epoch 28/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2955 - accuracy: 0.8000\n",
      "Epoch 00028: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.5929 - accuracy: 0.7083 - val_loss: 0.8331 - val_accuracy: 0.5385\n",
      "Epoch 29/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2192 - accuracy: 1.0000\n",
      "Epoch 00029: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.4924 - accuracy: 0.8125 - val_loss: 0.6193 - val_accuracy: 0.5385\n",
      "Epoch 30/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.5750 - accuracy: 0.7000\n",
      "Epoch 00030: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.5480 - accuracy: 0.7292 - val_loss: 0.6376 - val_accuracy: 0.5385\n",
      "Epoch 31/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0862 - accuracy: 1.0000\n",
      "Epoch 00031: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.5289 - accuracy: 0.7917 - val_loss: 0.4464 - val_accuracy: 0.6923\n",
      "Epoch 32/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.4260 - accuracy: 0.8000\n",
      "Epoch 00032: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.5683 - accuracy: 0.7292 - val_loss: 0.5839 - val_accuracy: 0.6154\n",
      "Epoch 33/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.8031 - accuracy: 0.6000\n",
      "Epoch 00033: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.5192 - accuracy: 0.7708 - val_loss: 0.5062 - val_accuracy: 0.6154\n",
      "Epoch 34/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1930 - accuracy: 1.0000\n",
      "Epoch 00034: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.4377 - accuracy: 0.7708 - val_loss: 0.4861 - val_accuracy: 0.6154\n",
      "Epoch 35/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.3280 - accuracy: 0.8000\n",
      "Epoch 00035: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.5484 - accuracy: 0.7292 - val_loss: 0.4288 - val_accuracy: 0.6923\n",
      "Epoch 36/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.6822 - accuracy: 0.7000\n",
      "Epoch 00036: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.5027 - accuracy: 0.7708 - val_loss: 0.5342 - val_accuracy: 0.5385\n",
      "Epoch 37/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2619 - accuracy: 0.9000\n",
      "Epoch 00037: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.4291 - accuracy: 0.8125 - val_loss: 0.3899 - val_accuracy: 0.7692\n",
      "Epoch 38/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.3525 - accuracy: 0.8000\n",
      "Epoch 00038: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.3729 - accuracy: 0.8542 - val_loss: 0.4809 - val_accuracy: 0.5385\n",
      "Epoch 39/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2887 - accuracy: 0.9000\n",
      "Epoch 00039: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.5232 - accuracy: 0.7708 - val_loss: 0.4796 - val_accuracy: 0.6154\n",
      "Epoch 40/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2108 - accuracy: 0.9000\n",
      "Epoch 00040: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.8744 - accuracy: 0.7083 - val_loss: 0.6699 - val_accuracy: 0.5385\n",
      "Epoch 41/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.3684 - accuracy: 0.9000\n",
      "Epoch 00041: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.7232 - accuracy: 0.6875 - val_loss: 0.5605 - val_accuracy: 0.5385\n",
      "Epoch 42/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.4267 - accuracy: 0.8000\n",
      "Epoch 00042: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.4647 - accuracy: 0.7500 - val_loss: 0.4928 - val_accuracy: 0.6154\n",
      "Epoch 43/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.4169 - accuracy: 0.8000\n",
      "Epoch 00043: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.5488 - accuracy: 0.7917 - val_loss: 0.5913 - val_accuracy: 0.5385\n",
      "Epoch 44/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.4861 - accuracy: 0.7000\n",
      "Epoch 00044: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.4215 - accuracy: 0.7917 - val_loss: 0.4538 - val_accuracy: 0.6923\n",
      "Epoch 45/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.5133 - accuracy: 0.8000\n",
      "Epoch 00045: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.3641 - accuracy: 0.8542 - val_loss: 0.6647 - val_accuracy: 0.5385\n",
      "Epoch 46/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.4730 - accuracy: 0.7000\n",
      "Epoch 00046: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.4102 - accuracy: 0.8125 - val_loss: 0.5291 - val_accuracy: 0.5385\n",
      "Epoch 47/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.3448 - accuracy: 0.9000\n",
      "Epoch 00047: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.4321 - accuracy: 0.7917 - val_loss: 0.6354 - val_accuracy: 0.5385\n",
      "Epoch 48/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.3942 - accuracy: 0.8000\n",
      "Epoch 00048: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.3218 - accuracy: 0.8750 - val_loss: 0.7217 - val_accuracy: 0.5385\n",
      "Epoch 49/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2915 - accuracy: 0.9000\n",
      "Epoch 00049: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.3617 - accuracy: 0.8750 - val_loss: 0.6445 - val_accuracy: 0.5385\n",
      "Epoch 50/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.4956 - accuracy: 0.7000\n",
      "Epoch 00050: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.2665 - accuracy: 0.8542 - val_loss: 0.6984 - val_accuracy: 0.5385\n",
      "Epoch 51/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2668 - accuracy: 0.9000\n",
      "Epoch 00051: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.3150 - accuracy: 0.8125 - val_loss: 0.6508 - val_accuracy: 0.6154\n",
      "Epoch 52/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1832 - accuracy: 1.0000\n",
      "Epoch 00052: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.2978 - accuracy: 0.8750 - val_loss: 0.5691 - val_accuracy: 0.6923\n",
      "Epoch 53/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2876 - accuracy: 0.8000\n",
      "Epoch 00053: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.2429 - accuracy: 0.8542 - val_loss: 0.6342 - val_accuracy: 0.6923\n",
      "Epoch 54/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2234 - accuracy: 0.9000\n",
      "Epoch 00054: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.2804 - accuracy: 0.8333 - val_loss: 0.5263 - val_accuracy: 0.6923\n",
      "Epoch 55/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.4132 - accuracy: 0.8000\n",
      "Epoch 00055: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.2203 - accuracy: 0.9375 - val_loss: 0.6193 - val_accuracy: 0.6923\n",
      "Epoch 56/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.6907 - accuracy: 0.6000\n",
      "Epoch 00056: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.3756 - accuracy: 0.7917 - val_loss: 0.5011 - val_accuracy: 0.6923\n",
      "Epoch 57/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1916 - accuracy: 1.0000\n",
      "Epoch 00057: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.2721 - accuracy: 0.9167 - val_loss: 0.4982 - val_accuracy: 0.6923\n",
      "Epoch 58/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1045 - accuracy: 1.0000\n",
      "Epoch 00058: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.1275 - accuracy: 0.9375 - val_loss: 0.3953 - val_accuracy: 0.6923\n",
      "Epoch 59/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1883 - accuracy: 0.9000\n",
      "Epoch 00059: val_accuracy did not improve from 0.76923\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.2749 - accuracy: 0.8542 - val_loss: 0.4881 - val_accuracy: 0.6923\n",
      "Epoch 60/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0377 - accuracy: 1.0000\n",
      "Epoch 00060: val_accuracy improved from 0.76923 to 0.84615, saving model to models/CNN2_dataset_1_no_augment_10_60.h5\n",
      "48/48 [==============================] - 0s 2ms/sample - loss: 0.1400 - accuracy: 0.9583 - val_loss: 0.2871 - val_accuracy: 0.8462\n",
      "Epoch 61/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2913 - accuracy: 0.9000\n",
      "Epoch 00061: val_accuracy did not improve from 0.84615\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.2232 - accuracy: 0.9167 - val_loss: 0.5020 - val_accuracy: 0.6154\n",
      "Epoch 62/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2270 - accuracy: 0.8000\n",
      "Epoch 00062: val_accuracy did not improve from 0.84615\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.2721 - accuracy: 0.8542 - val_loss: 0.2765 - val_accuracy: 0.8462\n",
      "Epoch 63/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.3710 - accuracy: 0.9000\n",
      "Epoch 00063: val_accuracy did not improve from 0.84615\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.2744 - accuracy: 0.9167 - val_loss: 0.3325 - val_accuracy: 0.8462\n",
      "Epoch 64/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0642 - accuracy: 1.0000\n",
      "Epoch 00064: val_accuracy did not improve from 0.84615\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.1351 - accuracy: 0.9792 - val_loss: 0.3220 - val_accuracy: 0.8462\n",
      "Epoch 65/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0392 - accuracy: 1.0000\n",
      "Epoch 00065: val_accuracy did not improve from 0.84615\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.1190 - accuracy: 0.9583 - val_loss: 0.2788 - val_accuracy: 0.8462\n",
      "Epoch 66/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2374 - accuracy: 0.9000\n",
      "Epoch 00066: val_accuracy did not improve from 0.84615\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.1462 - accuracy: 0.9583 - val_loss: 0.2807 - val_accuracy: 0.8462\n",
      "Epoch 67/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0889 - accuracy: 1.0000\n",
      "Epoch 00067: val_accuracy did not improve from 0.84615\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0786 - accuracy: 0.9792 - val_loss: 0.2310 - val_accuracy: 0.8462\n",
      "Epoch 68/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 00068: val_accuracy did not improve from 0.84615\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0897 - accuracy: 1.0000 - val_loss: 0.2298 - val_accuracy: 0.8462\n",
      "Epoch 69/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0525 - accuracy: 1.0000\n",
      "Epoch 00069: val_accuracy did not improve from 0.84615\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0838 - accuracy: 0.9792 - val_loss: 0.2501 - val_accuracy: 0.8462\n",
      "Epoch 70/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0942 - accuracy: 1.0000\n",
      "Epoch 00070: val_accuracy did not improve from 0.84615\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.1202 - accuracy: 0.9375 - val_loss: 0.1999 - val_accuracy: 0.8462\n",
      "Epoch 71/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1579 - accuracy: 0.9000\n",
      "Epoch 00071: val_accuracy did not improve from 0.84615\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.1371 - accuracy: 0.9375 - val_loss: 0.2153 - val_accuracy: 0.8462\n",
      "Epoch 72/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1843 - accuracy: 0.9000\n",
      "Epoch 00072: val_accuracy did not improve from 0.84615\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0878 - accuracy: 0.9792 - val_loss: 0.1784 - val_accuracy: 0.8462\n",
      "Epoch 73/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0662 - accuracy: 1.0000\n",
      "Epoch 00073: val_accuracy did not improve from 0.84615\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.1330 - accuracy: 0.9375 - val_loss: 0.1826 - val_accuracy: 0.8462\n",
      "Epoch 74/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0871 - accuracy: 1.0000\n",
      "Epoch 00074: val_accuracy did not improve from 0.84615\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0804 - accuracy: 0.9583 - val_loss: 0.1802 - val_accuracy: 0.8462\n",
      "Epoch 75/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0554 - accuracy: 1.0000\n",
      "Epoch 00075: val_accuracy did not improve from 0.84615\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0982 - accuracy: 0.9792 - val_loss: 0.1799 - val_accuracy: 0.8462\n",
      "Epoch 76/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0696 - accuracy: 1.0000\n",
      "Epoch 00076: val_accuracy did not improve from 0.84615\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.1267 - accuracy: 0.9167 - val_loss: 0.1832 - val_accuracy: 0.8462\n",
      "Epoch 77/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0615 - accuracy: 1.0000\n",
      "Epoch 00077: val_accuracy improved from 0.84615 to 0.92308, saving model to models/CNN2_dataset_1_no_augment_10_77.h5\n",
      "48/48 [==============================] - 0s 2ms/sample - loss: 0.1129 - accuracy: 0.9375 - val_loss: 0.1877 - val_accuracy: 0.9231\n",
      "Epoch 78/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1005 - accuracy: 0.9000\n",
      "Epoch 00078: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.1136 - accuracy: 0.9583 - val_loss: 0.2314 - val_accuracy: 0.8462\n",
      "Epoch 79/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0998 - accuracy: 0.9000\n",
      "Epoch 00079: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.1512 - accuracy: 0.9167 - val_loss: 0.1838 - val_accuracy: 0.9231\n",
      "Epoch 80/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2512 - accuracy: 0.9000\n",
      "Epoch 00080: val_accuracy did not improve from 0.92308\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.1461 - accuracy: 0.9375 - val_loss: 0.1605 - val_accuracy: 0.8462\n",
      "Epoch 81/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0524 - accuracy: 1.0000\n",
      "Epoch 00081: val_accuracy improved from 0.92308 to 1.00000, saving model to models/CNN2_dataset_1_no_augment_10_81.h5\n",
      "48/48 [==============================] - 0s 2ms/sample - loss: 0.1161 - accuracy: 0.9583 - val_loss: 0.1478 - val_accuracy: 1.0000\n",
      "Epoch 82/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0664 - accuracy: 1.0000\n",
      "Epoch 00082: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.1033 - accuracy: 0.9583 - val_loss: 0.1802 - val_accuracy: 0.9231\n",
      "Epoch 83/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0878 - accuracy: 1.0000\n",
      "Epoch 00083: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.1085 - accuracy: 0.9375 - val_loss: 0.1543 - val_accuracy: 0.9231\n",
      "Epoch 84/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 00084: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0471 - accuracy: 1.0000 - val_loss: 0.2395 - val_accuracy: 0.9231\n",
      "Epoch 85/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0580 - accuracy: 1.0000\n",
      "Epoch 00085: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0970 - accuracy: 0.9583 - val_loss: 0.1481 - val_accuracy: 1.0000\n",
      "Epoch 86/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0534 - accuracy: 1.0000\n",
      "Epoch 00086: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0667 - accuracy: 0.9792 - val_loss: 0.1408 - val_accuracy: 1.0000\n",
      "Epoch 87/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 00087: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0526 - accuracy: 1.0000 - val_loss: 0.1469 - val_accuracy: 1.0000\n",
      "Epoch 88/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0587 - accuracy: 1.0000\n",
      "Epoch 00088: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0569 - accuracy: 0.9792 - val_loss: 0.1494 - val_accuracy: 1.0000\n",
      "Epoch 89/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0188 - accuracy: 1.0000\n",
      "Epoch 00089: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0684 - accuracy: 0.9792 - val_loss: 0.1699 - val_accuracy: 0.9231\n",
      "Epoch 90/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0397 - accuracy: 1.0000\n",
      "Epoch 00090: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0333 - accuracy: 1.0000 - val_loss: 0.1325 - val_accuracy: 1.0000\n",
      "Epoch 91/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0620 - accuracy: 1.0000\n",
      "Epoch 00091: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0293 - accuracy: 1.0000 - val_loss: 0.1546 - val_accuracy: 0.9231\n",
      "Epoch 92/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0241 - accuracy: 1.0000\n",
      "Epoch 00092: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0215 - accuracy: 1.0000 - val_loss: 0.1742 - val_accuracy: 0.9231\n",
      "Epoch 93/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 2.8544e-04 - accuracy: 1.0000\n",
      "Epoch 00093: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0406 - accuracy: 1.0000 - val_loss: 0.1898 - val_accuracy: 0.9231\n",
      "Epoch 94/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 00094: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0268 - accuracy: 1.0000 - val_loss: 0.2044 - val_accuracy: 0.9231\n",
      "Epoch 95/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0331 - accuracy: 1.0000\n",
      "Epoch 00095: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0766 - accuracy: 0.9792 - val_loss: 0.1234 - val_accuracy: 1.0000\n",
      "Epoch 96/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1436 - accuracy: 0.9000\n",
      "Epoch 00096: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0740 - accuracy: 0.9583 - val_loss: 0.1208 - val_accuracy: 1.0000\n",
      "Epoch 97/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 00097: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0567 - accuracy: 0.9792 - val_loss: 0.2269 - val_accuracy: 0.9231\n",
      "Epoch 98/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0189 - accuracy: 1.0000\n",
      "Epoch 00098: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0156 - accuracy: 1.0000 - val_loss: 0.1241 - val_accuracy: 1.0000\n",
      "Epoch 99/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0992 - accuracy: 0.9000\n",
      "Epoch 00099: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0325 - accuracy: 0.9792 - val_loss: 0.2265 - val_accuracy: 0.9231\n",
      "Epoch 100/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1241 - accuracy: 0.9000\n",
      "Epoch 00100: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0403 - accuracy: 0.9792 - val_loss: 0.1091 - val_accuracy: 1.0000\n",
      "Epoch 101/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 00101: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0448 - accuracy: 0.9792 - val_loss: 0.2826 - val_accuracy: 0.8462\n",
      "Epoch 102/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0680 - accuracy: 1.0000\n",
      "Epoch 00102: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.2039 - accuracy: 0.9583 - val_loss: 0.0988 - val_accuracy: 1.0000\n",
      "Epoch 103/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0146 - accuracy: 1.0000\n",
      "Epoch 00103: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.1275 - accuracy: 0.9167 - val_loss: 0.1054 - val_accuracy: 0.9231\n",
      "Epoch 104/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 00104: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.2499 - accuracy: 0.9167 - val_loss: 0.0839 - val_accuracy: 1.0000\n",
      "Epoch 105/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0119 - accuracy: 1.0000\n",
      "Epoch 00105: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.2719 - accuracy: 0.9167 - val_loss: 0.1035 - val_accuracy: 1.0000\n",
      "Epoch 106/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0405 - accuracy: 1.0000\n",
      "Epoch 00106: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.1572 - accuracy: 0.9375 - val_loss: 0.2697 - val_accuracy: 0.8462\n",
      "Epoch 107/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2394 - accuracy: 0.9000\n",
      "Epoch 00107: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.1190 - accuracy: 0.9583 - val_loss: 0.1841 - val_accuracy: 0.8462\n",
      "Epoch 108/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.4861 - accuracy: 0.8000\n",
      "Epoch 00108: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.1416 - accuracy: 0.9583 - val_loss: 0.1592 - val_accuracy: 0.9231\n",
      "Epoch 109/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0197 - accuracy: 1.0000\n",
      "Epoch 00109: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.1332 - accuracy: 0.9375 - val_loss: 0.1341 - val_accuracy: 0.8462\n",
      "Epoch 110/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0404 - accuracy: 1.0000\n",
      "Epoch 00110: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0516 - accuracy: 1.0000 - val_loss: 0.1420 - val_accuracy: 0.8462\n",
      "Epoch 111/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 00111: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0294 - accuracy: 1.0000 - val_loss: 0.1817 - val_accuracy: 0.9231\n",
      "Epoch 112/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 00112: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.1031 - accuracy: 0.9375 - val_loss: 0.1483 - val_accuracy: 1.0000\n",
      "Epoch 113/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0206 - accuracy: 1.0000\n",
      "Epoch 00113: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0442 - accuracy: 0.9792 - val_loss: 0.1306 - val_accuracy: 1.0000\n",
      "Epoch 114/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 00114: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0276 - accuracy: 1.0000 - val_loss: 0.2716 - val_accuracy: 0.8462\n",
      "Epoch 115/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0480 - accuracy: 1.0000\n",
      "Epoch 00115: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0460 - accuracy: 0.9792 - val_loss: 0.1875 - val_accuracy: 0.9231\n",
      "Epoch 116/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 00116: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0314 - accuracy: 1.0000 - val_loss: 0.1248 - val_accuracy: 1.0000\n",
      "Epoch 117/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0301 - accuracy: 1.0000\n",
      "Epoch 00117: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0289 - accuracy: 1.0000 - val_loss: 0.2351 - val_accuracy: 0.8462\n",
      "Epoch 118/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0217 - accuracy: 1.0000\n",
      "Epoch 00118: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0210 - accuracy: 1.0000 - val_loss: 0.2874 - val_accuracy: 0.8462\n",
      "Epoch 119/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 00119: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0164 - accuracy: 1.0000 - val_loss: 0.2274 - val_accuracy: 0.9231\n",
      "Epoch 120/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 00120: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0227 - accuracy: 1.0000 - val_loss: 0.2350 - val_accuracy: 0.8462\n",
      "Epoch 121/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 00121: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0130 - accuracy: 1.0000 - val_loss: 0.3223 - val_accuracy: 0.8462\n",
      "Epoch 122/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 00122: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0361 - accuracy: 1.0000 - val_loss: 0.2079 - val_accuracy: 0.9231\n",
      "Epoch 123/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 00123: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0196 - accuracy: 1.0000 - val_loss: 0.2027 - val_accuracy: 0.9231\n",
      "Epoch 124/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0175 - accuracy: 1.0000\n",
      "Epoch 00124: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0130 - accuracy: 1.0000 - val_loss: 0.3191 - val_accuracy: 0.8462\n",
      "Epoch 125/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0872 - accuracy: 0.9000\n",
      "Epoch 00125: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0521 - accuracy: 0.9583 - val_loss: 0.1079 - val_accuracy: 1.0000\n",
      "Epoch 126/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0091 - accuracy: 1.0000\n",
      "Epoch 00126: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0484 - accuracy: 0.9583 - val_loss: 0.1020 - val_accuracy: 1.0000\n",
      "Epoch 127/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0145 - accuracy: 1.0000\n",
      "Epoch 00127: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.1703 - val_accuracy: 0.9231\n",
      "Epoch 128/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 00128: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.2005 - val_accuracy: 0.8462\n",
      "Epoch 129/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0715 - accuracy: 1.0000\n",
      "Epoch 00129: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0293 - accuracy: 1.0000 - val_loss: 0.1369 - val_accuracy: 0.9231\n",
      "Epoch 130/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0143 - accuracy: 1.0000\n",
      "Epoch 00130: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0375 - accuracy: 0.9792 - val_loss: 0.1750 - val_accuracy: 0.9231\n",
      "Epoch 131/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 00131: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0212 - accuracy: 1.0000 - val_loss: 0.2507 - val_accuracy: 0.8462\n",
      "Epoch 132/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0570 - accuracy: 1.0000\n",
      "Epoch 00132: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0327 - accuracy: 1.0000 - val_loss: 0.1067 - val_accuracy: 1.0000\n",
      "Epoch 133/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0301 - accuracy: 1.0000\n",
      "Epoch 00133: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.2894 - val_accuracy: 0.8462\n",
      "Epoch 134/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 00134: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0198 - accuracy: 1.0000 - val_loss: 0.2336 - val_accuracy: 0.9231\n",
      "Epoch 135/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 00135: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0309 - accuracy: 0.9792 - val_loss: 0.2247 - val_accuracy: 0.9231\n",
      "Epoch 136/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0172 - accuracy: 1.0000\n",
      "Epoch 00136: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0205 - accuracy: 1.0000 - val_loss: 0.2491 - val_accuracy: 0.8462\n",
      "Epoch 137/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0410 - accuracy: 1.0000\n",
      "Epoch 00137: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0596 - accuracy: 0.9792 - val_loss: 0.0681 - val_accuracy: 1.0000\n",
      "Epoch 138/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0299 - accuracy: 1.0000\n",
      "Epoch 00138: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0171 - accuracy: 1.0000 - val_loss: 0.4556 - val_accuracy: 0.8462\n",
      "Epoch 139/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2739 - accuracy: 0.9000\n",
      "Epoch 00139: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0711 - accuracy: 0.9792 - val_loss: 0.2217 - val_accuracy: 0.8462\n",
      "Epoch 140/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 00140: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0158 - accuracy: 1.0000 - val_loss: 0.0769 - val_accuracy: 1.0000\n",
      "Epoch 141/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1156 - accuracy: 0.9000\n",
      "Epoch 00141: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0326 - accuracy: 0.9792 - val_loss: 0.2108 - val_accuracy: 0.8462\n",
      "Epoch 142/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0140 - accuracy: 1.0000\n",
      "Epoch 00142: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.3189 - val_accuracy: 0.8462\n",
      "Epoch 143/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0449 - accuracy: 1.0000\n",
      "Epoch 00143: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0138 - accuracy: 1.0000 - val_loss: 0.1411 - val_accuracy: 0.9231\n",
      "Epoch 144/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0547 - accuracy: 1.0000\n",
      "Epoch 00144: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0268 - accuracy: 1.0000 - val_loss: 0.1088 - val_accuracy: 1.0000\n",
      "Epoch 145/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0220 - accuracy: 1.0000\n",
      "Epoch 00145: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0472 - accuracy: 0.9583 - val_loss: 0.1409 - val_accuracy: 0.9231\n",
      "Epoch 146/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 00146: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0441 - accuracy: 0.9792 - val_loss: 0.3410 - val_accuracy: 0.8462\n",
      "Epoch 147/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 00147: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.1332 - val_accuracy: 0.9231\n",
      "Epoch 148/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 00148: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.1095 - val_accuracy: 1.0000\n",
      "Epoch 149/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 00149: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.1225 - val_accuracy: 0.9231\n",
      "Epoch 150/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 3.4639e-04 - accuracy: 1.0000\n",
      "Epoch 00150: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.1029 - val_accuracy: 1.0000\n",
      "Epoch 151/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0590 - accuracy: 1.0000\n",
      "Epoch 00151: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0258 - accuracy: 1.0000 - val_loss: 0.0858 - val_accuracy: 1.0000\n",
      "Epoch 152/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 00152: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.0892 - val_accuracy: 1.0000\n",
      "Epoch 153/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 2.6868e-05 - accuracy: 1.0000\n",
      "Epoch 00153: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0322 - accuracy: 0.9792 - val_loss: 0.1269 - val_accuracy: 0.9231\n",
      "Epoch 154/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0612 - accuracy: 1.0000\n",
      "Epoch 00154: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0193 - accuracy: 1.0000 - val_loss: 0.0926 - val_accuracy: 1.0000\n",
      "Epoch 155/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 00155: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0147 - accuracy: 1.0000 - val_loss: 0.1051 - val_accuracy: 1.0000\n",
      "Epoch 156/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0588 - accuracy: 1.0000\n",
      "Epoch 00156: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0318 - accuracy: 1.0000 - val_loss: 0.1089 - val_accuracy: 1.0000\n",
      "Epoch 157/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0119 - accuracy: 1.0000\n",
      "Epoch 00157: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.1777 - val_accuracy: 0.9231\n",
      "Epoch 158/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 00158: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0231 - accuracy: 0.9792 - val_loss: 0.2343 - val_accuracy: 0.9231\n",
      "Epoch 159/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0401 - accuracy: 1.0000\n",
      "Epoch 00159: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0172 - accuracy: 1.0000 - val_loss: 0.1510 - val_accuracy: 0.8462\n",
      "Epoch 160/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.3632 - accuracy: 0.9000\n",
      "Epoch 00160: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0983 - accuracy: 0.9583 - val_loss: 0.4196 - val_accuracy: 0.8462\n",
      "Epoch 161/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0183 - accuracy: 1.0000\n",
      "Epoch 00161: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0585 - accuracy: 0.9583 - val_loss: 0.0723 - val_accuracy: 1.0000\n",
      "Epoch 162/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 00162: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.1829 - accuracy: 0.9375 - val_loss: 0.1393 - val_accuracy: 0.9231\n",
      "Epoch 163/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2116 - accuracy: 0.8000\n",
      "Epoch 00163: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0873 - accuracy: 0.9375 - val_loss: 0.1380 - val_accuracy: 0.9231\n",
      "Epoch 164/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0303 - accuracy: 1.0000\n",
      "Epoch 00164: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0163 - accuracy: 1.0000 - val_loss: 0.1032 - val_accuracy: 1.0000\n",
      "Epoch 165/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 00165: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.1272 - val_accuracy: 0.8462\n",
      "Epoch 166/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.7092e-04 - accuracy: 1.0000\n",
      "Epoch 00166: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0303 - accuracy: 0.9792 - val_loss: 0.0931 - val_accuracy: 1.0000\n",
      "Epoch 167/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 00167: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0394 - accuracy: 0.9792 - val_loss: 0.0851 - val_accuracy: 1.0000\n",
      "Epoch 168/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.4308e-04 - accuracy: 1.0000\n",
      "Epoch 00168: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.1159 - val_accuracy: 0.9231\n",
      "Epoch 169/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 00169: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0632 - accuracy: 0.9583 - val_loss: 0.1489 - val_accuracy: 0.9231\n",
      "Epoch 170/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0753 - accuracy: 0.9000\n",
      "Epoch 00170: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0291 - accuracy: 0.9792 - val_loss: 0.1009 - val_accuracy: 1.0000\n",
      "Epoch 171/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 00171: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.1266 - val_accuracy: 0.9231\n",
      "Epoch 172/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 00172: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.1438 - val_accuracy: 0.8462\n",
      "Epoch 173/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0598 - accuracy: 1.0000\n",
      "Epoch 00173: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0176 - accuracy: 1.0000 - val_loss: 0.1000 - val_accuracy: 1.0000\n",
      "Epoch 174/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0129 - accuracy: 1.0000\n",
      "Epoch 00174: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.1005 - val_accuracy: 1.0000\n",
      "Epoch 175/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 00175: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.0959 - val_accuracy: 1.0000\n",
      "Epoch 176/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 8.9642e-06 - accuracy: 1.0000\n",
      "Epoch 00176: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 2ms/sample - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.0951 - val_accuracy: 1.0000\n",
      "Epoch 177/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0239 - accuracy: 1.0000\n",
      "Epoch 00177: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.1293 - val_accuracy: 0.8462\n",
      "Epoch 178/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 00178: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0319 - accuracy: 0.9792 - val_loss: 0.0888 - val_accuracy: 1.0000\n",
      "Epoch 179/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0193 - accuracy: 1.0000\n",
      "Epoch 00179: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.0930 - val_accuracy: 1.0000\n",
      "Epoch 180/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 00180: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0236 - accuracy: 1.0000 - val_loss: 0.0955 - val_accuracy: 1.0000\n",
      "Epoch 181/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 00181: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.1197 - val_accuracy: 0.8462\n",
      "Epoch 182/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 00182: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0168 - accuracy: 1.0000 - val_loss: 0.0630 - val_accuracy: 1.0000\n",
      "Epoch 183/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 00183: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.0733 - val_accuracy: 1.0000\n",
      "Epoch 184/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0870 - accuracy: 1.0000\n",
      "Epoch 00184: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 2ms/sample - loss: 0.0457 - accuracy: 1.0000 - val_loss: 0.1254 - val_accuracy: 0.8462\n",
      "Epoch 185/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 00185: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0723 - accuracy: 0.9583 - val_loss: 0.0842 - val_accuracy: 1.0000\n",
      "Epoch 186/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 4.9093e-04 - accuracy: 1.0000\n",
      "Epoch 00186: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.1530 - accuracy: 0.9375 - val_loss: 0.0592 - val_accuracy: 1.0000\n",
      "Epoch 187/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 00187: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.7489 - accuracy: 0.8542 - val_loss: 0.0527 - val_accuracy: 1.0000\n",
      "Epoch 188/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 00188: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6335 - accuracy: 0.8750 - val_loss: 0.4955 - val_accuracy: 0.8462\n",
      "Epoch 189/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1471 - accuracy: 0.9000\n",
      "Epoch 00189: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.1952 - accuracy: 0.9167 - val_loss: 0.1124 - val_accuracy: 1.0000\n",
      "Epoch 190/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2670 - accuracy: 0.8000\n",
      "Epoch 00190: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0630 - accuracy: 0.9583 - val_loss: 0.7576 - val_accuracy: 0.8462\n",
      "Epoch 191/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2663 - accuracy: 0.9000\n",
      "Epoch 00191: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.1290 - accuracy: 0.9375 - val_loss: 0.4732 - val_accuracy: 0.8462\n",
      "Epoch 192/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 00192: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0832 - accuracy: 0.9792 - val_loss: 0.0997 - val_accuracy: 1.0000\n",
      "Epoch 193/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1448 - accuracy: 0.9000\n",
      "Epoch 00193: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0618 - accuracy: 0.9583 - val_loss: 0.4821 - val_accuracy: 0.8462\n",
      "Epoch 194/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0406 - accuracy: 1.0000\n",
      "Epoch 00194: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.1000 - accuracy: 0.9583 - val_loss: 0.4678 - val_accuracy: 0.8462\n",
      "Epoch 195/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.2492 - accuracy: 0.9000\n",
      "Epoch 00195: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.1025 - accuracy: 0.9375 - val_loss: 0.0921 - val_accuracy: 1.0000\n",
      "Epoch 196/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.4067 - accuracy: 0.9000\n",
      "Epoch 00196: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.1905 - accuracy: 0.9375 - val_loss: 0.4635 - val_accuracy: 0.8462\n",
      "Epoch 197/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1993 - accuracy: 0.9000\n",
      "Epoch 00197: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.1174 - accuracy: 0.9375 - val_loss: 0.4977 - val_accuracy: 0.8462\n",
      "Epoch 198/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.3344 - accuracy: 0.9000\n",
      "Epoch 00198: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0812 - accuracy: 0.9792 - val_loss: 0.0804 - val_accuracy: 1.0000\n",
      "Epoch 199/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 00199: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0688 - accuracy: 0.9792 - val_loss: 0.0792 - val_accuracy: 1.0000\n",
      "Epoch 200/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1127 - accuracy: 0.9000\n",
      "Epoch 00200: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0453 - accuracy: 0.9792 - val_loss: 0.4850 - val_accuracy: 0.8462\n",
      "Epoch 201/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0462 - accuracy: 1.0000\n",
      "Epoch 00201: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0385 - accuracy: 0.9792 - val_loss: 0.4440 - val_accuracy: 0.8462\n",
      "Epoch 202/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1195 - accuracy: 0.9000\n",
      "Epoch 00202: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0313 - accuracy: 0.9792 - val_loss: 0.1363 - val_accuracy: 0.9231\n",
      "Epoch 203/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0108 - accuracy: 1.0000\n",
      "Epoch 00203: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0178 - accuracy: 1.0000 - val_loss: 0.1251 - val_accuracy: 0.9231\n",
      "Epoch 204/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 5.3742e-04 - accuracy: 1.0000\n",
      "Epoch 00204: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.1922 - val_accuracy: 0.8462\n",
      "Epoch 205/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 3.9302e-04 - accuracy: 1.0000\n",
      "Epoch 00205: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.3364 - val_accuracy: 0.8462\n",
      "Epoch 206/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 00206: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0168 - accuracy: 1.0000 - val_loss: 0.3616 - val_accuracy: 0.8462\n",
      "Epoch 207/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 00207: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.1920 - val_accuracy: 0.9231\n",
      "Epoch 208/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 8.1627e-04 - accuracy: 1.0000\n",
      "Epoch 00208: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.1271 - val_accuracy: 0.9231\n",
      "Epoch 209/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0402 - accuracy: 1.0000\n",
      "Epoch 00209: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0314 - accuracy: 1.0000 - val_loss: 0.1531 - val_accuracy: 0.9231\n",
      "Epoch 210/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 3.2965e-04 - accuracy: 1.0000\n",
      "Epoch 00210: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.2612 - val_accuracy: 0.8462\n",
      "Epoch 211/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 00211: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.3377 - val_accuracy: 0.8462\n",
      "Epoch 212/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0551 - accuracy: 1.0000\n",
      "Epoch 00212: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0161 - accuracy: 1.0000 - val_loss: 0.1455 - val_accuracy: 0.9231\n",
      "Epoch 213/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 00213: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.1000 - val_accuracy: 1.0000\n",
      "Epoch 214/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 00214: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.1201 - val_accuracy: 0.9231\n",
      "Epoch 215/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 00215: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.1836 - val_accuracy: 0.8462\n",
      "Epoch 216/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.9247e-04 - accuracy: 1.0000\n",
      "Epoch 00216: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.2084 - val_accuracy: 0.8462\n",
      "Epoch 217/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 00217: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0160 - accuracy: 1.0000 - val_loss: 0.1687 - val_accuracy: 0.8462\n",
      "Epoch 218/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 00218: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.1234 - val_accuracy: 0.9231\n",
      "Epoch 219/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 00219: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0151 - accuracy: 1.0000 - val_loss: 0.2408 - val_accuracy: 0.8462\n",
      "Epoch 220/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0341 - accuracy: 1.0000\n",
      "Epoch 00220: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.2524 - val_accuracy: 0.8462\n",
      "Epoch 221/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 00221: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.2395 - val_accuracy: 0.8462\n",
      "Epoch 222/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0181 - accuracy: 1.0000\n",
      "Epoch 00222: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.1857 - val_accuracy: 0.9231\n",
      "Epoch 223/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0141 - accuracy: 1.0000\n",
      "Epoch 00223: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.1603 - val_accuracy: 0.9231\n",
      "Epoch 224/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 00224: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.1467 - val_accuracy: 0.9231\n",
      "Epoch 225/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 4.9479e-05 - accuracy: 1.0000\n",
      "Epoch 00225: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.1810 - val_accuracy: 0.9231\n",
      "Epoch 226/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 00226: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.2413 - val_accuracy: 0.8462\n",
      "Epoch 227/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 00227: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.2563 - val_accuracy: 0.8462\n",
      "Epoch 228/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0324 - accuracy: 1.0000\n",
      "Epoch 00228: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.1734 - val_accuracy: 0.9231\n",
      "Epoch 229/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 00229: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.1368 - val_accuracy: 0.9231\n",
      "Epoch 230/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 6.2790e-04 - accuracy: 1.0000\n",
      "Epoch 00230: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.1208 - val_accuracy: 0.9231\n",
      "Epoch 231/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 00231: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.1928 - val_accuracy: 0.9231\n",
      "Epoch 232/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0278 - accuracy: 1.0000\n",
      "Epoch 00232: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0157 - accuracy: 1.0000 - val_loss: 0.2099 - val_accuracy: 0.9231\n",
      "Epoch 233/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 00233: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.1572 - val_accuracy: 0.9231\n",
      "Epoch 234/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 00234: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.1125 - val_accuracy: 0.9231\n",
      "Epoch 235/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 00235: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.1323 - val_accuracy: 0.9231\n",
      "Epoch 236/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 00236: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.1950 - val_accuracy: 0.9231\n",
      "Epoch 237/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0138 - accuracy: 1.0000\n",
      "Epoch 00237: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.2229 - val_accuracy: 0.8462\n",
      "Epoch 238/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0872 - accuracy: 1.0000\n",
      "Epoch 00238: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0195 - accuracy: 1.0000 - val_loss: 0.0915 - val_accuracy: 1.0000\n",
      "Epoch 239/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0210 - accuracy: 1.0000\n",
      "Epoch 00239: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.1065 - val_accuracy: 0.9231\n",
      "Epoch 240/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 00240: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.1422 - val_accuracy: 0.9231\n",
      "Epoch 241/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0357 - accuracy: 1.0000\n",
      "Epoch 00241: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.1109 - val_accuracy: 0.9231\n",
      "Epoch 242/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 00242: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0349 - accuracy: 0.9583 - val_loss: 0.1753 - val_accuracy: 0.9231\n",
      "Epoch 243/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 9.3506e-04 - accuracy: 1.0000\n",
      "Epoch 00243: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.0825 - val_accuracy: 1.0000\n",
      "Epoch 244/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0221 - accuracy: 1.0000\n",
      "Epoch 00244: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.1267 - val_accuracy: 0.9231\n",
      "Epoch 245/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 3.7702e-05 - accuracy: 1.0000\n",
      "Epoch 00245: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.1919 - val_accuracy: 0.9231\n",
      "Epoch 246/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 00246: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.3313 - val_accuracy: 0.8462\n",
      "Epoch 247/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 00247: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.3000 - val_accuracy: 0.8462\n",
      "Epoch 248/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 00248: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.2067 - val_accuracy: 0.9231\n",
      "Epoch 249/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 00249: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.1674 - val_accuracy: 0.9231\n",
      "Epoch 250/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 7.1235e-04 - accuracy: 1.0000\n",
      "Epoch 00250: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.1493 - val_accuracy: 0.9231\n",
      "Epoch 251/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 7.3165e-04 - accuracy: 1.0000\n",
      "Epoch 00251: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.1265 - val_accuracy: 0.9231\n",
      "Epoch 252/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 00252: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.1419 - val_accuracy: 0.9231\n",
      "Epoch 253/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 00253: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.1041 - val_accuracy: 0.9231\n",
      "Epoch 254/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0161 - accuracy: 1.0000\n",
      "Epoch 00254: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.1083 - val_accuracy: 0.9231\n",
      "Epoch 255/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.1512e-04 - accuracy: 1.0000\n",
      "Epoch 00255: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0920 - val_accuracy: 1.0000\n",
      "Epoch 256/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 00256: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.0863 - val_accuracy: 1.0000\n",
      "Epoch 257/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 00257: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0776 - val_accuracy: 1.0000\n",
      "Epoch 258/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 00258: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.0900 - val_accuracy: 1.0000\n",
      "Epoch 259/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 5.2264e-04 - accuracy: 1.0000\n",
      "Epoch 00259: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0784 - val_accuracy: 1.0000\n",
      "Epoch 260/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 5.0048e-04 - accuracy: 1.0000\n",
      "Epoch 00260: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0655 - val_accuracy: 1.0000\n",
      "Epoch 261/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 3.4257e-04 - accuracy: 1.0000\n",
      "Epoch 00261: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.0561 - val_accuracy: 1.0000\n",
      "Epoch 262/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 00262: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.1122 - val_accuracy: 0.9231\n",
      "Epoch 263/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 00263: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.1565 - val_accuracy: 0.9231\n",
      "Epoch 264/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.7332e-05 - accuracy: 1.0000\n",
      "Epoch 00264: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.1520 - val_accuracy: 0.9231\n",
      "Epoch 265/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 00265: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.0984 - val_accuracy: 0.9231\n",
      "Epoch 266/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 00266: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.0789 - val_accuracy: 1.0000\n",
      "Epoch 267/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 9.9667e-04 - accuracy: 1.0000\n",
      "Epoch 00267: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 7.1322e-04 - accuracy: 1.0000 - val_loss: 0.1122 - val_accuracy: 0.9231\n",
      "Epoch 268/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 4.0689e-04 - accuracy: 1.0000\n",
      "Epoch 00268: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.0847 - val_accuracy: 1.0000\n",
      "Epoch 269/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 00269: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.0429 - val_accuracy: 1.0000\n",
      "Epoch 270/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 00270: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.0470 - val_accuracy: 1.0000\n",
      "Epoch 271/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 9.2025e-06 - accuracy: 1.0000\n",
      "Epoch 00271: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0732 - val_accuracy: 1.0000\n",
      "Epoch 272/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 00272: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0846 - val_accuracy: 1.0000\n",
      "Epoch 273/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 3.5997e-05 - accuracy: 1.0000\n",
      "Epoch 00273: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0818 - val_accuracy: 1.0000\n",
      "Epoch 274/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 2.7377e-04 - accuracy: 1.0000\n",
      "Epoch 00274: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0983 - val_accuracy: 0.9231\n",
      "Epoch 275/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 8.9825e-04 - accuracy: 1.0000\n",
      "Epoch 00275: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.1106 - val_accuracy: 0.9231\n",
      "Epoch 276/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0115 - accuracy: 1.0000\n",
      "Epoch 00276: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0631 - val_accuracy: 1.0000\n",
      "Epoch 277/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 00277: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.0675 - val_accuracy: 1.0000\n",
      "Epoch 278/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 00278: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.1106 - val_accuracy: 0.9231\n",
      "Epoch 279/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 8.9068e-04 - accuracy: 1.0000\n",
      "Epoch 00279: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0419 - val_accuracy: 1.0000\n",
      "Epoch 280/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0309 - accuracy: 1.0000\n",
      "Epoch 00280: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.0759 - val_accuracy: 1.0000\n",
      "Epoch 281/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 00281: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.1198 - val_accuracy: 0.9231\n",
      "Epoch 282/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 00282: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.2295 - val_accuracy: 0.9231\n",
      "Epoch 283/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 00283: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.1778 - val_accuracy: 0.9231\n",
      "Epoch 284/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0345 - accuracy: 1.0000\n",
      "Epoch 00284: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.0521 - val_accuracy: 1.0000\n",
      "Epoch 285/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 00285: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0312 - val_accuracy: 1.0000\n",
      "Epoch 286/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 00286: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.0480 - val_accuracy: 1.0000\n",
      "Epoch 287/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 2.5798e-04 - accuracy: 1.0000\n",
      "Epoch 00287: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.0682 - val_accuracy: 1.0000\n",
      "Epoch 288/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 00288: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 9.9383e-04 - accuracy: 1.0000 - val_loss: 0.0369 - val_accuracy: 1.0000\n",
      "Epoch 289/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 00289: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0308 - val_accuracy: 1.0000\n",
      "Epoch 290/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 00290: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0344 - val_accuracy: 1.0000\n",
      "Epoch 291/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 00291: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0449 - val_accuracy: 1.0000\n",
      "Epoch 292/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 00292: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.0780 - val_accuracy: 1.0000\n",
      "Epoch 293/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 3.7538e-04 - accuracy: 1.0000\n",
      "Epoch 00293: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.0727 - val_accuracy: 1.0000\n",
      "Epoch 294/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 00294: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 1.0000\n",
      "Epoch 295/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 00295: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0218 - val_accuracy: 1.0000\n",
      "Epoch 296/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 00296: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0216 - val_accuracy: 1.0000\n",
      "Epoch 297/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 2.3745e-05 - accuracy: 1.0000\n",
      "Epoch 00297: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.0302 - val_accuracy: 1.0000\n",
      "Epoch 298/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 00298: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0574 - accuracy: 0.9792 - val_loss: 0.0309 - val_accuracy: 1.0000\n",
      "Epoch 299/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 00299: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0165 - accuracy: 1.0000 - val_loss: 0.0367 - val_accuracy: 1.0000\n",
      "Epoch 300/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 00300: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.2308 - val_accuracy: 0.9231\n",
      "Epoch 301/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0143 - accuracy: 1.0000\n",
      "Epoch 00301: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.2113 - val_accuracy: 0.9231\n",
      "Epoch 302/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 5.2935e-04 - accuracy: 1.0000\n",
      "Epoch 00302: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.0995 - val_accuracy: 0.9231\n",
      "Epoch 303/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 3.1055e-04 - accuracy: 1.0000\n",
      "Epoch 00303: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.0338 - val_accuracy: 1.0000\n",
      "Epoch 304/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 8.1237e-04 - accuracy: 1.0000\n",
      "Epoch 00304: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.0382 - val_accuracy: 1.0000\n",
      "Epoch 305/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 00305: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1235 - val_accuracy: 0.9231\n",
      "Epoch 306/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 00306: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.1098 - val_accuracy: 0.9231\n",
      "Epoch 307/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 8.6974e-04 - accuracy: 1.0000\n",
      "Epoch 00307: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.1249 - val_accuracy: 0.9231\n",
      "Epoch 308/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 00308: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.1688 - val_accuracy: 0.9231\n",
      "Epoch 309/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 4.1863e-04 - accuracy: 1.0000\n",
      "Epoch 00309: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 7.7284e-04 - accuracy: 1.0000 - val_loss: 0.1783 - val_accuracy: 0.9231\n",
      "Epoch 310/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.9200e-04 - accuracy: 1.0000\n",
      "Epoch 00310: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1405 - val_accuracy: 0.9231\n",
      "Epoch 311/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 2.6094e-05 - accuracy: 1.0000\n",
      "Epoch 00311: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0637 - val_accuracy: 1.0000\n",
      "Epoch 312/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 9.0442e-04 - accuracy: 1.0000\n",
      "Epoch 00312: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 6.8266e-04 - accuracy: 1.0000 - val_loss: 0.0322 - val_accuracy: 1.0000\n",
      "Epoch 313/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 3.2608e-04 - accuracy: 1.0000\n",
      "Epoch 00313: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0266 - val_accuracy: 1.0000\n",
      "Epoch 314/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 00314: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.0312 - val_accuracy: 1.0000\n",
      "Epoch 315/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 6.5330e-05 - accuracy: 1.0000\n",
      "Epoch 00315: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.0344 - val_accuracy: 1.0000\n",
      "Epoch 316/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 4.5717e-04 - accuracy: 1.0000\n",
      "Epoch 00316: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 8.5898e-04 - accuracy: 1.0000 - val_loss: 0.0301 - val_accuracy: 1.0000\n",
      "Epoch 317/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 4.5748e-04 - accuracy: 1.0000\n",
      "Epoch 00317: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0278 - val_accuracy: 1.0000\n",
      "Epoch 318/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 6.2089e-04 - accuracy: 1.0000\n",
      "Epoch 00318: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 6.3389e-04 - accuracy: 1.0000 - val_loss: 0.0266 - val_accuracy: 1.0000\n",
      "Epoch 319/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 2.3561e-04 - accuracy: 1.0000\n",
      "Epoch 00319: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.0274 - val_accuracy: 1.0000\n",
      "Epoch 320/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 00320: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0572 - val_accuracy: 1.0000\n",
      "Epoch 321/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 00321: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0732 - val_accuracy: 1.0000\n",
      "Epoch 322/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 9.6761e-04 - accuracy: 1.0000\n",
      "Epoch 00322: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 6.2936e-04 - accuracy: 1.0000 - val_loss: 0.0662 - val_accuracy: 1.0000\n",
      "Epoch 323/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 2.6030e-04 - accuracy: 1.0000\n",
      "Epoch 00323: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 6.8886e-04 - accuracy: 1.0000 - val_loss: 0.0600 - val_accuracy: 1.0000\n",
      "Epoch 324/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 5.7159e-04 - accuracy: 1.0000\n",
      "Epoch 00324: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.0340 - val_accuracy: 1.0000\n",
      "Epoch 325/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 00325: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0301 - val_accuracy: 1.0000\n",
      "Epoch 326/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 00326: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0305 - val_accuracy: 1.0000\n",
      "Epoch 327/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 00327: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 7.0886e-04 - accuracy: 1.0000 - val_loss: 0.0354 - val_accuracy: 1.0000\n",
      "Epoch 328/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0174 - accuracy: 1.0000\n",
      "Epoch 00328: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.0322 - val_accuracy: 1.0000\n",
      "Epoch 329/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 00329: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0363 - val_accuracy: 1.0000\n",
      "Epoch 330/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.8812e-04 - accuracy: 1.0000\n",
      "Epoch 00330: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 7.0115e-04 - accuracy: 1.0000 - val_loss: 0.0406 - val_accuracy: 1.0000\n",
      "Epoch 331/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 5.8909e-04 - accuracy: 1.0000\n",
      "Epoch 00331: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.0370 - val_accuracy: 1.0000\n",
      "Epoch 332/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 00332: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.0361 - val_accuracy: 1.0000\n",
      "Epoch 333/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 4.0661e-04 - accuracy: 1.0000\n",
      "Epoch 00333: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 9.5625e-04 - accuracy: 1.0000 - val_loss: 0.1614 - val_accuracy: 0.9231\n",
      "Epoch 334/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 00334: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0585 - val_accuracy: 1.0000\n",
      "Epoch 335/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 00335: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0270 - val_accuracy: 1.0000\n",
      "Epoch 336/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 7.2232e-04 - accuracy: 1.0000\n",
      "Epoch 00336: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0245 - val_accuracy: 1.0000\n",
      "Epoch 337/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 00337: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0270 - val_accuracy: 1.0000\n",
      "Epoch 338/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 00338: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0261 - val_accuracy: 1.0000\n",
      "Epoch 339/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 6.1615e-04 - accuracy: 1.0000\n",
      "Epoch 00339: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0248 - val_accuracy: 1.0000\n",
      "Epoch 340/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 2.2258e-04 - accuracy: 1.0000\n",
      "Epoch 00340: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 8.6641e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 1.0000\n",
      "Epoch 341/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 00341: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 9.3048e-04 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 1.0000\n",
      "Epoch 342/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 00342: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 1.0000\n",
      "Epoch 343/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 00343: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 6.7162e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 1.0000\n",
      "Epoch 344/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 2.9026e-04 - accuracy: 1.0000\n",
      "Epoch 00344: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 9.1267e-04 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 1.0000\n",
      "Epoch 345/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.1420e-05 - accuracy: 1.0000\n",
      "Epoch 00345: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.0245 - val_accuracy: 1.0000\n",
      "Epoch 346/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 00346: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.0287 - val_accuracy: 1.0000\n",
      "Epoch 347/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 00347: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0332 - val_accuracy: 1.0000\n",
      "Epoch 348/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0103 - accuracy: 1.0000\n",
      "Epoch 00348: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 1.0000\n",
      "Epoch 349/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 7.3963e-04 - accuracy: 1.0000\n",
      "Epoch 00349: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0240 - val_accuracy: 1.0000\n",
      "Epoch 350/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 6.0891e-04 - accuracy: 1.0000\n",
      "Epoch 00350: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 5.9537e-04 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 1.0000\n",
      "Epoch 351/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 3.9577e-06 - accuracy: 1.0000\n",
      "Epoch 00351: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0153 - accuracy: 1.0000 - val_loss: 0.0395 - val_accuracy: 1.0000\n",
      "Epoch 352/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 00352: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0271 - accuracy: 0.9792 - val_loss: 0.0192 - val_accuracy: 1.0000\n",
      "Epoch 353/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 6.8888e-04 - accuracy: 1.0000\n",
      "Epoch 00353: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.1327 - val_accuracy: 0.8462\n",
      "Epoch 354/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0155 - accuracy: 1.0000\n",
      "Epoch 00354: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0184 - accuracy: 1.0000 - val_loss: 0.0178 - val_accuracy: 1.0000\n",
      "Epoch 355/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 00355: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 1.0000\n",
      "Epoch 356/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 00356: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 1.0000\n",
      "Epoch 357/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.8037e-04 - accuracy: 1.0000\n",
      "Epoch 00357: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1322 - val_accuracy: 0.8462\n",
      "Epoch 358/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0122 - accuracy: 1.0000\n",
      "Epoch 00358: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.1243 - val_accuracy: 0.8462\n",
      "Epoch 359/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 8.5948e-06 - accuracy: 1.0000\n",
      "Epoch 00359: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.0264 - val_accuracy: 1.0000\n",
      "Epoch 360/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 6.8069e-04 - accuracy: 1.0000\n",
      "Epoch 00360: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 6.8980e-04 - accuracy: 1.0000 - val_loss: 0.0168 - val_accuracy: 1.0000\n",
      "Epoch 361/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 6.9254e-04 - accuracy: 1.0000\n",
      "Epoch 00361: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.0166 - val_accuracy: 1.0000\n",
      "Epoch 362/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0266 - accuracy: 1.0000\n",
      "Epoch 00362: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0164 - accuracy: 1.0000 - val_loss: 0.2096 - val_accuracy: 0.8462\n",
      "Epoch 363/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 00363: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0299 - accuracy: 0.9792 - val_loss: 0.1310 - val_accuracy: 0.9231\n",
      "Epoch 364/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.2636e-06 - accuracy: 1.0000\n",
      "Epoch 00364: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 9.3420e-04 - accuracy: 1.0000 - val_loss: 0.0187 - val_accuracy: 1.0000\n",
      "Epoch 365/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.1114e-04 - accuracy: 1.0000\n",
      "Epoch 00365: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.0247 - val_accuracy: 1.0000\n",
      "Epoch 366/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 00366: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0212 - val_accuracy: 1.0000\n",
      "Epoch 367/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 5.0729e-04 - accuracy: 1.0000\n",
      "Epoch 00367: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0340 - val_accuracy: 1.0000\n",
      "Epoch 368/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.0559e-04 - accuracy: 1.0000\n",
      "Epoch 00368: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 3.3573e-04 - accuracy: 1.0000 - val_loss: 0.0777 - val_accuracy: 0.9231\n",
      "Epoch 369/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 2.9013e-04 - accuracy: 1.0000\n",
      "Epoch 00369: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0746 - val_accuracy: 1.0000\n",
      "Epoch 370/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.4084e-04 - accuracy: 1.0000\n",
      "Epoch 00370: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 5.7427e-04 - accuracy: 1.0000 - val_loss: 0.0383 - val_accuracy: 1.0000\n",
      "Epoch 371/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 00371: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 6.9745e-04 - accuracy: 1.0000 - val_loss: 0.0304 - val_accuracy: 1.0000\n",
      "Epoch 372/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 3.9451e-04 - accuracy: 1.0000\n",
      "Epoch 00372: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 5.2749e-04 - accuracy: 1.0000 - val_loss: 0.0253 - val_accuracy: 1.0000\n",
      "Epoch 373/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 2.4479e-04 - accuracy: 1.0000\n",
      "Epoch 00373: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 1.0000\n",
      "Epoch 374/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 3.2744e-04 - accuracy: 1.0000\n",
      "Epoch 00374: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 8.0325e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 1.0000\n",
      "Epoch 375/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 8.5280e-04 - accuracy: 1.0000\n",
      "Epoch 00375: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 3.4440e-04 - accuracy: 1.0000 - val_loss: 0.0267 - val_accuracy: 1.0000\n",
      "Epoch 376/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 2.1096e-04 - accuracy: 1.0000\n",
      "Epoch 00376: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 7.9435e-04 - accuracy: 1.0000 - val_loss: 0.0263 - val_accuracy: 1.0000\n",
      "Epoch 377/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 8.7315e-04 - accuracy: 1.0000\n",
      "Epoch 00377: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0218 - val_accuracy: 1.0000\n",
      "Epoch 378/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 4.0676e-04 - accuracy: 1.0000\n",
      "Epoch 00378: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0186 - val_accuracy: 1.0000\n",
      "Epoch 379/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 00379: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 3.8714e-04 - accuracy: 1.0000 - val_loss: 0.0188 - val_accuracy: 1.0000\n",
      "Epoch 380/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 3.0360e-04 - accuracy: 1.0000\n",
      "Epoch 00380: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0185 - val_accuracy: 1.0000\n",
      "Epoch 381/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 2.6069e-04 - accuracy: 1.0000\n",
      "Epoch 00381: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 4.5197e-04 - accuracy: 1.0000 - val_loss: 0.0178 - val_accuracy: 1.0000\n",
      "Epoch 382/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.9358e-05 - accuracy: 1.0000\n",
      "Epoch 00382: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 9.0361e-04 - accuracy: 1.0000 - val_loss: 0.0178 - val_accuracy: 1.0000\n",
      "Epoch 383/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 6.6937e-04 - accuracy: 1.0000\n",
      "Epoch 00383: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 1.0000\n",
      "Epoch 384/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.0647e-04 - accuracy: 1.0000\n",
      "Epoch 00384: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.0171 - val_accuracy: 1.0000\n",
      "Epoch 385/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.7638e-04 - accuracy: 1.0000\n",
      "Epoch 00385: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.0183 - val_accuracy: 1.0000\n",
      "Epoch 386/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 9.8639e-05 - accuracy: 1.0000\n",
      "Epoch 00386: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 2.5746e-04 - accuracy: 1.0000 - val_loss: 0.1124 - val_accuracy: 0.9231\n",
      "Epoch 387/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 7.4522e-04 - accuracy: 1.0000\n",
      "Epoch 00387: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.2218 - val_accuracy: 0.9231\n",
      "Epoch 388/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 00388: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0281 - val_accuracy: 1.0000\n",
      "Epoch 389/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 3.5177e-04 - accuracy: 1.0000\n",
      "Epoch 00389: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 5.5324e-04 - accuracy: 1.0000 - val_loss: 0.0155 - val_accuracy: 1.0000\n",
      "Epoch 390/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 9.4490e-05 - accuracy: 1.0000\n",
      "Epoch 00390: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0161 - val_accuracy: 1.0000\n",
      "Epoch 391/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 6.9203e-04 - accuracy: 1.0000\n",
      "Epoch 00391: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 9.6692e-04 - accuracy: 1.0000 - val_loss: 0.0156 - val_accuracy: 1.0000\n",
      "Epoch 392/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.3208e-05 - accuracy: 1.0000\n",
      "Epoch 00392: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 3.2035e-04 - accuracy: 1.0000 - val_loss: 0.0150 - val_accuracy: 1.0000\n",
      "Epoch 393/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 6.7486e-05 - accuracy: 1.0000\n",
      "Epoch 00393: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 8.5765e-04 - accuracy: 1.0000 - val_loss: 0.0149 - val_accuracy: 1.0000\n",
      "Epoch 394/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 2.3433e-04 - accuracy: 1.0000\n",
      "Epoch 00394: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 4.9901e-04 - accuracy: 1.0000 - val_loss: 0.0154 - val_accuracy: 1.0000\n",
      "Epoch 395/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 00395: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0153 - val_accuracy: 1.0000\n",
      "Epoch 396/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.4125e-04 - accuracy: 1.0000\n",
      "Epoch 00396: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 1.9712e-04 - accuracy: 1.0000 - val_loss: 0.0153 - val_accuracy: 1.0000\n",
      "Epoch 397/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 4.1524e-04 - accuracy: 1.0000\n",
      "Epoch 00397: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0151 - val_accuracy: 1.0000\n",
      "Epoch 398/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.2087e-05 - accuracy: 1.0000\n",
      "Epoch 00398: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.0502 - val_accuracy: 1.0000\n",
      "Epoch 399/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.3588e-04 - accuracy: 1.0000\n",
      "Epoch 00399: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0319 - accuracy: 0.9792 - val_loss: 0.0420 - val_accuracy: 1.0000\n",
      "Epoch 400/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1237 - accuracy: 0.9000\n",
      "Epoch 00400: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0447 - accuracy: 0.9583 - val_loss: 0.0317 - val_accuracy: 1.0000\n",
      "Epoch 401/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 4.1064e-05 - accuracy: 1.0000\n",
      "Epoch 00401: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0393 - accuracy: 0.9792 - val_loss: 0.0539 - val_accuracy: 1.0000\n",
      "Epoch 402/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 9.3837e-05 - accuracy: 1.0000\n",
      "Epoch 00402: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 1.2048e-04 - accuracy: 1.0000 - val_loss: 0.0129 - val_accuracy: 1.0000\n",
      "Epoch 403/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 00403: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.0158 - val_accuracy: 1.0000\n",
      "Epoch 404/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0090 - accuracy: 1.0000\n",
      "Epoch 00404: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.0125 - val_accuracy: 1.0000\n",
      "Epoch 405/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 6.1424e-04 - accuracy: 1.0000\n",
      "Epoch 00405: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0825 - val_accuracy: 0.9231\n",
      "Epoch 406/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 00406: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.1077 - val_accuracy: 0.9231\n",
      "Epoch 407/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0152 - accuracy: 1.0000\n",
      "Epoch 00407: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.0103 - val_accuracy: 1.0000\n",
      "Epoch 408/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 4.7277e-04 - accuracy: 1.0000\n",
      "Epoch 00408: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0257 - accuracy: 0.9792 - val_loss: 0.0206 - val_accuracy: 1.0000\n",
      "Epoch 409/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 3.9022e-04 - accuracy: 1.0000\n",
      "Epoch 00409: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0898 - accuracy: 0.9792 - val_loss: 0.2453 - val_accuracy: 0.8462\n",
      "Epoch 410/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0633 - accuracy: 1.0000\n",
      "Epoch 00410: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0721 - accuracy: 0.9792 - val_loss: 0.0137 - val_accuracy: 1.0000\n",
      "Epoch 411/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 00411: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.0939 - val_accuracy: 0.9231\n",
      "Epoch 412/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 00412: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.3630 - val_accuracy: 0.8462\n",
      "Epoch 413/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 5.3082e-05 - accuracy: 1.0000\n",
      "Epoch 00413: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0192 - accuracy: 0.9792 - val_loss: 0.0440 - val_accuracy: 1.0000\n",
      "Epoch 414/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 2.1360e-05 - accuracy: 1.0000\n",
      "Epoch 00414: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0080 - val_accuracy: 1.0000\n",
      "Epoch 415/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 7.4934e-04 - accuracy: 1.0000\n",
      "Epoch 00415: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0196 - accuracy: 0.9792 - val_loss: 0.0207 - val_accuracy: 1.0000\n",
      "Epoch 416/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 3.6325e-04 - accuracy: 1.0000\n",
      "Epoch 00416: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 5.9868e-04 - accuracy: 1.0000 - val_loss: 0.7461 - val_accuracy: 0.8462\n",
      "Epoch 417/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 00417: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0443 - accuracy: 0.9792 - val_loss: 0.0075 - val_accuracy: 1.0000\n",
      "Epoch 418/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0423 - accuracy: 1.0000\n",
      "Epoch 00418: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.0077 - val_accuracy: 1.0000\n",
      "Epoch 419/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.2141e-04 - accuracy: 1.0000\n",
      "Epoch 00419: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 8.1222e-04 - accuracy: 1.0000 - val_loss: 0.0087 - val_accuracy: 1.0000\n",
      "Epoch 420/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 00420: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 4.2717e-04 - accuracy: 1.0000 - val_loss: 0.0124 - val_accuracy: 1.0000\n",
      "Epoch 421/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.2360e-04 - accuracy: 1.0000\n",
      "Epoch 00421: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 9.7079e-04 - accuracy: 1.0000 - val_loss: 0.0173 - val_accuracy: 1.0000\n",
      "Epoch 422/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 5.2451e-06 - accuracy: 1.0000\n",
      "Epoch 00422: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 3.8903e-04 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 1.0000\n",
      "Epoch 423/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 00423: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 8.2033e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 1.0000\n",
      "Epoch 424/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 9.0930e-04 - accuracy: 1.0000\n",
      "Epoch 00424: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0184 - val_accuracy: 1.0000\n",
      "Epoch 425/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0190 - accuracy: 1.0000\n",
      "Epoch 00425: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.0096 - val_accuracy: 1.0000\n",
      "Epoch 426/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 6.8362e-04 - accuracy: 1.0000\n",
      "Epoch 00426: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0084 - val_accuracy: 1.0000\n",
      "Epoch 427/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 4.7853e-04 - accuracy: 1.0000\n",
      "Epoch 00427: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0086 - val_accuracy: 1.0000\n",
      "Epoch 428/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.1327e-04 - accuracy: 1.0000\n",
      "Epoch 00428: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0107 - val_accuracy: 1.0000\n",
      "Epoch 429/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 2.7299e-06 - accuracy: 1.0000\n",
      "Epoch 00429: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 4.5759e-04 - accuracy: 1.0000 - val_loss: 0.0140 - val_accuracy: 1.0000\n",
      "Epoch 430/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 00430: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0286 - val_accuracy: 1.0000\n",
      "Epoch 431/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 3.6835e-06 - accuracy: 1.0000\n",
      "Epoch 00431: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0356 - val_accuracy: 1.0000\n",
      "Epoch 432/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 5.1040e-04 - accuracy: 1.0000\n",
      "Epoch 00432: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0256 - val_accuracy: 1.0000\n",
      "Epoch 433/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 2.1242e-04 - accuracy: 1.0000\n",
      "Epoch 00433: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 8.6222e-05 - accuracy: 1.0000 - val_loss: 0.0183 - val_accuracy: 1.0000\n",
      "Epoch 434/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.2368e-04 - accuracy: 1.0000\n",
      "Epoch 00434: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 1.5100e-04 - accuracy: 1.0000 - val_loss: 0.0152 - val_accuracy: 1.0000\n",
      "Epoch 435/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0111 - accuracy: 1.0000\n",
      "Epoch 00435: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0211 - val_accuracy: 1.0000\n",
      "Epoch 436/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 6.6020e-05 - accuracy: 1.0000\n",
      "Epoch 00436: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0298 - val_accuracy: 1.0000\n",
      "Epoch 437/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.4397e-04 - accuracy: 1.0000\n",
      "Epoch 00437: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 1.7816e-04 - accuracy: 1.0000 - val_loss: 0.0562 - val_accuracy: 1.0000\n",
      "Epoch 438/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 2.2164e-04 - accuracy: 1.0000\n",
      "Epoch 00438: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 6.9973e-04 - accuracy: 1.0000 - val_loss: 0.0678 - val_accuracy: 1.0000\n",
      "Epoch 439/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 3.4213e-06 - accuracy: 1.0000\n",
      "Epoch 00439: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 6.6811e-04 - accuracy: 1.0000 - val_loss: 0.0671 - val_accuracy: 1.0000\n",
      "Epoch 440/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 4.3496e-05 - accuracy: 1.0000\n",
      "Epoch 00440: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 4.8330e-04 - accuracy: 1.0000 - val_loss: 0.0558 - val_accuracy: 1.0000\n",
      "Epoch 441/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.3309e-04 - accuracy: 1.0000\n",
      "Epoch 00441: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 6.0347e-04 - accuracy: 1.0000 - val_loss: 0.0390 - val_accuracy: 1.0000\n",
      "Epoch 442/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 00442: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0246 - accuracy: 0.9792 - val_loss: 0.0212 - val_accuracy: 1.0000\n",
      "Epoch 443/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.3985 - accuracy: 0.9000\n",
      "Epoch 00443: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.1547 - accuracy: 0.9583 - val_loss: 0.4929 - val_accuracy: 0.8462\n",
      "Epoch 444/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 8.4118e-04 - accuracy: 1.0000\n",
      "Epoch 00444: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.0822 - val_accuracy: 1.0000\n",
      "Epoch 445/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0700 - accuracy: 1.0000\n",
      "Epoch 00445: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0238 - accuracy: 1.0000 - val_loss: 0.7454 - val_accuracy: 0.8462\n",
      "Epoch 446/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0352 - accuracy: 1.0000\n",
      "Epoch 00446: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.1413 - accuracy: 0.9792 - val_loss: 0.1097 - val_accuracy: 0.9231\n",
      "Epoch 447/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 3.5483e-04 - accuracy: 1.0000\n",
      "Epoch 00447: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.1671 - accuracy: 0.9375 - val_loss: 1.2113 - val_accuracy: 0.8462\n",
      "Epoch 448/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.8300 - accuracy: 0.8000\n",
      "Epoch 00448: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.3451 - accuracy: 0.9167 - val_loss: 0.0236 - val_accuracy: 1.0000\n",
      "Epoch 449/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1035 - accuracy: 0.9000\n",
      "Epoch 00449: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0276 - accuracy: 0.9792 - val_loss: 0.2021 - val_accuracy: 0.9231\n",
      "Epoch 450/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.1856 - accuracy: 0.9000\n",
      "Epoch 00450: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0901 - accuracy: 0.9375 - val_loss: 0.0643 - val_accuracy: 0.9231\n",
      "Epoch 451/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 00451: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0234 - accuracy: 1.0000 - val_loss: 0.0212 - val_accuracy: 1.0000\n",
      "Epoch 452/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0829 - accuracy: 0.9000\n",
      "Epoch 00452: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0197 - accuracy: 0.9792 - val_loss: 0.1163 - val_accuracy: 0.9231\n",
      "Epoch 453/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.6548e-04 - accuracy: 1.0000\n",
      "Epoch 00453: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0205 - accuracy: 0.9792 - val_loss: 0.2672 - val_accuracy: 0.8462\n",
      "Epoch 454/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0159 - accuracy: 1.0000\n",
      "Epoch 00454: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0152 - accuracy: 1.0000 - val_loss: 0.0324 - val_accuracy: 1.0000\n",
      "Epoch 455/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 00455: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.0202 - val_accuracy: 1.0000\n",
      "Epoch 456/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 3.4042e-04 - accuracy: 1.0000\n",
      "Epoch 00456: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 1.0000\n",
      "Epoch 457/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.7231e-04 - accuracy: 1.0000\n",
      "Epoch 00457: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0508 - val_accuracy: 1.0000\n",
      "Epoch 458/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 6.4981e-05 - accuracy: 1.0000\n",
      "Epoch 00458: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 6.7818e-04 - accuracy: 1.0000 - val_loss: 0.1045 - val_accuracy: 0.9231\n",
      "Epoch 459/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 7.7421e-04 - accuracy: 1.0000\n",
      "Epoch 00459: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 4.3033e-04 - accuracy: 1.0000 - val_loss: 0.1445 - val_accuracy: 0.9231\n",
      "Epoch 460/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 00460: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.2115 - val_accuracy: 0.9231\n",
      "Epoch 461/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 00461: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.1807 - val_accuracy: 0.9231\n",
      "Epoch 462/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 9.7248e-04 - accuracy: 1.0000\n",
      "Epoch 00462: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.0495 - val_accuracy: 1.0000\n",
      "Epoch 463/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 2.2336e-04 - accuracy: 1.0000\n",
      "Epoch 00463: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0189 - val_accuracy: 1.0000\n",
      "Epoch 464/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 00464: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0209 - val_accuracy: 1.0000\n",
      "Epoch 465/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00465: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 4.5480e-04 - accuracy: 1.0000 - val_loss: 0.0263 - val_accuracy: 1.0000\n",
      "Epoch 466/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 00466: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0450 - val_accuracy: 1.0000\n",
      "Epoch 467/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 00467: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 3.2834e-04 - accuracy: 1.0000 - val_loss: 0.0869 - val_accuracy: 1.0000\n",
      "Epoch 468/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 00468: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.0730 - val_accuracy: 1.0000\n",
      "Epoch 469/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 8.4397e-06 - accuracy: 1.0000\n",
      "Epoch 00469: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 5.7296e-04 - accuracy: 1.0000 - val_loss: 0.0245 - val_accuracy: 1.0000\n",
      "Epoch 470/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 6.4066e-04 - accuracy: 1.0000\n",
      "Epoch 00470: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 3.2432e-04 - accuracy: 1.0000 - val_loss: 0.0177 - val_accuracy: 1.0000\n",
      "Epoch 471/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 00471: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0187 - val_accuracy: 1.0000\n",
      "Epoch 472/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 00472: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 1.0000\n",
      "Epoch 473/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 00473: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 5.6425e-04 - accuracy: 1.0000 - val_loss: 0.0312 - val_accuracy: 1.0000\n",
      "Epoch 474/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 00474: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0502 - val_accuracy: 1.0000\n",
      "Epoch 475/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.4780e-04 - accuracy: 1.0000\n",
      "Epoch 00475: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 4.8898e-04 - accuracy: 1.0000 - val_loss: 0.0666 - val_accuracy: 1.0000\n",
      "Epoch 476/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 00476: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0647 - val_accuracy: 1.0000\n",
      "Epoch 477/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 8.6587e-04 - accuracy: 1.0000\n",
      "Epoch 00477: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 3.6032e-04 - accuracy: 1.0000 - val_loss: 0.0503 - val_accuracy: 1.0000\n",
      "Epoch 478/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 7.1733e-04 - accuracy: 1.0000\n",
      "Epoch 00478: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 5.8845e-04 - accuracy: 1.0000 - val_loss: 0.0395 - val_accuracy: 1.0000\n",
      "Epoch 479/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 4.2464e-04 - accuracy: 1.0000\n",
      "Epoch 00479: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 5.4324e-04 - accuracy: 1.0000 - val_loss: 0.0344 - val_accuracy: 1.0000\n",
      "Epoch 480/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 00480: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0370 - val_accuracy: 1.0000\n",
      "Epoch 481/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 4.6252e-06 - accuracy: 1.0000\n",
      "Epoch 00481: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 4.0422e-04 - accuracy: 1.0000 - val_loss: 0.0437 - val_accuracy: 1.0000\n",
      "Epoch 482/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 2.7574e-04 - accuracy: 1.0000\n",
      "Epoch 00482: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 5.9637e-04 - accuracy: 1.0000 - val_loss: 0.0509 - val_accuracy: 1.0000\n",
      "Epoch 483/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 00483: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 7.6708e-04 - accuracy: 1.0000 - val_loss: 0.0536 - val_accuracy: 1.0000\n",
      "Epoch 484/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 9.7271e-06 - accuracy: 1.0000\n",
      "Epoch 00484: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 4.5450e-04 - accuracy: 1.0000 - val_loss: 0.0554 - val_accuracy: 1.0000\n",
      "Epoch 485/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 9.7987e-05 - accuracy: 1.0000\n",
      "Epoch 00485: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0399 - val_accuracy: 1.0000\n",
      "Epoch 486/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 6.5921e-06 - accuracy: 1.0000\n",
      "Epoch 00486: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 7.8285e-04 - accuracy: 1.0000 - val_loss: 0.0263 - val_accuracy: 1.0000\n",
      "Epoch 487/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 00487: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 4.5593e-04 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 1.0000\n",
      "Epoch 488/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 3.1113e-06 - accuracy: 1.0000\n",
      "Epoch 00488: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 1.0000\n",
      "Epoch 489/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 00489: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0298 - val_accuracy: 1.0000\n",
      "Epoch 490/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 7.9576e-05 - accuracy: 1.0000\n",
      "Epoch 00490: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 4.2031e-04 - accuracy: 1.0000 - val_loss: 0.0525 - val_accuracy: 1.0000\n",
      "Epoch 491/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.5497e-07 - accuracy: 1.0000\n",
      "Epoch 00491: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 5.2837e-04 - accuracy: 1.0000 - val_loss: 0.0714 - val_accuracy: 1.0000\n",
      "Epoch 492/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.6689e-07 - accuracy: 1.0000\n",
      "Epoch 00492: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 1.7126e-04 - accuracy: 1.0000 - val_loss: 0.0794 - val_accuracy: 1.0000\n",
      "Epoch 493/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 7.8273e-05 - accuracy: 1.0000\n",
      "Epoch 00493: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 5.5515e-04 - accuracy: 1.0000 - val_loss: 0.0766 - val_accuracy: 1.0000\n",
      "Epoch 494/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 00494: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0484 - val_accuracy: 1.0000\n",
      "Epoch 495/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 00495: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.0287 - val_accuracy: 1.0000\n",
      "Epoch 496/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 2.6497e-05 - accuracy: 1.0000\n",
      "Epoch 00496: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0141 - val_accuracy: 1.0000\n",
      "Epoch 497/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 1.3768e-05 - accuracy: 1.0000\n",
      "Epoch 00497: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 6.0030e-04 - accuracy: 1.0000 - val_loss: 0.0130 - val_accuracy: 1.0000\n",
      "Epoch 498/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 5.4554e-04 - accuracy: 1.0000\n",
      "Epoch 00498: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 4.8946e-04 - accuracy: 1.0000 - val_loss: 0.0128 - val_accuracy: 1.0000\n",
      "Epoch 499/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 0.0117 - accuracy: 1.0000\n",
      "Epoch 00499: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0174 - val_accuracy: 1.0000\n",
      "Epoch 500/500\n",
      "10/48 [=====>........................] - ETA: 0s - loss: 2.4809e-04 - accuracy: 1.0000\n",
      "Epoch 00500: val_accuracy did not improve from 1.00000\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 2.8781e-04 - accuracy: 1.0000 - val_loss: 0.0277 - val_accuracy: 1.0000\n",
      "Training completed in time:  0:00:31.424365\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "num_epochs = 500\n",
    "num_batch_size = 10\n",
    "\n",
    "os.makedirs('models', exist_ok=True)\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        filepath='models/' + MODEL_NAME + '_{epoch:02d}.h5',\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_accuracy` score has improved.\n",
    "        save_best_only=True,\n",
    "        monitor='val_accuracy',\n",
    "        verbose=1)\n",
    "]\n",
    "start = datetime.now()\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs,\n",
    "                    validation_data=(x_val, y_val), callbacks=callbacks, verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot accuracies and losses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "diDQglrhSR1n"
   },
   "source": [
    "## Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Qyg7uylzSR1p",
    "outputId": "4b953e0d-5437-422d-cbc5-6b19a29879f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  1.0\n",
      "Testing Accuracy:  0.45454547\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Training Accuracy: \", score[1])\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Testing Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KnzqoZRbSR1v"
   },
   "outputs": [],
   "source": [
    "preds = model.predict(x_test) # label scores \n",
    "\n",
    "classpreds = np.argmax(preds, axis=1) # predicted classes \n",
    "\n",
    "y_testclass = np.argmax(y_test, axis=1) # true classes\n",
    "\n",
    "n_classes=2 # number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EMrG8FeJSR14"
   },
   "outputs": [],
   "source": [
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], preds[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oOZSWEULSR17"
   },
   "outputs": [],
   "source": [
    "c_names = ['Bronchiectasis', 'Bronchiolitis', 'COPD', 'Healthy', 'Pneumonia', 'URTI']\n",
    "c_names = ['Healthy', 'Pneumonia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 851
    },
    "colab_type": "code",
    "id": "HY5qEVRrSR2A",
    "outputId": "9c5341de-bbdf-4c87-c999-82ee08ddd1fb"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot ROC curves\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "ax.plot([0, 1], [0, 1], 'k--')\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curve for Each Class')\n",
    "for i in range(n_classes):\n",
    "    ax.plot(fpr[i], tpr[i], linewidth=3, label='ROC curve (area = %0.2f) for %s' % (roc_auc[i], c_names[i]))\n",
    "ax.legend(loc=\"best\", fontsize='x-large')\n",
    "ax.grid(alpha=.4)\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240
    },
    "colab_type": "code",
    "id": "qlQ12gHQSR2D",
    "outputId": "42af0b0e-037c-4177-a382-aff8a8550509"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Healthy       0.33      1.00      0.50         3\n",
      "   Pneumonia       1.00      0.25      0.40         8\n",
      "\n",
      "    accuracy                           0.45        11\n",
      "   macro avg       0.67      0.62      0.45        11\n",
      "weighted avg       0.82      0.45      0.43        11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report\n",
    "print(classification_report(y_testclass, classpreds, target_names=c_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "colab_type": "code",
    "id": "jGzk3U9kSR2G",
    "outputId": "cd147c90-fc8e-47c5-9cd0-6e3e224b3def"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 0]\n",
      " [6 2]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "print(confusion_matrix(y_testclass, classpreds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def plot_confusion_matrix(cm, unique_labels, show=True, output=None,\n",
    "                          title='Confusion matrix', cmap=plt.cm.Oranges):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(cm.shape[1])\n",
    "    plt.xticks(tick_marks, rotation=45)\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticklabels((ax.get_xticks() + 1).astype(str))\n",
    "    plt.yticks(tick_marks)\n",
    "\n",
    "    ax.set_xticklabels(unique_labels)\n",
    "    ax.set_yticklabels(unique_labels)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], '.1f'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    if output is not None:\n",
    "        plt.savefig(output)\n",
    "    plt.close()\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(confusion_matrix(y_testclass, classpreds), unique_labels=[\"Healthy\", \"Pneumonia\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Corona-Disease Classification by CNN using MFCC.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
